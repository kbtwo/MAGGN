{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8535776,"sourceType":"datasetVersion","datasetId":4725071}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install fair-esm  # latest release, OR:\n# !pip install git+https://github.com/facebookresearch/esm.git  # bleeding edge, current repo main branch\n# !pip install torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.0+cpu.html\n# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-2.0.0+cpu.html\n# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.0.0+cpu.html\n# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.0+cpu.html\n# !pip install torch-scatter\n\n!pip install torch-geometric   \n!pip install rdkit","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:49:45.702135Z","iopub.execute_input":"2024-10-15T02:49:45.702512Z","iopub.status.idle":"2024-10-15T02:50:19.896342Z","shell.execute_reply.started":"2024-10-15T02:49:45.702481Z","shell.execute_reply":"2024-10-15T02:50:19.895189Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.3.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nCollecting rdkit\n  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit) (9.5.0)\nDownloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit\nSuccessfully installed rdkit-2024.3.5\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install xlstm","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:19.899221Z","iopub.execute_input":"2024-10-15T02:50:19.899682Z","iopub.status.idle":"2024-10-15T02:50:19.905858Z","shell.execute_reply.started":"2024-10-15T02:50:19.899636Z","shell.execute_reply":"2024-10-15T02:50:19.904511Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf1 = pd.read_csv('dataset/Train_set/ProLaTherm_train.csv')\nprint(df1.shape)\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:19.907596Z","iopub.execute_input":"2024-10-15T02:50:19.907946Z","iopub.status.idle":"2024-10-15T02:50:22.353733Z","shell.execute_reply.started":"2024-10-15T02:50:19.907917Z","shell.execute_reply":"2024-10-15T02:50:22.352577Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(5139, 604)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         meta_protein_id                                     meta_file_name  \\\n0  sp|Q8TWU6|CDAT8_METKA  ProtThermoPred_evidence_only_clean_thermo-cdhi...   \n1    sp|Q8TXY4|MER_METKA  ProtThermoPred_evidence_only_clean_thermo-cdhi...   \n2   sp|Q9UXR8|HEM1_METKA  ProtThermoPred_evidence_only_clean_thermo-cdhi...   \n3    sp|P94951|MTD_METKA  ProtThermoPred_evidence_only_clean_thermo-cdhi...   \n4    sp|Q02394|HMD_METKA  ProtThermoPred_evidence_only_clean_thermo-cdhi...   \n\n  label_description  label_binary  \\\n0            thermo             1   \n1            thermo             1   \n2            thermo             1   \n3            thermo             1   \n4            thermo             1   \n\n                                         seq_peptide  feat_Mw  \\\n0  MKVSLAGQTVDVKKILNEIPKRTVTAALLEGGEIVAVEEADDEHAE...  30624.8   \n1  MAEVSFGIELLPDDKPTKIAHLIKVAEDNGFEYAWICDHYNNYSYM...  37509.0   \n2  MEDLVCVGITHKEAEVEELEKARFESDEAVRDIVESFGLSGCVLLQ...  45453.4   \n3  MTVAKAIFIKCGNLGTSMMMDMLLDERADREDVEFRVVGTSVKMDP...  31387.7   \n4  MVEINKVAILGAGCWRTHAATGITTFKRACEVADETGIKEAALTHS...  39046.5   \n\n   feat_charge_of_all  feat_pos_charge  feat_neg_charge  feat_polar  ...  \\\n0               -10.0             40.0             50.0       126.0  ...   \n1               -22.0             36.0             58.0       154.0  ...   \n2                -6.0             76.0             82.0       212.0  ...   \n3               -27.0             34.0             61.0       130.0  ...   \n4               -36.0             36.0             72.0       167.0  ...   \n\n   feat_DPC_YM  feat_DPC_YN  feat_DPC_YP  feat_DPC_YQ  feat_DPC_YR  \\\n0     0.000000     0.000000     0.000000     0.000000     0.000000   \n1     0.005747     0.002874     0.000000     0.002874     0.000000   \n2     0.000000     0.000000     0.000000     0.000000     0.002481   \n3     0.000000     0.003546     0.003546     0.000000     0.000000   \n4     0.000000     0.000000     0.000000     0.000000     0.000000   \n\n   feat_DPC_YS  feat_DPC_YT  feat_DPC_YV  feat_DPC_YW  feat_DPC_YY  \n0     0.000000     0.000000     0.000000          0.0      0.00361  \n1     0.002874     0.005747     0.002874          0.0      0.00000  \n2     0.000000     0.002481     0.000000          0.0      0.00000  \n3     0.000000     0.000000     0.003546          0.0      0.00000  \n4     0.002801     0.000000     0.002801          0.0      0.00000  \n\n[5 rows x 604 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meta_protein_id</th>\n      <th>meta_file_name</th>\n      <th>label_description</th>\n      <th>label_binary</th>\n      <th>seq_peptide</th>\n      <th>feat_Mw</th>\n      <th>feat_charge_of_all</th>\n      <th>feat_pos_charge</th>\n      <th>feat_neg_charge</th>\n      <th>feat_polar</th>\n      <th>...</th>\n      <th>feat_DPC_YM</th>\n      <th>feat_DPC_YN</th>\n      <th>feat_DPC_YP</th>\n      <th>feat_DPC_YQ</th>\n      <th>feat_DPC_YR</th>\n      <th>feat_DPC_YS</th>\n      <th>feat_DPC_YT</th>\n      <th>feat_DPC_YV</th>\n      <th>feat_DPC_YW</th>\n      <th>feat_DPC_YY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sp|Q8TWU6|CDAT8_METKA</td>\n      <td>ProtThermoPred_evidence_only_clean_thermo-cdhi...</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MKVSLAGQTVDVKKILNEIPKRTVTAALLEGGEIVAVEEADDEHAE...</td>\n      <td>30624.8</td>\n      <td>-10.0</td>\n      <td>40.0</td>\n      <td>50.0</td>\n      <td>126.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sp|Q8TXY4|MER_METKA</td>\n      <td>ProtThermoPred_evidence_only_clean_thermo-cdhi...</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MAEVSFGIELLPDDKPTKIAHLIKVAEDNGFEYAWICDHYNNYSYM...</td>\n      <td>37509.0</td>\n      <td>-22.0</td>\n      <td>36.0</td>\n      <td>58.0</td>\n      <td>154.0</td>\n      <td>...</td>\n      <td>0.005747</td>\n      <td>0.002874</td>\n      <td>0.000000</td>\n      <td>0.002874</td>\n      <td>0.000000</td>\n      <td>0.002874</td>\n      <td>0.005747</td>\n      <td>0.002874</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sp|Q9UXR8|HEM1_METKA</td>\n      <td>ProtThermoPred_evidence_only_clean_thermo-cdhi...</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MEDLVCVGITHKEAEVEELEKARFESDEAVRDIVESFGLSGCVLLQ...</td>\n      <td>45453.4</td>\n      <td>-6.0</td>\n      <td>76.0</td>\n      <td>82.0</td>\n      <td>212.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002481</td>\n      <td>0.000000</td>\n      <td>0.002481</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sp|P94951|MTD_METKA</td>\n      <td>ProtThermoPred_evidence_only_clean_thermo-cdhi...</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MTVAKAIFIKCGNLGTSMMMDMLLDERADREDVEFRVVGTSVKMDP...</td>\n      <td>31387.7</td>\n      <td>-27.0</td>\n      <td>34.0</td>\n      <td>61.0</td>\n      <td>130.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.003546</td>\n      <td>0.003546</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003546</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sp|Q02394|HMD_METKA</td>\n      <td>ProtThermoPred_evidence_only_clean_thermo-cdhi...</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MVEINKVAILGAGCWRTHAATGITTFKRACEVADETGIKEAALTHS...</td>\n      <td>39046.5</td>\n      <td>-36.0</td>\n      <td>36.0</td>\n      <td>72.0</td>\n      <td>167.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002801</td>\n      <td>0.000000</td>\n      <td>0.002801</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 604 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sequence = df1['sequence']\nlabel = df1['label']\nprint(len(label))\nprint(len(sequence))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.355641Z","iopub.execute_input":"2024-10-15T02:50:22.356593Z","iopub.status.idle":"2024-10-15T02:50:22.364550Z","shell.execute_reply.started":"2024-10-15T02:50:22.356534Z","shell.execute_reply":"2024-10-15T02:50:22.363516Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"5139\n5139\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndf2 = pd.read_csv('dataset/Test_set/ProLaTherm_test1.csv')\nprint(df2.shape)\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.367530Z","iopub.execute_input":"2024-10-15T02:50:22.367871Z","iopub.status.idle":"2024-10-15T02:50:22.399035Z","shell.execute_reply.started":"2024-10-15T02:50:22.367842Z","shell.execute_reply":"2024-10-15T02:50:22.397729Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(569, 2)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                            Sequence  Label\n0  MPKPYVAINMAELKNEPKTFEMFASVGPKVCMVTARHPGFVGFQNH...      1\n1  MSGKQSEEFKRTEKMTRMEYLFPVRFAVGWMFLDGGLRKAVLKPAK...      1\n2  MERVTIIGIIFAILVVGWILATGQWAYGNVVGPLVNHSKIPLLKIT...      1\n3  MTKVLVLGGRFGALTAAYTLKRLVGSKADVKVINKSRFSYFRPALP...      1\n4  MNLKILVGLFILGIIILSAMTFLNFTTIVAQDKGDQQPKGPIVYTY...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sequence</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MPKPYVAINMAELKNEPKTFEMFASVGPKVCMVTARHPGFVGFQNH...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MSGKQSEEFKRTEKMTRMEYLFPVRFAVGWMFLDGGLRKAVLKPAK...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MERVTIIGIIFAILVVGWILATGQWAYGNVVGPLVNHSKIPLLKIT...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MTKVLVLGGRFGALTAAYTLKRLVGSKADVKVINKSRFSYFRPALP...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MNLKILVGLFILGIIILSAMTFLNFTTIVAQDKGDQQPKGPIVYTY...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sequence2 = df2['sequence']\nlabel2 = df2['label']\nprint(len(label2))\nprint(len(sequence2))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.400658Z","iopub.execute_input":"2024-10-15T02:50:22.401120Z","iopub.status.idle":"2024-10-15T02:50:22.407988Z","shell.execute_reply.started":"2024-10-15T02:50:22.401079Z","shell.execute_reply":"2024-10-15T02:50:22.406904Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"569\n569\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndf3 = pd.read_csv('dataset/Test_set/ProLaTherm_test2.csv')\nprint(df3.shape)\ndf3.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.409594Z","iopub.execute_input":"2024-10-15T02:50:22.410045Z","iopub.status.idle":"2024-10-15T02:50:22.575209Z","shell.execute_reply.started":"2024-10-15T02:50:22.410012Z","shell.execute_reply":"2024-10-15T02:50:22.574104Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(395, 603)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"              uniProtID label_description  label_binary  \\\n0   sp|P29082|SOR_ACIAM            thermo             1   \n1  sp|P97207|DOXD_ACIAM            thermo             1   \n2  sp|P97224|DOXA_ACIAM            thermo             1   \n3  sp|Q7ZAG8|SQRD_ACIAM            thermo             1   \n4   sp|G8YXZ9|TTH_ACIAM            thermo             1   \n\n                                         seq_peptide  feat_Mw  \\\n0  MPKPYVAINMAELKNEPKTFEMFASVGPKVCMVTARHPGFVGFQNH...  35325.1   \n1  MSGKQSEEFKRTEKMTRMEYLFPVRFAVGWMFLDGGLRKAVLKPAK...  20414.8   \n2  MERVTIIGIIFAILVVGWILATGQWAYGNVVGPLVNHSKIPLLKIT...  18761.0   \n3  MTKVLVLGGRFGALTAAYTLKRLVGSKADVKVINKSRFSYFRPALP...  45161.8   \n4  MNLKILVGLFILGIIILSAMTFLNFTTIVAQDKGDQQPKGPIVYTY...  57672.7   \n\n   feat_charge_of_all  feat_pos_charge  feat_neg_charge  feat_polar  \\\n0                 4.0             38.0             34.0       141.0   \n1                 7.0             21.0             14.0        66.0   \n2                 5.0             14.0              9.0        78.0   \n3                 6.0             55.0             49.0       197.0   \n4                 7.0             29.0             22.0       210.0   \n\n   feat_unpolar  ...  feat_DPC_YM  feat_DPC_YN  feat_DPC_YP  feat_DPC_YQ  \\\n0         168.0  ...     0.000000     0.000000     0.003247     0.000000   \n1         118.0  ...     0.000000     0.000000     0.000000     0.000000   \n2          90.0  ...     0.000000     0.011976     0.005988     0.000000   \n3         212.0  ...     0.002451     0.002451     0.002451     0.002451   \n4         325.0  ...     0.000000     0.001873     0.003745     0.000000   \n\n   feat_DPC_YR  feat_DPC_YS  feat_DPC_YT  feat_DPC_YV  feat_DPC_YW  \\\n0          0.0     0.006494     0.003247     0.003247     0.000000   \n1          0.0     0.000000     0.000000     0.000000     0.005464   \n2          0.0     0.011976     0.011976     0.005988     0.000000   \n3          0.0     0.002451     0.007353     0.002451     0.000000   \n4          0.0     0.000000     0.007491     0.014981     0.001873   \n\n   feat_DPC_YY  \n0     0.000000  \n1     0.000000  \n2     0.000000  \n3     0.007353  \n4     0.009363  \n\n[5 rows x 603 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniProtID</th>\n      <th>label_description</th>\n      <th>label_binary</th>\n      <th>seq_peptide</th>\n      <th>feat_Mw</th>\n      <th>feat_charge_of_all</th>\n      <th>feat_pos_charge</th>\n      <th>feat_neg_charge</th>\n      <th>feat_polar</th>\n      <th>feat_unpolar</th>\n      <th>...</th>\n      <th>feat_DPC_YM</th>\n      <th>feat_DPC_YN</th>\n      <th>feat_DPC_YP</th>\n      <th>feat_DPC_YQ</th>\n      <th>feat_DPC_YR</th>\n      <th>feat_DPC_YS</th>\n      <th>feat_DPC_YT</th>\n      <th>feat_DPC_YV</th>\n      <th>feat_DPC_YW</th>\n      <th>feat_DPC_YY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sp|P29082|SOR_ACIAM</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MPKPYVAINMAELKNEPKTFEMFASVGPKVCMVTARHPGFVGFQNH...</td>\n      <td>35325.1</td>\n      <td>4.0</td>\n      <td>38.0</td>\n      <td>34.0</td>\n      <td>141.0</td>\n      <td>168.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003247</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.006494</td>\n      <td>0.003247</td>\n      <td>0.003247</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sp|P97207|DOXD_ACIAM</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MSGKQSEEFKRTEKMTRMEYLFPVRFAVGWMFLDGGLRKAVLKPAK...</td>\n      <td>20414.8</td>\n      <td>7.0</td>\n      <td>21.0</td>\n      <td>14.0</td>\n      <td>66.0</td>\n      <td>118.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.005464</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sp|P97224|DOXA_ACIAM</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MERVTIIGIIFAILVVGWILATGQWAYGNVVGPLVNHSKIPLLKIT...</td>\n      <td>18761.0</td>\n      <td>5.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>78.0</td>\n      <td>90.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.011976</td>\n      <td>0.005988</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.011976</td>\n      <td>0.011976</td>\n      <td>0.005988</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sp|Q7ZAG8|SQRD_ACIAM</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MTKVLVLGGRFGALTAAYTLKRLVGSKADVKVINKSRFSYFRPALP...</td>\n      <td>45161.8</td>\n      <td>6.0</td>\n      <td>55.0</td>\n      <td>49.0</td>\n      <td>197.0</td>\n      <td>212.0</td>\n      <td>...</td>\n      <td>0.002451</td>\n      <td>0.002451</td>\n      <td>0.002451</td>\n      <td>0.002451</td>\n      <td>0.0</td>\n      <td>0.002451</td>\n      <td>0.007353</td>\n      <td>0.002451</td>\n      <td>0.000000</td>\n      <td>0.007353</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sp|G8YXZ9|TTH_ACIAM</td>\n      <td>thermo</td>\n      <td>1</td>\n      <td>MNLKILVGLFILGIIILSAMTFLNFTTIVAQDKGDQQPKGPIVYTY...</td>\n      <td>57672.7</td>\n      <td>7.0</td>\n      <td>29.0</td>\n      <td>22.0</td>\n      <td>210.0</td>\n      <td>325.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.001873</td>\n      <td>0.003745</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.007491</td>\n      <td>0.014981</td>\n      <td>0.001873</td>\n      <td>0.009363</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 603 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sequence3 = df3['sequence']\nlabel3 = df3['label']\nprint(len(label3))\nprint(len(sequence3))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.576623Z","iopub.execute_input":"2024-10-15T02:50:22.577046Z","iopub.status.idle":"2024-10-15T02:50:22.584263Z","shell.execute_reply.started":"2024-10-15T02:50:22.577008Z","shell.execute_reply":"2024-10-15T02:50:22.582889Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"395\n395\n","output_type":"stream"}]},{"cell_type":"code","source":"X = sequence.values\ny = label.values","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.585763Z","iopub.execute_input":"2024-10-15T02:50:22.586242Z","iopub.status.idle":"2024-10-15T02:50:22.595624Z","shell.execute_reply.started":"2024-10-15T02:50:22.586206Z","shell.execute_reply":"2024-10-15T02:50:22.594291Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X2 = sequence2.values\ny2 = label2.values","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.596967Z","iopub.execute_input":"2024-10-15T02:50:22.597387Z","iopub.status.idle":"2024-10-15T02:50:22.607693Z","shell.execute_reply.started":"2024-10-15T02:50:22.597347Z","shell.execute_reply":"2024-10-15T02:50:22.606616Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X3 = sequence3.values\ny3 = label3.values","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.609176Z","iopub.execute_input":"2024-10-15T02:50:22.609604Z","iopub.status.idle":"2024-10-15T02:50:22.616509Z","shell.execute_reply.started":"2024-10-15T02:50:22.609566Z","shell.execute_reply":"2024-10-15T02:50:22.615397Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport uuid\n\ndef create_custom_id(index):\n    return f\"fulldata_{index}\"\n\ndef convert_to_dict(data): \n    protein_dict = {}\n    for i, sequence in enumerate(data):\n        unique_id = create_custom_id(i)\n        protein_dict[unique_id] = sequence\n    return protein_dict\n\nprotein_array = X\nproteins_train = convert_to_dict(protein_array) \n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.617969Z","iopub.execute_input":"2024-10-15T02:50:22.618377Z","iopub.status.idle":"2024-10-15T02:50:22.631339Z","shell.execute_reply.started":"2024-10-15T02:50:22.618334Z","shell.execute_reply":"2024-10-15T02:50:22.630248Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport uuid\n\ndef create_custom_id(index):\n    return f\"seq_test1_{index}\"\n\ndef convert_to_dict(data):\n    protein_dict = {}\n    for i, sequence in enumerate(data):\n        unique_id = create_custom_id(i)\n        protein_dict[unique_id] = sequence\n    return protein_dict\n\nprotein_array = X2\nproteins_test = convert_to_dict(protein_array)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.632553Z","iopub.execute_input":"2024-10-15T02:50:22.632892Z","iopub.status.idle":"2024-10-15T02:50:22.644417Z","shell.execute_reply.started":"2024-10-15T02:50:22.632854Z","shell.execute_reply":"2024-10-15T02:50:22.643399Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport uuid\n\ndef create_custom_id(index):\n    return f\"test_set2_{index}\"\n\ndef convert_to_dict(data):\n    protein_dict = {}\n    for i, sequence in enumerate(data):\n        unique_id = create_custom_id(i)\n        protein_dict[unique_id] = sequence\n    return protein_dict\n\nprotein_array = X3\nproteins_test2 = convert_to_dict(protein_array)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.650513Z","iopub.execute_input":"2024-10-15T02:50:22.650886Z","iopub.status.idle":"2024-10-15T02:50:22.657985Z","shell.execute_reply.started":"2024-10-15T02:50:22.650825Z","shell.execute_reply":"2024-10-15T02:50:22.656897Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nimport esm\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json, pickle\nfrom collections import OrderedDict\nimport os\nfrom tqdm import tqdm\n\ndef protein_graph_construct(proteins, save_dir):\n    # Load ESM-1b model\n    # torch.set_grad_enabled(False)\n    #Predicting protein contact maps\n    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n    batch_converter = alphabet.get_batch_converter()\n    target_graph = {}\n\n    count = 0\n    key_list=[]\n    for key in proteins:\n        key_list.append(key)\n\n\n    for k_i in tqdm(range(len(key_list))):\n        key=key_list[k_i]\n        # if len(proteins[key]) < 1500:\n        #     continue\n        data = []\n        pro_id = key\n        if os.path.exists(save_dir + pro_id + '.npy'):\n            continue\n        seq = proteins[key]\n        if len(seq) <= 1000:\n            data.append((pro_id, seq))\n            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n            with torch.no_grad():\n                results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n            contact_map = results[\"contacts\"][0].numpy()\n            target_graph[pro_id] = contact_map\n        else:\n            contact_prob_map = np.zeros((len(seq), len(seq)))  # global contact map prediction\n            interval = 500\n            i = math.ceil(len(seq) / interval)\n            \n            for s in range(i):\n                start = s * interval  # sub seq predict start\n                end = min((s + 2) * interval, len(seq))  # sub seq predict end\n                sub_seq_len = end - start\n\n                # prediction\n                temp_seq = seq[start:end]\n                temp_data = []\n                temp_data.append((pro_id, temp_seq))\n                batch_labels, batch_strs, batch_tokens = batch_converter(temp_data)\n                with torch.no_grad():\n                    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n                # insert into the global contact map\n                row, col = np.where(contact_prob_map[start:end, start:end] != 0)\n                row = row + start\n                col = col + start\n                contact_prob_map[start:end, start:end] = contact_prob_map[start:end, start:end] + results[\"contacts\"][\n                    0].numpy()\n                contact_prob_map[row, col] = contact_prob_map[row, col] / 2.0\n                if end == len(seq):\n                    break\n            target_graph[pro_id] = contact_prob_map\n\n        np.save(save_dir + pro_id + '.npy', target_graph[pro_id])\n        count += 1\n\n\n# df1 = pd.read_csv('dataset/Train_set/ProLaTherm_train.csv')\n# sequence = df1['sequence']\n# label = df1['label']\n# X = sequence.values\n# y = label.values\n# def create_custom_id1(index):\n#     return f\"fulldata_{index}\"\n# def convert_to_dict1(data):\n#     protein_dict = {}\n#     for i, sequence in enumerate(data):\n#         unique_id = create_custom_id1(i)\n#         protein_dict[unique_id] = sequence\n#     return protein_dict\n# protein_array1 = X\n# proteins_train = convert_to_dict1(protein_array1)\n# save_dir='/'\n#protein_graph_construct(proteins_train, save_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import traceback\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport sys\nimport os\nimport random\n\nimport json, pickle\nfrom collections import OrderedDict\nfrom rdkit import Chem\nfrom rdkit.Chem import MolFromSmiles\nfrom tqdm import tqdm\n\n\ndef dic_normalize(dic): \n    # print(dic)\n    max_value = dic[max(dic, key=dic.get)]\n    min_value = dic[min(dic, key=dic.get)]\n    # print(max_value)\n    interval = float(max_value) - float(min_value)\n    for key in dic.keys():\n        dic[key] = (dic[key] - min_value) / interval\n    dic['X'] = (max_value + min_value) / 2.0\n    return dic\n\n\npro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', \n                 'S', 'T', 'V', 'W', 'Y','X']\n\npro_res_aliphatic_table = ['A', 'I', 'L', 'M', 'V']\npro_res_aromatic_table = ['F', 'W', 'Y']\npro_res_polar_neutral_table = ['C', 'N', 'Q', 'S', 'T']\npro_res_acidic_charged_table = ['D', 'E']\npro_res_basic_charged_table = ['H', 'K', 'R']\n\nres_weight_table = {'A': 71.08, 'C': 103.15, 'D': 115.09, 'E': 129.12, 'F': 147.18, \n                    'G': 57.05, 'H': 137.14,\n                    'I': 113.16, 'K': 128.18, 'L': 113.16, 'M': 131.20, 'N': 114.11, \n                    'P': 97.12, 'Q': 128.13,\n                    'R': 156.19, 'S': 87.08, 'T': 101.11, 'V': 99.13, 'W': 186.22, \n                    'Y': 163.18}\nres_weight_table['X'] = np.average([res_weight_table[k] for k in res_weight_table.keys()])\n\nres_pka_table = {'A': 2.34, 'C': 1.96, 'D': 1.88, 'E': 2.19, 'F': 1.83, 'G': 2.34, \n                 'H': 1.82, 'I': 2.36,\n                 'K': 2.18, 'L': 2.36, 'M': 2.28, 'N': 2.02, 'P': 1.99, 'Q': 2.17, \n                 'R': 2.17, 'S': 2.21,\n                 'T': 2.09, 'V': 2.32, 'W': 2.83, 'Y': 2.32}\nres_pka_table['X'] = np.average([res_pka_table[k] for k in res_pka_table.keys()])\n\nres_pkb_table = {'A': 9.69, 'C': 10.28, 'D': 9.60, 'E': 9.67, 'F': 9.13, 'G': 9.60, \n                 'H': 9.17,\n                 'I': 9.60, 'K': 8.95, 'L': 9.60, 'M': 9.21, 'N': 8.80, 'P': 10.60, \n                 'Q': 9.13,\n                 'R': 9.04, 'S': 9.15, 'T': 9.10, 'V': 9.62, 'W': 9.39, 'Y': 9.62}\nres_pkb_table['X'] = np.average([res_pkb_table[k] for k in res_pkb_table.keys()])\n\nres_pkx_table = {'A': 0.00, 'C': 8.18, 'D': 3.65, 'E': 4.25, 'F': 0.00, 'G': 0, 'H': 6.00,\n                 'I': 0.00, 'K': 10.53, 'L': 0.00, 'M': 0.00, 'N': 0.00, 'P': 0.00, \n                 'Q': 0.00,\n                 'R': 12.48, 'S': 0.00, 'T': 0.00, 'V': 0.00, 'W': 0.00, 'Y': 0.00}\nres_pkx_table['X'] = np.average([res_pkx_table[k] for k in res_pkx_table.keys()])\n\nres_pl_table = {'A': 6.00, 'C': 5.07, 'D': 2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, \n                'H': 7.59,\n                'I': 6.02, 'K': 9.74, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.30, \n                'Q': 5.65,\n                'R': 10.76, 'S': 5.68, 'T': 5.60, 'V': 5.96, 'W': 5.89, 'Y': 5.96}\nres_pl_table['X'] = np.average([res_pl_table[k] for k in res_pl_table.keys()])\n\nres_hydrophobic_ph2_table = {'A': 47, 'C': 52, 'D': -18, 'E': 8, 'F': 92, 'G': 0, \n                             'H': -42, 'I': 100,\n                             'K': -37, 'L': 100, 'M': 74, 'N': -41, 'P': -46, 'Q': -18, \n                             'R': -26, 'S': -7,\n                             'T': 13, 'V': 79, 'W': 84, 'Y': 49}\nres_hydrophobic_ph2_table['X'] = np.average([res_hydrophobic_ph2_table[k] for k in res_hydrophobic_ph2_table.keys()])\n\nres_hydrophobic_ph7_table = {'A': 41, 'C': 49, 'D': -55, 'E': -31, 'F': 100, 'G': 0, \n                             'H': 8, 'I': 99,\n                             'K': -23, 'L': 97, 'M': 74, 'N': -28, 'P': -46, 'Q': -10, \n                             'R': -14, 'S': -5,\n                             'T': 13, 'V': 76, 'W': 97, 'Y': 63}\nres_hydrophobic_ph7_table['X'] = np.average([res_hydrophobic_ph7_table[k] for k in res_hydrophobic_ph7_table.keys()])\n\n# nomarlize the residue feature\nres_weight_table = dic_normalize(res_weight_table)\nres_pka_table = dic_normalize(res_pka_table)\nres_pkb_table = dic_normalize(res_pkb_table)\nres_pkx_table = dic_normalize(res_pkx_table)\nres_pl_table = dic_normalize(res_pl_table)\nres_hydrophobic_ph2_table = dic_normalize(res_hydrophobic_ph2_table)\nres_hydrophobic_ph7_table = dic_normalize(res_hydrophobic_ph7_table)\n\n\n# one ont encoding\ndef one_hot_encoding(x, allowable_set):\n    if x not in allowable_set: \n        # print(x)\n        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n# one ont encoding with unknown symbol\ndef one_hot_encoding_unk(x, allowable_set):\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\ndef seq_feature(seq): \n    residue_feature = []\n    for residue in seq: \n        if residue not in pro_res_table: \n            residue = 'X'\n        res_property1 = [1 if residue in pro_res_aliphatic_table else 0, \n                         1 if residue in pro_res_aromatic_table else 0,\n                         1 if residue in pro_res_polar_neutral_table else 0,\n                         1 if residue in pro_res_acidic_charged_table else 0,\n                         1 if residue in pro_res_basic_charged_table else 0]\n        \n        res_property2 = [res_weight_table[residue], res_pka_table[residue], \n                         res_pkb_table[residue],res_pkx_table[residue],\n                         res_pl_table[residue], res_hydrophobic_ph2_table[residue], \n                         res_hydrophobic_ph7_table[residue]]\n        residue_feature.append(res_property1 + res_property2) \n\n    pro_hot = np.zeros((len(seq), len(pro_res_table))) \n    pro_property = np.zeros((len(seq), 12)) \n    for i in range(len(seq)):\n        pro_hot[i,] = one_hot_encoding_unk(seq[i], pro_res_table)\n        pro_property[i,] = residue_feature[i]\n\n    seq_feature = np.concatenate((pro_hot, pro_property), axis=1)\n    return seq_feature","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:22.659799Z","iopub.execute_input":"2024-10-15T02:50:22.660207Z","iopub.status.idle":"2024-10-15T02:50:23.846162Z","shell.execute_reply.started":"2024-10-15T02:50:22.660170Z","shell.execute_reply":"2024-10-15T02:50:23.845211Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def sequence_to_graph(target_key, target_sequence, distance_dir,distance_dir_pssm):\n    target_edge_index = []\n    target_edge_distance = []\n    target_size = len(target_sequence)\n    contact_map_file = os.path.join(distance_dir, target_key + '.npy')\n    pssm_data_file = os.path.join(distance_dir_pssm,target_key+'.npy')\n    distance_map = np.load(contact_map_file)\n    pssm_data = np.load(pssm_data_file)   \n    normalized_pssm = 1 / (1 + np.exp(-pssm_data))\n    \n    # the neighbor residue should have a edge\n    # add self loop\n    for i in range(target_size):\n        distance_map[i, i] = 1\n        if i + 1 < target_size:\n            distance_map[i, i + 1] = 1\n    # print(distance_map)\n    index_row, index_col = np.where(distance_map >= 0.5)  # for threshold\n    for i, j in zip(index_row, index_col):\n        target_edge_index.append([i, j]) \n        target_edge_distance.append(distance_map[i, j])  \n\n    target_feature = np.concatenate((seq_feature(target_sequence), normalized_pssm), axis=1)\n    return target_size, target_feature, target_edge_index, target_edge_distance\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:23.847468Z","iopub.execute_input":"2024-10-15T02:50:23.847751Z","iopub.status.idle":"2024-10-15T02:50:23.860099Z","shell.execute_reply.started":"2024-10-15T02:50:23.847724Z","shell.execute_reply":"2024-10-15T02:50:23.858769Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torch_geometric.data import InMemoryDataset, DataLoader, Batch\nfrom torch_geometric import data as DATA\nimport torch\nfrom tqdm import tqdm\n\n\n# initialize the dataset\nclass DTADataset(InMemoryDataset):\n    def __init__(self, root='/', y=None, transform=None,\n                 pre_transform=None,target_key=None, target_graph=None):\n        super(DTADataset, self).__init__(root, transform, pre_transform)\n\n        self.target = target_key\n        self.y = y\n        self.target_graph = target_graph\n        self.process(target_key, y,target_graph)\n\n    @property\n    def raw_file_names(self):\n        pass\n        # return ['some_file_1', 'some_file_2', ...]\n\n    @property\n    def processed_file_names(self):\n        #return [self.dataset + '_data_mol.pt', self.dataset + '_data_pro.pt']\n        pass\n\n    def download(self):\n        # Download to `self.raw_dir`.\n        pass\n\n    def _download(self):\n        pass\n\n    def _process(self):\n#         if not os.path.exists(self.processed_dir):\n#             os.makedirs(self.processed_dir)\n        pass\n\n    def process(self, target_key, y, target_graph):\n        data_list_pro = []\n        data_len = len(target_key)\n        print('loading tensors ...')\n        for i in tqdm(range(data_len)):\n            tar_key = target_key[i]\n            labels = y[i]\n            # print(labels,type(labels))\n\n            target_size, target_features, target_edge_index, target_edge_weight = target_graph[tar_key]\n         \n            GCNData_pro = DATA.Data(x=torch.Tensor(target_features),\n                                    edge_index=torch.LongTensor(target_edge_index).transpose(1, 0),\n                                    edge_weight=torch.FloatTensor(target_edge_weight),\n                                    y=torch.FloatTensor([labels]))\n            GCNData_pro.__setitem__('target_size', torch.LongTensor([target_size]))\n            data_list_pro.append(GCNData_pro)\n\n        if self.pre_filter is not None:\n            data_list_pro = [data for data in data_list_pro if self.pre_filter(data)]\n        if self.pre_transform is not None:\n            data_list_pro = [self.pre_transform(data) for data in data_list_pro]\n        self.data_pro = data_list_pro\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        # return GNNData_pro\n        return self.data_pro[idx]\n    \n    \ndef collate(data_list):\n    batchA = Batch.from_data_list([data for data in data_list])\n    \n    return batchA\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:23.861931Z","iopub.execute_input":"2024-10-15T02:50:23.862238Z","iopub.status.idle":"2024-10-15T02:50:32.218805Z","shell.execute_reply.started":"2024-10-15T02:50:23.862212Z","shell.execute_reply":"2024-10-15T02:50:32.217818Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def create_train_dataset():\n    \n    # load protein feature and predicted distance map\n    pro_distance_dir = 'contact_map/Train'  \n    pssm_dir = 'pssm/Train'\n    \n    prots = []  # sequences\n    prot_keys = []  # protein id (or name)\n    labels = y\n\n    # seqs\n    for t in proteins_train.keys():\n        prots.append(proteins_train[t])\n        prot_keys.append(t)\n    \n    train_prot_keys = prot_keys #蛋白质序列的唯一标识符\n    train_Y = labels #蛋白质的类别\n\n    # create target graph\n    # print('target_key', len(target_key), len(set(target_key)))\n    target_graph = {}\n    for i in tqdm(range(len(prot_keys))):\n        key = prot_keys[i] #标识符\n        g_t = sequence_to_graph(key, proteins_train[key], pro_distance_dir,pssm_dir)\n        target_graph[key] = g_t\n\n        \n    train_prot_keys, train_Y = np.asarray(train_prot_keys), np.asarray(train_Y)\n    train_dataset = DTADataset(root='/', target_key=train_prot_keys, y=train_Y, \n                               target_graph=target_graph)\n\n    return train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:32.220033Z","iopub.execute_input":"2024-10-15T02:50:32.220619Z","iopub.status.idle":"2024-10-15T02:50:32.230445Z","shell.execute_reply.started":"2024-10-15T02:50:32.220588Z","shell.execute_reply":"2024-10-15T02:50:32.229260Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def create_test_dataset():\n    \n    # load protein feature and predicted distance map\n    pro_distance_dir = 'contact_map/Test 1'   \n    pssm_dir = 'pssm/Test 1'\n    \n    prots = []  # sequences\n    prot_keys = []  # protein id (or name)\n    labels = y2\n\n    # seqs\n    for t in proteins_test.keys():\n        prots.append(proteins_test[t])\n        prot_keys.append(t)\n    \n    train_prot_keys = prot_keys\n    train_Y = labels\n\n    # create target graph\n    # print('target_key', len(target_key), len(set(target_key)))\n    target_graph = {}\n    for i in tqdm(range(len(prot_keys))):\n        key = prot_keys[i]\n        g_t = sequence_to_graph(key, proteins_test[key], pro_distance_dir,pssm_dir)\n        target_graph[key] = g_t\n\n        \n    train_prot_keys, train_Y = np.asarray(train_prot_keys), np.asarray(train_Y)\n    train_dataset = DTADataset(root='/', target_key=train_prot_keys, y=train_Y, \n                               target_graph=target_graph)\n\n    return train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:32.232196Z","iopub.execute_input":"2024-10-15T02:50:32.232643Z","iopub.status.idle":"2024-10-15T02:50:32.264974Z","shell.execute_reply.started":"2024-10-15T02:50:32.232604Z","shell.execute_reply":"2024-10-15T02:50:32.263914Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def create_test_dataset2():\n    \n    # load protein feature and predicted distance map\n    pro_distance_dir = 'contact_map/Test 2'   \n    pssm_dir = 'pssm/Test 2'\n    \n    prots = []  # sequences\n    prot_keys = []  # protein id (or name)\n    labels = y3\n\n    # seqs\n    for t in proteins_test2.keys():\n        prots.append(proteins_test2[t])\n        prot_keys.append(t)\n    \n    train_prot_keys = prot_keys\n    train_Y = labels\n\n    target_graph = {}\n    for i in tqdm(range(len(prot_keys))):\n        key = prot_keys[i]\n        g_t = sequence_to_graph(key, proteins_test2[key], pro_distance_dir,pssm_dir)\n        target_graph[key] = g_t\n\n        \n    train_prot_keys, train_Y = np.asarray(train_prot_keys), np.asarray(train_Y)\n    train_dataset = DTADataset(root='/', target_key=train_prot_keys, y=train_Y, \n                               target_graph=target_graph)\n\n    return train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:32.267040Z","iopub.execute_input":"2024-10-15T02:50:32.267557Z","iopub.status.idle":"2024-10-15T02:50:32.282118Z","shell.execute_reply.started":"2024-10-15T02:50:32.267504Z","shell.execute_reply":"2024-10-15T02:50:32.281182Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 256\nTEST_BATCH_SIZE = 256\n\ntrain_data = create_train_dataset()\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True,\n                                               collate_fn=collate)\n\ntest_data = create_test_dataset()\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=True,\n                                               collate_fn=collate)\n\ntest_data2 = create_test_dataset2()\ntest_loader2 = torch.utils.data.DataLoader(test_data2, batch_size=TEST_BATCH_SIZE, shuffle=True,\n                                               collate_fn=collate)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:50:32.283528Z","iopub.execute_input":"2024-10-15T02:50:32.283844Z","iopub.status.idle":"2024-10-15T02:53:35.877296Z","shell.execute_reply.started":"2024-10-15T02:50:32.283804Z","shell.execute_reply":"2024-10-15T02:53:35.876256Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 5139/5139 [02:24<00:00, 35.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"loading tensors ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5139/5139 [00:06<00:00, 764.30it/s]\n100%|██████████| 569/569 [00:18<00:00, 31.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"loading tensors ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 569/569 [00:00<00:00, 719.96it/s]\n100%|██████████| 395/395 [00:12<00:00, 30.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"loading tensors ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 395/395 [00:00<00:00, 693.50it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GRU_gate(nn.Module):\n    def __init__(self, n_features):\n\n        super(GRU_gate, self).__init__()\n        self.n_features = n_features\n\n        \"\"\"Reset Gate\"\"\"\n        self.reset_gate = nn.Sequential(\n            nn.Linear(2 * n_features, n_features),\n            nn.Sigmoid()\n        )\n        \"\"\"Update Gate\"\"\"\n        self.update_gate = nn.Sequential(\n            nn.Linear(2 * n_features, n_features),\n            nn.Sigmoid()\n        )\n        \"\"\"The output transform\"\"\"\n        self.transform = nn.Sequential(\n            nn.Linear(2 * n_features, n_features),\n            nn.Tanh()\n        )\n\n    def forward(self, h, h_in):\n        a = torch.cat((h, h_in), 1)\n\n        r = self.reset_gate(a)\n        z = self.update_gate(a)\n\n        joined_input = torch.cat((h, r * h_in), 1)\n        h_hat = self.transform(joined_input)\n\n        output = (1 - z) * h_in + z * h_hat\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:53:35.923427Z","iopub.execute_input":"2024-10-15T02:53:35.923770Z","iopub.status.idle":"2024-10-15T02:53:35.933672Z","shell.execute_reply.started":"2024-10-15T02:53:35.923738Z","shell.execute_reply":"2024-10-15T02:53:35.932679Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from torch_geometric.nn import GCNConv, GCN2Conv,GraphConv, GATConv,GATv2Conv, global_max_pool as gmp, global_add_pool as gap, \\\n    global_mean_pool as gep, global_sort_pool\nfrom torch_geometric.utils import dropout_adj\nimport torch.nn as nn\nimport torch\nfrom torch_geometric.nn import EdgeConv,RGATConv,GINConv\n\nclass Model(torch.nn.Module):\n    def __init__(self, n_output=1, num_features_pro=53, output_dim=128, \n        hidden_channels=64,dropout=0.5):\n        super(Model, self).__init__()\n        \n        self.gru_gate1 = GRU_gate(num_features_pro*4)\n        self.transform1 = nn.Linear(num_features_pro, num_features_pro*4)\n        \n        self.n_output = n_output\n        self.pro_conv1 = GATConv(num_features_pro, num_features_pro, heads=4, dropout=dropout)\n\n        self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 256)\n        self.pro_fc_g2 = torch.nn.Linear(256, output_dim) \n        \n        \n        self.pro_conv4 = GINConv(nn=nn.Sequential(\n            nn.Linear(num_features_pro, num_features_pro*2), \n            nn.ReLU(), \n            nn.Linear(num_features_pro*2, num_features_pro*2)\n        ))\n        self.pro_fc_g3 = torch.nn.Linear(num_features_pro * 2, 256)\n        self.pro_fc_g4 = torch.nn.Linear(256, output_dim)\n        \n       \n        self.pro_conv7 = EdgeConv(nn=nn.Sequential(\n            nn.Linear(num_features_pro*2, num_features_pro*2),  \n            nn.ReLU(),\n            nn.Linear(num_features_pro*2, num_features_pro*2)  \n        ))\n        self.pro_fc_g5 = torch.nn.Linear(num_features_pro*2, 256)\n        self.pro_fc_g6 = torch.nn.Linear(256, output_dim)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n        self.fc1 = nn.Linear(output_dim*3, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.out = nn.Linear(64, self.n_output)\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, data_pro):\n\n        target_x = data_pro.x \n        target_edge_index = data_pro.edge_index \n        target_weight = data_pro.edge_weight \n        target_batch = data_pro.batch\n        \n        xt = self.pro_conv1(target_x, target_edge_index)\n        xt = self.relu(xt)\n        xt = self.dropout(xt)\n        xt = gep(xt, target_batch) \n        xt = self.relu(self.pro_fc_g1(xt))\n        xt = self.dropout(xt)\n        xt = self.pro_fc_g2(xt)\n        xt = self.relu(xt)\n        xt = self.dropout(xt)\n        \n        xt2 = self.pro_conv4(target_x, target_edge_index)\n        xt2 = self.relu(xt2)\n        xt2 = self.dropout(xt2) \n        xt2 = gep(xt2, target_batch)\n        xt2 = self.relu(self.pro_fc_g3(xt2))\n        xt2 = self.dropout(xt2)\n        xt2 = self.pro_fc_g4(xt2)\n        xt2 = self.relu(xt2)\n        xt2 = self.dropout(xt2) \n        \n        \n        \n        xt3 = self.pro_conv7(target_x, target_edge_index)\n        xt3 = self.relu(xt3)\n        xt3 = self.dropout(xt3)\n        xt3 = gep(xt3, target_batch) \n        xt3 = self.relu(self.pro_fc_g5(xt3))\n        xt3 = self.dropout(xt3)\n        xt3 = self.pro_fc_g6(xt3)\n        xt3 = self.relu(xt3)\n        xt3 = self.dropout(xt3) \n        \n        xt4 = torch.cat((xt,xt2, xt3), dim=1)\n        xt4 = self.fc1(xt4)\n        xt4 = self.relu(xt4)\n        xt4 = self.dropout(xt4)\n        xt4 = self.fc2(xt4)\n        xt4 = self.relu(xt4)\n        xt4 = self.dropout(xt4)\n        out = self.sigmoid(self.out(xt4))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:53:35.946954Z","iopub.execute_input":"2024-10-15T02:53:35.947375Z","iopub.status.idle":"2024-10-15T02:53:35.973665Z","shell.execute_reply.started":"2024-10-15T02:53:35.947323Z","shell.execute_reply":"2024-10-15T02:53:35.972672Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\ndef train(model, device, train_loader, optimizer, epoch, loss_fn, TRAIN_BATCH_SIZE=512):\n    #print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    LOG_INTERVAL = 10\n    for batch_idx, data in enumerate(train_loader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        output = model(data)\n        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n        loss.backward()\n        optimizer.step()\n        if batch_idx % LOG_INTERVAL == 0:\n            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n                                                                           batch_idx * TRAIN_BATCH_SIZE,\n                                                                           len(train_loader.dataset),\n                                                                           100. * batch_idx / len(train_loader),\n                                                                           loss.item()))  \ndef predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    #print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds, output.cpu()), 0)\n            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n    return total_labels.numpy().flatten(), total_preds.numpy().flatten()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:53:35.975112Z","iopub.execute_input":"2024-10-15T02:53:35.975477Z","iopub.status.idle":"2024-10-15T02:53:35.987485Z","shell.execute_reply.started":"2024-10-15T02:53:35.975448Z","shell.execute_reply":"2024-10-15T02:53:35.986532Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score, matthews_corrcoef\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Model()\nmodel.to(device)\n\nLR = 0.001\nNUM_EPOCHS = 1000\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nmodel_file_name =  'MAGGN.model'\n\nfor epoch in range(NUM_EPOCHS):\n    train(model, device, train_loader, optimizer, epoch + 1, loss_fn, TRAIN_BATCH_SIZE)\ntorch.save(model.state_dict(), model_file_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T02:53:35.988943Z","iopub.execute_input":"2024-10-15T02:53:35.989662Z","iopub.status.idle":"2024-10-15T03:16:41.363937Z","shell.execute_reply.started":"2024-10-15T02:53:35.989622Z","shell.execute_reply":"2024-10-15T03:16:41.362846Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Train epoch: 1 [0/5139 (0%)]\tLoss: 0.702987\nTrain epoch: 1 [2560/5139 (48%)]\tLoss: 0.594824\nTrain epoch: 1 [5120/5139 (95%)]\tLoss: 0.674819\nTrain epoch: 2 [0/5139 (0%)]\tLoss: 0.663341\nTrain epoch: 2 [2560/5139 (48%)]\tLoss: 0.609671\nTrain epoch: 2 [5120/5139 (95%)]\tLoss: 0.601156\nTrain epoch: 3 [0/5139 (0%)]\tLoss: 0.640347\nTrain epoch: 3 [2560/5139 (48%)]\tLoss: 0.638651\nTrain epoch: 3 [5120/5139 (95%)]\tLoss: 0.663662\nTrain epoch: 4 [0/5139 (0%)]\tLoss: 0.634127\nTrain epoch: 4 [2560/5139 (48%)]\tLoss: 0.585691\nTrain epoch: 4 [5120/5139 (95%)]\tLoss: 0.645940\nTrain epoch: 5 [0/5139 (0%)]\tLoss: 0.479800\nTrain epoch: 5 [2560/5139 (48%)]\tLoss: 0.567698\nTrain epoch: 5 [5120/5139 (95%)]\tLoss: 0.391607\nTrain epoch: 6 [0/5139 (0%)]\tLoss: 0.449993\nTrain epoch: 6 [2560/5139 (48%)]\tLoss: 0.504042\nTrain epoch: 6 [5120/5139 (95%)]\tLoss: 0.354972\nTrain epoch: 7 [0/5139 (0%)]\tLoss: 0.392825\nTrain epoch: 7 [2560/5139 (48%)]\tLoss: 0.377775\nTrain epoch: 7 [5120/5139 (95%)]\tLoss: 0.347438\nTrain epoch: 8 [0/5139 (0%)]\tLoss: 0.436098\nTrain epoch: 8 [2560/5139 (48%)]\tLoss: 0.322201\nTrain epoch: 8 [5120/5139 (95%)]\tLoss: 0.304756\nTrain epoch: 9 [0/5139 (0%)]\tLoss: 0.330100\nTrain epoch: 9 [2560/5139 (48%)]\tLoss: 0.303561\nTrain epoch: 9 [5120/5139 (95%)]\tLoss: 0.183226\nTrain epoch: 10 [0/5139 (0%)]\tLoss: 0.347459\nTrain epoch: 10 [2560/5139 (48%)]\tLoss: 0.318430\nTrain epoch: 10 [5120/5139 (95%)]\tLoss: 0.191869\nTrain epoch: 11 [0/5139 (0%)]\tLoss: 0.306947\nTrain epoch: 11 [2560/5139 (48%)]\tLoss: 0.322458\nTrain epoch: 11 [5120/5139 (95%)]\tLoss: 0.346945\nTrain epoch: 12 [0/5139 (0%)]\tLoss: 0.380277\nTrain epoch: 12 [2560/5139 (48%)]\tLoss: 0.373587\nTrain epoch: 12 [5120/5139 (95%)]\tLoss: 0.448318\nTrain epoch: 13 [0/5139 (0%)]\tLoss: 0.391504\nTrain epoch: 13 [2560/5139 (48%)]\tLoss: 0.328478\nTrain epoch: 13 [5120/5139 (95%)]\tLoss: 0.291782\nTrain epoch: 14 [0/5139 (0%)]\tLoss: 0.282355\nTrain epoch: 14 [2560/5139 (48%)]\tLoss: 0.297657\nTrain epoch: 14 [5120/5139 (95%)]\tLoss: 0.411653\nTrain epoch: 15 [0/5139 (0%)]\tLoss: 0.277799\nTrain epoch: 15 [2560/5139 (48%)]\tLoss: 0.307545\nTrain epoch: 15 [5120/5139 (95%)]\tLoss: 0.569320\nTrain epoch: 16 [0/5139 (0%)]\tLoss: 0.448708\nTrain epoch: 16 [2560/5139 (48%)]\tLoss: 0.310851\nTrain epoch: 16 [5120/5139 (95%)]\tLoss: 0.220350\nTrain epoch: 17 [0/5139 (0%)]\tLoss: 0.424569\nTrain epoch: 17 [2560/5139 (48%)]\tLoss: 0.249062\nTrain epoch: 17 [5120/5139 (95%)]\tLoss: 0.315474\nTrain epoch: 18 [0/5139 (0%)]\tLoss: 0.253879\nTrain epoch: 18 [2560/5139 (48%)]\tLoss: 0.286800\nTrain epoch: 18 [5120/5139 (95%)]\tLoss: 0.529104\nTrain epoch: 19 [0/5139 (0%)]\tLoss: 0.317995\nTrain epoch: 19 [2560/5139 (48%)]\tLoss: 0.342644\nTrain epoch: 19 [5120/5139 (95%)]\tLoss: 0.327552\nTrain epoch: 20 [0/5139 (0%)]\tLoss: 0.301055\nTrain epoch: 20 [2560/5139 (48%)]\tLoss: 0.341793\nTrain epoch: 20 [5120/5139 (95%)]\tLoss: 0.413713\nTrain epoch: 21 [0/5139 (0%)]\tLoss: 0.303465\nTrain epoch: 21 [2560/5139 (48%)]\tLoss: 0.254349\nTrain epoch: 21 [5120/5139 (95%)]\tLoss: 0.188554\nTrain epoch: 22 [0/5139 (0%)]\tLoss: 0.268985\nTrain epoch: 22 [2560/5139 (48%)]\tLoss: 0.254251\nTrain epoch: 22 [5120/5139 (95%)]\tLoss: 0.097422\nTrain epoch: 23 [0/5139 (0%)]\tLoss: 0.307921\nTrain epoch: 23 [2560/5139 (48%)]\tLoss: 0.299837\nTrain epoch: 23 [5120/5139 (95%)]\tLoss: 0.246796\nTrain epoch: 24 [0/5139 (0%)]\tLoss: 0.234611\nTrain epoch: 24 [2560/5139 (48%)]\tLoss: 0.288147\nTrain epoch: 24 [5120/5139 (95%)]\tLoss: 0.223844\nTrain epoch: 25 [0/5139 (0%)]\tLoss: 0.288873\nTrain epoch: 25 [2560/5139 (48%)]\tLoss: 0.275228\nTrain epoch: 25 [5120/5139 (95%)]\tLoss: 0.326725\nTrain epoch: 26 [0/5139 (0%)]\tLoss: 0.260731\nTrain epoch: 26 [2560/5139 (48%)]\tLoss: 0.224603\nTrain epoch: 26 [5120/5139 (95%)]\tLoss: 0.142636\nTrain epoch: 27 [0/5139 (0%)]\tLoss: 0.305198\nTrain epoch: 27 [2560/5139 (48%)]\tLoss: 0.263857\nTrain epoch: 27 [5120/5139 (95%)]\tLoss: 0.479366\nTrain epoch: 28 [0/5139 (0%)]\tLoss: 0.259330\nTrain epoch: 28 [2560/5139 (48%)]\tLoss: 0.244615\nTrain epoch: 28 [5120/5139 (95%)]\tLoss: 0.299824\nTrain epoch: 29 [0/5139 (0%)]\tLoss: 0.378355\nTrain epoch: 29 [2560/5139 (48%)]\tLoss: 0.323328\nTrain epoch: 29 [5120/5139 (95%)]\tLoss: 0.123313\nTrain epoch: 30 [0/5139 (0%)]\tLoss: 0.247062\nTrain epoch: 30 [2560/5139 (48%)]\tLoss: 0.250410\nTrain epoch: 30 [5120/5139 (95%)]\tLoss: 0.176754\nTrain epoch: 31 [0/5139 (0%)]\tLoss: 0.233407\nTrain epoch: 31 [2560/5139 (48%)]\tLoss: 0.300312\nTrain epoch: 31 [5120/5139 (95%)]\tLoss: 0.476700\nTrain epoch: 32 [0/5139 (0%)]\tLoss: 0.242859\nTrain epoch: 32 [2560/5139 (48%)]\tLoss: 0.301277\nTrain epoch: 32 [5120/5139 (95%)]\tLoss: 0.120299\nTrain epoch: 33 [0/5139 (0%)]\tLoss: 0.287271\nTrain epoch: 33 [2560/5139 (48%)]\tLoss: 0.228801\nTrain epoch: 33 [5120/5139 (95%)]\tLoss: 0.241228\nTrain epoch: 34 [0/5139 (0%)]\tLoss: 0.274341\nTrain epoch: 34 [2560/5139 (48%)]\tLoss: 0.224577\nTrain epoch: 34 [5120/5139 (95%)]\tLoss: 0.396108\nTrain epoch: 35 [0/5139 (0%)]\tLoss: 0.447748\nTrain epoch: 35 [2560/5139 (48%)]\tLoss: 0.266154\nTrain epoch: 35 [5120/5139 (95%)]\tLoss: 0.610863\nTrain epoch: 36 [0/5139 (0%)]\tLoss: 0.263092\nTrain epoch: 36 [2560/5139 (48%)]\tLoss: 0.285272\nTrain epoch: 36 [5120/5139 (95%)]\tLoss: 0.143144\nTrain epoch: 37 [0/5139 (0%)]\tLoss: 0.281639\nTrain epoch: 37 [2560/5139 (48%)]\tLoss: 0.241065\nTrain epoch: 37 [5120/5139 (95%)]\tLoss: 0.315561\nTrain epoch: 38 [0/5139 (0%)]\tLoss: 0.255015\nTrain epoch: 38 [2560/5139 (48%)]\tLoss: 0.247931\nTrain epoch: 38 [5120/5139 (95%)]\tLoss: 0.426739\nTrain epoch: 39 [0/5139 (0%)]\tLoss: 0.354218\nTrain epoch: 39 [2560/5139 (48%)]\tLoss: 0.334806\nTrain epoch: 39 [5120/5139 (95%)]\tLoss: 0.287874\nTrain epoch: 40 [0/5139 (0%)]\tLoss: 0.310826\nTrain epoch: 40 [2560/5139 (48%)]\tLoss: 0.236787\nTrain epoch: 40 [5120/5139 (95%)]\tLoss: 0.364285\nTrain epoch: 41 [0/5139 (0%)]\tLoss: 0.229727\nTrain epoch: 41 [2560/5139 (48%)]\tLoss: 0.258520\nTrain epoch: 41 [5120/5139 (95%)]\tLoss: 0.117456\nTrain epoch: 42 [0/5139 (0%)]\tLoss: 0.230563\nTrain epoch: 42 [2560/5139 (48%)]\tLoss: 0.201589\nTrain epoch: 42 [5120/5139 (95%)]\tLoss: 0.483123\nTrain epoch: 43 [0/5139 (0%)]\tLoss: 0.272418\nTrain epoch: 43 [2560/5139 (48%)]\tLoss: 0.281276\nTrain epoch: 43 [5120/5139 (95%)]\tLoss: 0.124062\nTrain epoch: 44 [0/5139 (0%)]\tLoss: 0.228660\nTrain epoch: 44 [2560/5139 (48%)]\tLoss: 0.251615\nTrain epoch: 44 [5120/5139 (95%)]\tLoss: 0.292528\nTrain epoch: 45 [0/5139 (0%)]\tLoss: 0.248665\nTrain epoch: 45 [2560/5139 (48%)]\tLoss: 0.252320\nTrain epoch: 45 [5120/5139 (95%)]\tLoss: 0.282492\nTrain epoch: 46 [0/5139 (0%)]\tLoss: 0.234097\nTrain epoch: 46 [2560/5139 (48%)]\tLoss: 0.270584\nTrain epoch: 46 [5120/5139 (95%)]\tLoss: 0.167488\nTrain epoch: 47 [0/5139 (0%)]\tLoss: 0.232300\nTrain epoch: 47 [2560/5139 (48%)]\tLoss: 0.262828\nTrain epoch: 47 [5120/5139 (95%)]\tLoss: 0.238173\nTrain epoch: 48 [0/5139 (0%)]\tLoss: 0.219582\nTrain epoch: 48 [2560/5139 (48%)]\tLoss: 0.207880\nTrain epoch: 48 [5120/5139 (95%)]\tLoss: 0.406572\nTrain epoch: 49 [0/5139 (0%)]\tLoss: 0.227867\nTrain epoch: 49 [2560/5139 (48%)]\tLoss: 0.261866\nTrain epoch: 49 [5120/5139 (95%)]\tLoss: 0.426798\nTrain epoch: 50 [0/5139 (0%)]\tLoss: 0.324704\nTrain epoch: 50 [2560/5139 (48%)]\tLoss: 0.274113\nTrain epoch: 50 [5120/5139 (95%)]\tLoss: 0.253174\nTrain epoch: 51 [0/5139 (0%)]\tLoss: 0.283750\nTrain epoch: 51 [2560/5139 (48%)]\tLoss: 0.289977\nTrain epoch: 51 [5120/5139 (95%)]\tLoss: 0.465035\nTrain epoch: 52 [0/5139 (0%)]\tLoss: 0.163943\nTrain epoch: 52 [2560/5139 (48%)]\tLoss: 0.173101\nTrain epoch: 52 [5120/5139 (95%)]\tLoss: 0.411204\nTrain epoch: 53 [0/5139 (0%)]\tLoss: 0.201655\nTrain epoch: 53 [2560/5139 (48%)]\tLoss: 0.223746\nTrain epoch: 53 [5120/5139 (95%)]\tLoss: 0.092301\nTrain epoch: 54 [0/5139 (0%)]\tLoss: 0.249618\nTrain epoch: 54 [2560/5139 (48%)]\tLoss: 0.240536\nTrain epoch: 54 [5120/5139 (95%)]\tLoss: 0.712846\nTrain epoch: 55 [0/5139 (0%)]\tLoss: 0.268066\nTrain epoch: 55 [2560/5139 (48%)]\tLoss: 0.291301\nTrain epoch: 55 [5120/5139 (95%)]\tLoss: 0.242483\nTrain epoch: 56 [0/5139 (0%)]\tLoss: 0.203442\nTrain epoch: 56 [2560/5139 (48%)]\tLoss: 0.163772\nTrain epoch: 56 [5120/5139 (95%)]\tLoss: 0.304759\nTrain epoch: 57 [0/5139 (0%)]\tLoss: 0.212371\nTrain epoch: 57 [2560/5139 (48%)]\tLoss: 0.218839\nTrain epoch: 57 [5120/5139 (95%)]\tLoss: 0.030168\nTrain epoch: 58 [0/5139 (0%)]\tLoss: 0.237666\nTrain epoch: 58 [2560/5139 (48%)]\tLoss: 0.264078\nTrain epoch: 58 [5120/5139 (95%)]\tLoss: 0.056650\nTrain epoch: 59 [0/5139 (0%)]\tLoss: 0.247523\nTrain epoch: 59 [2560/5139 (48%)]\tLoss: 0.227230\nTrain epoch: 59 [5120/5139 (95%)]\tLoss: 0.322936\nTrain epoch: 60 [0/5139 (0%)]\tLoss: 0.220780\nTrain epoch: 60 [2560/5139 (48%)]\tLoss: 0.247251\nTrain epoch: 60 [5120/5139 (95%)]\tLoss: 0.255006\nTrain epoch: 61 [0/5139 (0%)]\tLoss: 0.213297\nTrain epoch: 61 [2560/5139 (48%)]\tLoss: 0.254956\nTrain epoch: 61 [5120/5139 (95%)]\tLoss: 0.334531\nTrain epoch: 62 [0/5139 (0%)]\tLoss: 0.479032\nTrain epoch: 62 [2560/5139 (48%)]\tLoss: 0.242867\nTrain epoch: 62 [5120/5139 (95%)]\tLoss: 0.238287\nTrain epoch: 63 [0/5139 (0%)]\tLoss: 0.272084\nTrain epoch: 63 [2560/5139 (48%)]\tLoss: 0.173353\nTrain epoch: 63 [5120/5139 (95%)]\tLoss: 0.339040\nTrain epoch: 64 [0/5139 (0%)]\tLoss: 0.204063\nTrain epoch: 64 [2560/5139 (48%)]\tLoss: 0.309916\nTrain epoch: 64 [5120/5139 (95%)]\tLoss: 0.114307\nTrain epoch: 65 [0/5139 (0%)]\tLoss: 0.231906\nTrain epoch: 65 [2560/5139 (48%)]\tLoss: 0.251650\nTrain epoch: 65 [5120/5139 (95%)]\tLoss: 0.128607\nTrain epoch: 66 [0/5139 (0%)]\tLoss: 0.186347\nTrain epoch: 66 [2560/5139 (48%)]\tLoss: 0.237569\nTrain epoch: 66 [5120/5139 (95%)]\tLoss: 0.202412\nTrain epoch: 67 [0/5139 (0%)]\tLoss: 0.209927\nTrain epoch: 67 [2560/5139 (48%)]\tLoss: 0.201671\nTrain epoch: 67 [5120/5139 (95%)]\tLoss: 0.508863\nTrain epoch: 68 [0/5139 (0%)]\tLoss: 0.181329\nTrain epoch: 68 [2560/5139 (48%)]\tLoss: 0.241378\nTrain epoch: 68 [5120/5139 (95%)]\tLoss: 0.143447\nTrain epoch: 69 [0/5139 (0%)]\tLoss: 0.196099\nTrain epoch: 69 [2560/5139 (48%)]\tLoss: 0.198715\nTrain epoch: 69 [5120/5139 (95%)]\tLoss: 0.270188\nTrain epoch: 70 [0/5139 (0%)]\tLoss: 0.181245\nTrain epoch: 70 [2560/5139 (48%)]\tLoss: 0.167129\nTrain epoch: 70 [5120/5139 (95%)]\tLoss: 0.146768\nTrain epoch: 71 [0/5139 (0%)]\tLoss: 0.189965\nTrain epoch: 71 [2560/5139 (48%)]\tLoss: 0.226980\nTrain epoch: 71 [5120/5139 (95%)]\tLoss: 0.187974\nTrain epoch: 72 [0/5139 (0%)]\tLoss: 0.289465\nTrain epoch: 72 [2560/5139 (48%)]\tLoss: 0.229790\nTrain epoch: 72 [5120/5139 (95%)]\tLoss: 0.145424\nTrain epoch: 73 [0/5139 (0%)]\tLoss: 0.221309\nTrain epoch: 73 [2560/5139 (48%)]\tLoss: 0.265678\nTrain epoch: 73 [5120/5139 (95%)]\tLoss: 0.066265\nTrain epoch: 74 [0/5139 (0%)]\tLoss: 0.164191\nTrain epoch: 74 [2560/5139 (48%)]\tLoss: 0.207107\nTrain epoch: 74 [5120/5139 (95%)]\tLoss: 0.236173\nTrain epoch: 75 [0/5139 (0%)]\tLoss: 0.298372\nTrain epoch: 75 [2560/5139 (48%)]\tLoss: 0.169516\nTrain epoch: 75 [5120/5139 (95%)]\tLoss: 0.129221\nTrain epoch: 76 [0/5139 (0%)]\tLoss: 0.197663\nTrain epoch: 76 [2560/5139 (48%)]\tLoss: 0.193213\nTrain epoch: 76 [5120/5139 (95%)]\tLoss: 0.330149\nTrain epoch: 77 [0/5139 (0%)]\tLoss: 0.260081\nTrain epoch: 77 [2560/5139 (48%)]\tLoss: 0.263395\nTrain epoch: 77 [5120/5139 (95%)]\tLoss: 0.164931\nTrain epoch: 78 [0/5139 (0%)]\tLoss: 0.194122\nTrain epoch: 78 [2560/5139 (48%)]\tLoss: 0.189426\nTrain epoch: 78 [5120/5139 (95%)]\tLoss: 0.167823\nTrain epoch: 79 [0/5139 (0%)]\tLoss: 0.165147\nTrain epoch: 79 [2560/5139 (48%)]\tLoss: 0.189831\nTrain epoch: 79 [5120/5139 (95%)]\tLoss: 0.066775\nTrain epoch: 80 [0/5139 (0%)]\tLoss: 0.266205\nTrain epoch: 80 [2560/5139 (48%)]\tLoss: 0.180242\nTrain epoch: 80 [5120/5139 (95%)]\tLoss: 0.089086\nTrain epoch: 81 [0/5139 (0%)]\tLoss: 0.201486\nTrain epoch: 81 [2560/5139 (48%)]\tLoss: 0.314084\nTrain epoch: 81 [5120/5139 (95%)]\tLoss: 0.087131\nTrain epoch: 82 [0/5139 (0%)]\tLoss: 0.194402\nTrain epoch: 82 [2560/5139 (48%)]\tLoss: 0.155005\nTrain epoch: 82 [5120/5139 (95%)]\tLoss: 0.108246\nTrain epoch: 83 [0/5139 (0%)]\tLoss: 0.318746\nTrain epoch: 83 [2560/5139 (48%)]\tLoss: 0.155705\nTrain epoch: 83 [5120/5139 (95%)]\tLoss: 0.250818\nTrain epoch: 84 [0/5139 (0%)]\tLoss: 0.185999\nTrain epoch: 84 [2560/5139 (48%)]\tLoss: 0.179181\nTrain epoch: 84 [5120/5139 (95%)]\tLoss: 0.217250\nTrain epoch: 85 [0/5139 (0%)]\tLoss: 0.190005\nTrain epoch: 85 [2560/5139 (48%)]\tLoss: 0.184253\nTrain epoch: 85 [5120/5139 (95%)]\tLoss: 0.542786\nTrain epoch: 86 [0/5139 (0%)]\tLoss: 0.210978\nTrain epoch: 86 [2560/5139 (48%)]\tLoss: 0.178617\nTrain epoch: 86 [5120/5139 (95%)]\tLoss: 0.113808\nTrain epoch: 87 [0/5139 (0%)]\tLoss: 0.265427\nTrain epoch: 87 [2560/5139 (48%)]\tLoss: 0.213145\nTrain epoch: 87 [5120/5139 (95%)]\tLoss: 0.169153\nTrain epoch: 88 [0/5139 (0%)]\tLoss: 0.121632\nTrain epoch: 88 [2560/5139 (48%)]\tLoss: 0.143200\nTrain epoch: 88 [5120/5139 (95%)]\tLoss: 0.211533\nTrain epoch: 89 [0/5139 (0%)]\tLoss: 0.183080\nTrain epoch: 89 [2560/5139 (48%)]\tLoss: 0.130111\nTrain epoch: 89 [5120/5139 (95%)]\tLoss: 0.316026\nTrain epoch: 90 [0/5139 (0%)]\tLoss: 0.297941\nTrain epoch: 90 [2560/5139 (48%)]\tLoss: 0.143771\nTrain epoch: 90 [5120/5139 (95%)]\tLoss: 0.039032\nTrain epoch: 91 [0/5139 (0%)]\tLoss: 0.174926\nTrain epoch: 91 [2560/5139 (48%)]\tLoss: 0.209267\nTrain epoch: 91 [5120/5139 (95%)]\tLoss: 0.107888\nTrain epoch: 92 [0/5139 (0%)]\tLoss: 0.158064\nTrain epoch: 92 [2560/5139 (48%)]\tLoss: 0.154840\nTrain epoch: 92 [5120/5139 (95%)]\tLoss: 0.082820\nTrain epoch: 93 [0/5139 (0%)]\tLoss: 0.188314\nTrain epoch: 93 [2560/5139 (48%)]\tLoss: 0.179829\nTrain epoch: 93 [5120/5139 (95%)]\tLoss: 0.151458\nTrain epoch: 94 [0/5139 (0%)]\tLoss: 0.149994\nTrain epoch: 94 [2560/5139 (48%)]\tLoss: 0.159715\nTrain epoch: 94 [5120/5139 (95%)]\tLoss: 0.203825\nTrain epoch: 95 [0/5139 (0%)]\tLoss: 0.105522\nTrain epoch: 95 [2560/5139 (48%)]\tLoss: 0.243270\nTrain epoch: 95 [5120/5139 (95%)]\tLoss: 0.172801\nTrain epoch: 96 [0/5139 (0%)]\tLoss: 0.182907\nTrain epoch: 96 [2560/5139 (48%)]\tLoss: 0.208425\nTrain epoch: 96 [5120/5139 (95%)]\tLoss: 0.111655\nTrain epoch: 97 [0/5139 (0%)]\tLoss: 0.150705\nTrain epoch: 97 [2560/5139 (48%)]\tLoss: 0.162086\nTrain epoch: 97 [5120/5139 (95%)]\tLoss: 0.029498\nTrain epoch: 98 [0/5139 (0%)]\tLoss: 0.135126\nTrain epoch: 98 [2560/5139 (48%)]\tLoss: 0.250566\nTrain epoch: 98 [5120/5139 (95%)]\tLoss: 0.125166\nTrain epoch: 99 [0/5139 (0%)]\tLoss: 0.200107\nTrain epoch: 99 [2560/5139 (48%)]\tLoss: 0.182301\nTrain epoch: 99 [5120/5139 (95%)]\tLoss: 0.040429\nTrain epoch: 100 [0/5139 (0%)]\tLoss: 0.185398\nTrain epoch: 100 [2560/5139 (48%)]\tLoss: 0.203910\nTrain epoch: 100 [5120/5139 (95%)]\tLoss: 0.310086\nTrain epoch: 101 [0/5139 (0%)]\tLoss: 0.116130\nTrain epoch: 101 [2560/5139 (48%)]\tLoss: 0.136866\nTrain epoch: 101 [5120/5139 (95%)]\tLoss: 0.212012\nTrain epoch: 102 [0/5139 (0%)]\tLoss: 0.163760\nTrain epoch: 102 [2560/5139 (48%)]\tLoss: 0.260785\nTrain epoch: 102 [5120/5139 (95%)]\tLoss: 0.124154\nTrain epoch: 103 [0/5139 (0%)]\tLoss: 0.160651\nTrain epoch: 103 [2560/5139 (48%)]\tLoss: 0.140656\nTrain epoch: 103 [5120/5139 (95%)]\tLoss: 0.081394\nTrain epoch: 104 [0/5139 (0%)]\tLoss: 0.153024\nTrain epoch: 104 [2560/5139 (48%)]\tLoss: 0.134007\nTrain epoch: 104 [5120/5139 (95%)]\tLoss: 0.089769\nTrain epoch: 105 [0/5139 (0%)]\tLoss: 0.098679\nTrain epoch: 105 [2560/5139 (48%)]\tLoss: 0.180379\nTrain epoch: 105 [5120/5139 (95%)]\tLoss: 0.250965\nTrain epoch: 106 [0/5139 (0%)]\tLoss: 0.234488\nTrain epoch: 106 [2560/5139 (48%)]\tLoss: 0.150831\nTrain epoch: 106 [5120/5139 (95%)]\tLoss: 0.121397\nTrain epoch: 107 [0/5139 (0%)]\tLoss: 0.171913\nTrain epoch: 107 [2560/5139 (48%)]\tLoss: 0.138253\nTrain epoch: 107 [5120/5139 (95%)]\tLoss: 0.086024\nTrain epoch: 108 [0/5139 (0%)]\tLoss: 0.277811\nTrain epoch: 108 [2560/5139 (48%)]\tLoss: 0.280351\nTrain epoch: 108 [5120/5139 (95%)]\tLoss: 0.272391\nTrain epoch: 109 [0/5139 (0%)]\tLoss: 0.211264\nTrain epoch: 109 [2560/5139 (48%)]\tLoss: 0.165665\nTrain epoch: 109 [5120/5139 (95%)]\tLoss: 0.033646\nTrain epoch: 110 [0/5139 (0%)]\tLoss: 0.155531\nTrain epoch: 110 [2560/5139 (48%)]\tLoss: 0.141226\nTrain epoch: 110 [5120/5139 (95%)]\tLoss: 0.052132\nTrain epoch: 111 [0/5139 (0%)]\tLoss: 0.224552\nTrain epoch: 111 [2560/5139 (48%)]\tLoss: 0.079005\nTrain epoch: 111 [5120/5139 (95%)]\tLoss: 0.140185\nTrain epoch: 112 [0/5139 (0%)]\tLoss: 0.120125\nTrain epoch: 112 [2560/5139 (48%)]\tLoss: 0.114398\nTrain epoch: 112 [5120/5139 (95%)]\tLoss: 0.104295\nTrain epoch: 113 [0/5139 (0%)]\tLoss: 0.306611\nTrain epoch: 113 [2560/5139 (48%)]\tLoss: 0.189670\nTrain epoch: 113 [5120/5139 (95%)]\tLoss: 0.205545\nTrain epoch: 114 [0/5139 (0%)]\tLoss: 0.180343\nTrain epoch: 114 [2560/5139 (48%)]\tLoss: 0.171583\nTrain epoch: 114 [5120/5139 (95%)]\tLoss: 0.043219\nTrain epoch: 115 [0/5139 (0%)]\tLoss: 0.179555\nTrain epoch: 115 [2560/5139 (48%)]\tLoss: 0.180113\nTrain epoch: 115 [5120/5139 (95%)]\tLoss: 0.167089\nTrain epoch: 116 [0/5139 (0%)]\tLoss: 0.187798\nTrain epoch: 116 [2560/5139 (48%)]\tLoss: 0.119463\nTrain epoch: 116 [5120/5139 (95%)]\tLoss: 0.038280\nTrain epoch: 117 [0/5139 (0%)]\tLoss: 0.130100\nTrain epoch: 117 [2560/5139 (48%)]\tLoss: 0.180993\nTrain epoch: 117 [5120/5139 (95%)]\tLoss: 0.103980\nTrain epoch: 118 [0/5139 (0%)]\tLoss: 0.193013\nTrain epoch: 118 [2560/5139 (48%)]\tLoss: 0.125874\nTrain epoch: 118 [5120/5139 (95%)]\tLoss: 0.169440\nTrain epoch: 119 [0/5139 (0%)]\tLoss: 0.102538\nTrain epoch: 119 [2560/5139 (48%)]\tLoss: 0.135674\nTrain epoch: 119 [5120/5139 (95%)]\tLoss: 0.352736\nTrain epoch: 120 [0/5139 (0%)]\tLoss: 0.144858\nTrain epoch: 120 [2560/5139 (48%)]\tLoss: 0.092824\nTrain epoch: 120 [5120/5139 (95%)]\tLoss: 0.065059\nTrain epoch: 121 [0/5139 (0%)]\tLoss: 0.169081\nTrain epoch: 121 [2560/5139 (48%)]\tLoss: 0.162432\nTrain epoch: 121 [5120/5139 (95%)]\tLoss: 0.059145\nTrain epoch: 122 [0/5139 (0%)]\tLoss: 0.169200\nTrain epoch: 122 [2560/5139 (48%)]\tLoss: 0.090607\nTrain epoch: 122 [5120/5139 (95%)]\tLoss: 0.072132\nTrain epoch: 123 [0/5139 (0%)]\tLoss: 0.216769\nTrain epoch: 123 [2560/5139 (48%)]\tLoss: 0.159886\nTrain epoch: 123 [5120/5139 (95%)]\tLoss: 0.119719\nTrain epoch: 124 [0/5139 (0%)]\tLoss: 0.149214\nTrain epoch: 124 [2560/5139 (48%)]\tLoss: 0.147357\nTrain epoch: 124 [5120/5139 (95%)]\tLoss: 0.181263\nTrain epoch: 125 [0/5139 (0%)]\tLoss: 0.230454\nTrain epoch: 125 [2560/5139 (48%)]\tLoss: 0.145750\nTrain epoch: 125 [5120/5139 (95%)]\tLoss: 0.081858\nTrain epoch: 126 [0/5139 (0%)]\tLoss: 0.167659\nTrain epoch: 126 [2560/5139 (48%)]\tLoss: 0.145003\nTrain epoch: 126 [5120/5139 (95%)]\tLoss: 0.031978\nTrain epoch: 127 [0/5139 (0%)]\tLoss: 0.168597\nTrain epoch: 127 [2560/5139 (48%)]\tLoss: 0.205329\nTrain epoch: 127 [5120/5139 (95%)]\tLoss: 0.190386\nTrain epoch: 128 [0/5139 (0%)]\tLoss: 0.133194\nTrain epoch: 128 [2560/5139 (48%)]\tLoss: 0.105305\nTrain epoch: 128 [5120/5139 (95%)]\tLoss: 0.046428\nTrain epoch: 129 [0/5139 (0%)]\tLoss: 0.155906\nTrain epoch: 129 [2560/5139 (48%)]\tLoss: 0.102961\nTrain epoch: 129 [5120/5139 (95%)]\tLoss: 0.021017\nTrain epoch: 130 [0/5139 (0%)]\tLoss: 0.143085\nTrain epoch: 130 [2560/5139 (48%)]\tLoss: 0.138692\nTrain epoch: 130 [5120/5139 (95%)]\tLoss: 0.383565\nTrain epoch: 131 [0/5139 (0%)]\tLoss: 0.154199\nTrain epoch: 131 [2560/5139 (48%)]\tLoss: 0.156097\nTrain epoch: 131 [5120/5139 (95%)]\tLoss: 0.059255\nTrain epoch: 132 [0/5139 (0%)]\tLoss: 0.124666\nTrain epoch: 132 [2560/5139 (48%)]\tLoss: 0.076096\nTrain epoch: 132 [5120/5139 (95%)]\tLoss: 0.095469\nTrain epoch: 133 [0/5139 (0%)]\tLoss: 0.127704\nTrain epoch: 133 [2560/5139 (48%)]\tLoss: 0.127903\nTrain epoch: 133 [5120/5139 (95%)]\tLoss: 0.205252\nTrain epoch: 134 [0/5139 (0%)]\tLoss: 0.189346\nTrain epoch: 134 [2560/5139 (48%)]\tLoss: 0.062599\nTrain epoch: 134 [5120/5139 (95%)]\tLoss: 0.059022\nTrain epoch: 135 [0/5139 (0%)]\tLoss: 0.113946\nTrain epoch: 135 [2560/5139 (48%)]\tLoss: 0.097687\nTrain epoch: 135 [5120/5139 (95%)]\tLoss: 0.108500\nTrain epoch: 136 [0/5139 (0%)]\tLoss: 0.151380\nTrain epoch: 136 [2560/5139 (48%)]\tLoss: 0.145955\nTrain epoch: 136 [5120/5139 (95%)]\tLoss: 0.033613\nTrain epoch: 137 [0/5139 (0%)]\tLoss: 0.071437\nTrain epoch: 137 [2560/5139 (48%)]\tLoss: 0.151127\nTrain epoch: 137 [5120/5139 (95%)]\tLoss: 0.066989\nTrain epoch: 138 [0/5139 (0%)]\tLoss: 0.117470\nTrain epoch: 138 [2560/5139 (48%)]\tLoss: 0.092275\nTrain epoch: 138 [5120/5139 (95%)]\tLoss: 0.250826\nTrain epoch: 139 [0/5139 (0%)]\tLoss: 0.076375\nTrain epoch: 139 [2560/5139 (48%)]\tLoss: 0.078654\nTrain epoch: 139 [5120/5139 (95%)]\tLoss: 0.072288\nTrain epoch: 140 [0/5139 (0%)]\tLoss: 0.106071\nTrain epoch: 140 [2560/5139 (48%)]\tLoss: 0.150398\nTrain epoch: 140 [5120/5139 (95%)]\tLoss: 0.312718\nTrain epoch: 141 [0/5139 (0%)]\tLoss: 0.133686\nTrain epoch: 141 [2560/5139 (48%)]\tLoss: 0.080091\nTrain epoch: 141 [5120/5139 (95%)]\tLoss: 0.013713\nTrain epoch: 142 [0/5139 (0%)]\tLoss: 0.064313\nTrain epoch: 142 [2560/5139 (48%)]\tLoss: 0.092385\nTrain epoch: 142 [5120/5139 (95%)]\tLoss: 0.049579\nTrain epoch: 143 [0/5139 (0%)]\tLoss: 0.135670\nTrain epoch: 143 [2560/5139 (48%)]\tLoss: 0.114433\nTrain epoch: 143 [5120/5139 (95%)]\tLoss: 0.039325\nTrain epoch: 144 [0/5139 (0%)]\tLoss: 0.108802\nTrain epoch: 144 [2560/5139 (48%)]\tLoss: 0.062062\nTrain epoch: 144 [5120/5139 (95%)]\tLoss: 0.669985\nTrain epoch: 145 [0/5139 (0%)]\tLoss: 0.957176\nTrain epoch: 145 [2560/5139 (48%)]\tLoss: 0.315568\nTrain epoch: 145 [5120/5139 (95%)]\tLoss: 0.097821\nTrain epoch: 146 [0/5139 (0%)]\tLoss: 0.143745\nTrain epoch: 146 [2560/5139 (48%)]\tLoss: 0.210700\nTrain epoch: 146 [5120/5139 (95%)]\tLoss: 0.037984\nTrain epoch: 147 [0/5139 (0%)]\tLoss: 0.171602\nTrain epoch: 147 [2560/5139 (48%)]\tLoss: 0.200517\nTrain epoch: 147 [5120/5139 (95%)]\tLoss: 0.081661\nTrain epoch: 148 [0/5139 (0%)]\tLoss: 0.147283\nTrain epoch: 148 [2560/5139 (48%)]\tLoss: 0.115836\nTrain epoch: 148 [5120/5139 (95%)]\tLoss: 0.126679\nTrain epoch: 149 [0/5139 (0%)]\tLoss: 0.132861\nTrain epoch: 149 [2560/5139 (48%)]\tLoss: 0.140910\nTrain epoch: 149 [5120/5139 (95%)]\tLoss: 0.085577\nTrain epoch: 150 [0/5139 (0%)]\tLoss: 0.167570\nTrain epoch: 150 [2560/5139 (48%)]\tLoss: 0.107371\nTrain epoch: 150 [5120/5139 (95%)]\tLoss: 0.022436\nTrain epoch: 151 [0/5139 (0%)]\tLoss: 0.113238\nTrain epoch: 151 [2560/5139 (48%)]\tLoss: 0.136237\nTrain epoch: 151 [5120/5139 (95%)]\tLoss: 0.101270\nTrain epoch: 152 [0/5139 (0%)]\tLoss: 0.090780\nTrain epoch: 152 [2560/5139 (48%)]\tLoss: 0.106636\nTrain epoch: 152 [5120/5139 (95%)]\tLoss: 0.183851\nTrain epoch: 153 [0/5139 (0%)]\tLoss: 0.126815\nTrain epoch: 153 [2560/5139 (48%)]\tLoss: 0.100843\nTrain epoch: 153 [5120/5139 (95%)]\tLoss: 0.415805\nTrain epoch: 154 [0/5139 (0%)]\tLoss: 0.145257\nTrain epoch: 154 [2560/5139 (48%)]\tLoss: 0.107619\nTrain epoch: 154 [5120/5139 (95%)]\tLoss: 0.111582\nTrain epoch: 155 [0/5139 (0%)]\tLoss: 0.105934\nTrain epoch: 155 [2560/5139 (48%)]\tLoss: 0.151410\nTrain epoch: 155 [5120/5139 (95%)]\tLoss: 0.129235\nTrain epoch: 156 [0/5139 (0%)]\tLoss: 0.182582\nTrain epoch: 156 [2560/5139 (48%)]\tLoss: 0.084339\nTrain epoch: 156 [5120/5139 (95%)]\tLoss: 0.031587\nTrain epoch: 157 [0/5139 (0%)]\tLoss: 0.099235\nTrain epoch: 157 [2560/5139 (48%)]\tLoss: 0.105237\nTrain epoch: 157 [5120/5139 (95%)]\tLoss: 0.271100\nTrain epoch: 158 [0/5139 (0%)]\tLoss: 0.145692\nTrain epoch: 158 [2560/5139 (48%)]\tLoss: 0.147064\nTrain epoch: 158 [5120/5139 (95%)]\tLoss: 0.035239\nTrain epoch: 159 [0/5139 (0%)]\tLoss: 0.069055\nTrain epoch: 159 [2560/5139 (48%)]\tLoss: 0.096128\nTrain epoch: 159 [5120/5139 (95%)]\tLoss: 0.073165\nTrain epoch: 160 [0/5139 (0%)]\tLoss: 0.083970\nTrain epoch: 160 [2560/5139 (48%)]\tLoss: 0.103969\nTrain epoch: 160 [5120/5139 (95%)]\tLoss: 0.008285\nTrain epoch: 161 [0/5139 (0%)]\tLoss: 0.122061\nTrain epoch: 161 [2560/5139 (48%)]\tLoss: 0.116526\nTrain epoch: 161 [5120/5139 (95%)]\tLoss: 0.039759\nTrain epoch: 162 [0/5139 (0%)]\tLoss: 0.143058\nTrain epoch: 162 [2560/5139 (48%)]\tLoss: 0.089440\nTrain epoch: 162 [5120/5139 (95%)]\tLoss: 0.015371\nTrain epoch: 163 [0/5139 (0%)]\tLoss: 0.144443\nTrain epoch: 163 [2560/5139 (48%)]\tLoss: 0.065589\nTrain epoch: 163 [5120/5139 (95%)]\tLoss: 0.126245\nTrain epoch: 164 [0/5139 (0%)]\tLoss: 0.076946\nTrain epoch: 164 [2560/5139 (48%)]\tLoss: 0.099652\nTrain epoch: 164 [5120/5139 (95%)]\tLoss: 0.009962\nTrain epoch: 165 [0/5139 (0%)]\tLoss: 0.101675\nTrain epoch: 165 [2560/5139 (48%)]\tLoss: 0.086934\nTrain epoch: 165 [5120/5139 (95%)]\tLoss: 0.023805\nTrain epoch: 166 [0/5139 (0%)]\tLoss: 0.176039\nTrain epoch: 166 [2560/5139 (48%)]\tLoss: 0.067480\nTrain epoch: 166 [5120/5139 (95%)]\tLoss: 0.086756\nTrain epoch: 167 [0/5139 (0%)]\tLoss: 0.075687\nTrain epoch: 167 [2560/5139 (48%)]\tLoss: 0.103678\nTrain epoch: 167 [5120/5139 (95%)]\tLoss: 0.101298\nTrain epoch: 168 [0/5139 (0%)]\tLoss: 0.079323\nTrain epoch: 168 [2560/5139 (48%)]\tLoss: 0.119861\nTrain epoch: 168 [5120/5139 (95%)]\tLoss: 0.136055\nTrain epoch: 169 [0/5139 (0%)]\tLoss: 0.088180\nTrain epoch: 169 [2560/5139 (48%)]\tLoss: 0.072016\nTrain epoch: 169 [5120/5139 (95%)]\tLoss: 0.106275\nTrain epoch: 170 [0/5139 (0%)]\tLoss: 0.112379\nTrain epoch: 170 [2560/5139 (48%)]\tLoss: 0.083426\nTrain epoch: 170 [5120/5139 (95%)]\tLoss: 0.204344\nTrain epoch: 171 [0/5139 (0%)]\tLoss: 0.140920\nTrain epoch: 171 [2560/5139 (48%)]\tLoss: 0.134193\nTrain epoch: 171 [5120/5139 (95%)]\tLoss: 0.124835\nTrain epoch: 172 [0/5139 (0%)]\tLoss: 0.062627\nTrain epoch: 172 [2560/5139 (48%)]\tLoss: 0.116238\nTrain epoch: 172 [5120/5139 (95%)]\tLoss: 0.030090\nTrain epoch: 173 [0/5139 (0%)]\tLoss: 0.142777\nTrain epoch: 173 [2560/5139 (48%)]\tLoss: 0.149605\nTrain epoch: 173 [5120/5139 (95%)]\tLoss: 0.088840\nTrain epoch: 174 [0/5139 (0%)]\tLoss: 0.113061\nTrain epoch: 174 [2560/5139 (48%)]\tLoss: 0.038093\nTrain epoch: 174 [5120/5139 (95%)]\tLoss: 0.170094\nTrain epoch: 175 [0/5139 (0%)]\tLoss: 0.340074\nTrain epoch: 175 [2560/5139 (48%)]\tLoss: 0.099027\nTrain epoch: 175 [5120/5139 (95%)]\tLoss: 0.260685\nTrain epoch: 176 [0/5139 (0%)]\tLoss: 0.079641\nTrain epoch: 176 [2560/5139 (48%)]\tLoss: 0.118087\nTrain epoch: 176 [5120/5139 (95%)]\tLoss: 0.030720\nTrain epoch: 177 [0/5139 (0%)]\tLoss: 0.080520\nTrain epoch: 177 [2560/5139 (48%)]\tLoss: 0.090247\nTrain epoch: 177 [5120/5139 (95%)]\tLoss: 0.078426\nTrain epoch: 178 [0/5139 (0%)]\tLoss: 0.104671\nTrain epoch: 178 [2560/5139 (48%)]\tLoss: 0.117082\nTrain epoch: 178 [5120/5139 (95%)]\tLoss: 0.068604\nTrain epoch: 179 [0/5139 (0%)]\tLoss: 0.084519\nTrain epoch: 179 [2560/5139 (48%)]\tLoss: 0.077138\nTrain epoch: 179 [5120/5139 (95%)]\tLoss: 0.038536\nTrain epoch: 180 [0/5139 (0%)]\tLoss: 0.062942\nTrain epoch: 180 [2560/5139 (48%)]\tLoss: 0.057405\nTrain epoch: 180 [5120/5139 (95%)]\tLoss: 0.302874\nTrain epoch: 181 [0/5139 (0%)]\tLoss: 0.219292\nTrain epoch: 181 [2560/5139 (48%)]\tLoss: 0.105901\nTrain epoch: 181 [5120/5139 (95%)]\tLoss: 0.131663\nTrain epoch: 182 [0/5139 (0%)]\tLoss: 0.378087\nTrain epoch: 182 [2560/5139 (48%)]\tLoss: 0.179806\nTrain epoch: 182 [5120/5139 (95%)]\tLoss: 0.042786\nTrain epoch: 183 [0/5139 (0%)]\tLoss: 0.090002\nTrain epoch: 183 [2560/5139 (48%)]\tLoss: 0.084511\nTrain epoch: 183 [5120/5139 (95%)]\tLoss: 0.071942\nTrain epoch: 184 [0/5139 (0%)]\tLoss: 0.077008\nTrain epoch: 184 [2560/5139 (48%)]\tLoss: 0.077471\nTrain epoch: 184 [5120/5139 (95%)]\tLoss: 0.183757\nTrain epoch: 185 [0/5139 (0%)]\tLoss: 0.030839\nTrain epoch: 185 [2560/5139 (48%)]\tLoss: 0.071454\nTrain epoch: 185 [5120/5139 (95%)]\tLoss: 0.043032\nTrain epoch: 186 [0/5139 (0%)]\tLoss: 0.043538\nTrain epoch: 186 [2560/5139 (48%)]\tLoss: 0.076295\nTrain epoch: 186 [5120/5139 (95%)]\tLoss: 0.003618\nTrain epoch: 187 [0/5139 (0%)]\tLoss: 0.042844\nTrain epoch: 187 [2560/5139 (48%)]\tLoss: 0.054984\nTrain epoch: 187 [5120/5139 (95%)]\tLoss: 0.091160\nTrain epoch: 188 [0/5139 (0%)]\tLoss: 0.050620\nTrain epoch: 188 [2560/5139 (48%)]\tLoss: 0.068872\nTrain epoch: 188 [5120/5139 (95%)]\tLoss: 0.053938\nTrain epoch: 189 [0/5139 (0%)]\tLoss: 0.085486\nTrain epoch: 189 [2560/5139 (48%)]\tLoss: 0.064710\nTrain epoch: 189 [5120/5139 (95%)]\tLoss: 0.057903\nTrain epoch: 190 [0/5139 (0%)]\tLoss: 0.077429\nTrain epoch: 190 [2560/5139 (48%)]\tLoss: 0.116241\nTrain epoch: 190 [5120/5139 (95%)]\tLoss: 0.021157\nTrain epoch: 191 [0/5139 (0%)]\tLoss: 0.092339\nTrain epoch: 191 [2560/5139 (48%)]\tLoss: 0.092622\nTrain epoch: 191 [5120/5139 (95%)]\tLoss: 0.302596\nTrain epoch: 192 [0/5139 (0%)]\tLoss: 0.085421\nTrain epoch: 192 [2560/5139 (48%)]\tLoss: 0.116157\nTrain epoch: 192 [5120/5139 (95%)]\tLoss: 0.076675\nTrain epoch: 193 [0/5139 (0%)]\tLoss: 0.099034\nTrain epoch: 193 [2560/5139 (48%)]\tLoss: 0.115391\nTrain epoch: 193 [5120/5139 (95%)]\tLoss: 0.009309\nTrain epoch: 194 [0/5139 (0%)]\tLoss: 0.044863\nTrain epoch: 194 [2560/5139 (48%)]\tLoss: 0.057429\nTrain epoch: 194 [5120/5139 (95%)]\tLoss: 0.043673\nTrain epoch: 195 [0/5139 (0%)]\tLoss: 0.128381\nTrain epoch: 195 [2560/5139 (48%)]\tLoss: 0.075541\nTrain epoch: 195 [5120/5139 (95%)]\tLoss: 0.152308\nTrain epoch: 196 [0/5139 (0%)]\tLoss: 0.105635\nTrain epoch: 196 [2560/5139 (48%)]\tLoss: 0.121468\nTrain epoch: 196 [5120/5139 (95%)]\tLoss: 0.019042\nTrain epoch: 197 [0/5139 (0%)]\tLoss: 0.086687\nTrain epoch: 197 [2560/5139 (48%)]\tLoss: 0.115276\nTrain epoch: 197 [5120/5139 (95%)]\tLoss: 0.051422\nTrain epoch: 198 [0/5139 (0%)]\tLoss: 0.068293\nTrain epoch: 198 [2560/5139 (48%)]\tLoss: 0.047929\nTrain epoch: 198 [5120/5139 (95%)]\tLoss: 0.063698\nTrain epoch: 199 [0/5139 (0%)]\tLoss: 0.083929\nTrain epoch: 199 [2560/5139 (48%)]\tLoss: 0.145106\nTrain epoch: 199 [5120/5139 (95%)]\tLoss: 0.014856\nTrain epoch: 200 [0/5139 (0%)]\tLoss: 0.039788\nTrain epoch: 200 [2560/5139 (48%)]\tLoss: 0.084718\nTrain epoch: 200 [5120/5139 (95%)]\tLoss: 0.020906\nTrain epoch: 201 [0/5139 (0%)]\tLoss: 0.053997\nTrain epoch: 201 [2560/5139 (48%)]\tLoss: 0.046500\nTrain epoch: 201 [5120/5139 (95%)]\tLoss: 0.136355\nTrain epoch: 202 [0/5139 (0%)]\tLoss: 0.097576\nTrain epoch: 202 [2560/5139 (48%)]\tLoss: 0.098690\nTrain epoch: 202 [5120/5139 (95%)]\tLoss: 0.037015\nTrain epoch: 203 [0/5139 (0%)]\tLoss: 0.077891\nTrain epoch: 203 [2560/5139 (48%)]\tLoss: 0.071702\nTrain epoch: 203 [5120/5139 (95%)]\tLoss: 0.030842\nTrain epoch: 204 [0/5139 (0%)]\tLoss: 0.037209\nTrain epoch: 204 [2560/5139 (48%)]\tLoss: 0.025100\nTrain epoch: 204 [5120/5139 (95%)]\tLoss: 0.035069\nTrain epoch: 205 [0/5139 (0%)]\tLoss: 0.069196\nTrain epoch: 205 [2560/5139 (48%)]\tLoss: 0.046071\nTrain epoch: 205 [5120/5139 (95%)]\tLoss: 0.020432\nTrain epoch: 206 [0/5139 (0%)]\tLoss: 0.070926\nTrain epoch: 206 [2560/5139 (48%)]\tLoss: 0.027274\nTrain epoch: 206 [5120/5139 (95%)]\tLoss: 0.186357\nTrain epoch: 207 [0/5139 (0%)]\tLoss: 0.093867\nTrain epoch: 207 [2560/5139 (48%)]\tLoss: 0.043338\nTrain epoch: 207 [5120/5139 (95%)]\tLoss: 0.023341\nTrain epoch: 208 [0/5139 (0%)]\tLoss: 0.101994\nTrain epoch: 208 [2560/5139 (48%)]\tLoss: 0.109730\nTrain epoch: 208 [5120/5139 (95%)]\tLoss: 0.105312\nTrain epoch: 209 [0/5139 (0%)]\tLoss: 0.088039\nTrain epoch: 209 [2560/5139 (48%)]\tLoss: 0.050016\nTrain epoch: 209 [5120/5139 (95%)]\tLoss: 0.042980\nTrain epoch: 210 [0/5139 (0%)]\tLoss: 0.091142\nTrain epoch: 210 [2560/5139 (48%)]\tLoss: 0.094112\nTrain epoch: 210 [5120/5139 (95%)]\tLoss: 0.053808\nTrain epoch: 211 [0/5139 (0%)]\tLoss: 0.052921\nTrain epoch: 211 [2560/5139 (48%)]\tLoss: 0.030974\nTrain epoch: 211 [5120/5139 (95%)]\tLoss: 0.066451\nTrain epoch: 212 [0/5139 (0%)]\tLoss: 0.283381\nTrain epoch: 212 [2560/5139 (48%)]\tLoss: 0.173627\nTrain epoch: 212 [5120/5139 (95%)]\tLoss: 0.032837\nTrain epoch: 213 [0/5139 (0%)]\tLoss: 0.085658\nTrain epoch: 213 [2560/5139 (48%)]\tLoss: 0.086793\nTrain epoch: 213 [5120/5139 (95%)]\tLoss: 0.060389\nTrain epoch: 214 [0/5139 (0%)]\tLoss: 0.069692\nTrain epoch: 214 [2560/5139 (48%)]\tLoss: 0.063015\nTrain epoch: 214 [5120/5139 (95%)]\tLoss: 0.187126\nTrain epoch: 215 [0/5139 (0%)]\tLoss: 0.179350\nTrain epoch: 215 [2560/5139 (48%)]\tLoss: 0.082217\nTrain epoch: 215 [5120/5139 (95%)]\tLoss: 0.011005\nTrain epoch: 216 [0/5139 (0%)]\tLoss: 0.034652\nTrain epoch: 216 [2560/5139 (48%)]\tLoss: 0.057785\nTrain epoch: 216 [5120/5139 (95%)]\tLoss: 0.028121\nTrain epoch: 217 [0/5139 (0%)]\tLoss: 0.160545\nTrain epoch: 217 [2560/5139 (48%)]\tLoss: 0.067301\nTrain epoch: 217 [5120/5139 (95%)]\tLoss: 0.009121\nTrain epoch: 218 [0/5139 (0%)]\tLoss: 0.064268\nTrain epoch: 218 [2560/5139 (48%)]\tLoss: 0.037250\nTrain epoch: 218 [5120/5139 (95%)]\tLoss: 0.003335\nTrain epoch: 219 [0/5139 (0%)]\tLoss: 0.043145\nTrain epoch: 219 [2560/5139 (48%)]\tLoss: 0.020220\nTrain epoch: 219 [5120/5139 (95%)]\tLoss: 0.083469\nTrain epoch: 220 [0/5139 (0%)]\tLoss: 0.017950\nTrain epoch: 220 [2560/5139 (48%)]\tLoss: 0.071576\nTrain epoch: 220 [5120/5139 (95%)]\tLoss: 0.017031\nTrain epoch: 221 [0/5139 (0%)]\tLoss: 0.040687\nTrain epoch: 221 [2560/5139 (48%)]\tLoss: 0.034906\nTrain epoch: 221 [5120/5139 (95%)]\tLoss: 0.017534\nTrain epoch: 222 [0/5139 (0%)]\tLoss: 0.058016\nTrain epoch: 222 [2560/5139 (48%)]\tLoss: 0.046604\nTrain epoch: 222 [5120/5139 (95%)]\tLoss: 0.230400\nTrain epoch: 223 [0/5139 (0%)]\tLoss: 0.063628\nTrain epoch: 223 [2560/5139 (48%)]\tLoss: 0.054450\nTrain epoch: 223 [5120/5139 (95%)]\tLoss: 0.040544\nTrain epoch: 224 [0/5139 (0%)]\tLoss: 0.049457\nTrain epoch: 224 [2560/5139 (48%)]\tLoss: 0.078962\nTrain epoch: 224 [5120/5139 (95%)]\tLoss: 0.006293\nTrain epoch: 225 [0/5139 (0%)]\tLoss: 0.033905\nTrain epoch: 225 [2560/5139 (48%)]\tLoss: 0.163105\nTrain epoch: 225 [5120/5139 (95%)]\tLoss: 0.114991\nTrain epoch: 226 [0/5139 (0%)]\tLoss: 0.038791\nTrain epoch: 226 [2560/5139 (48%)]\tLoss: 0.054764\nTrain epoch: 226 [5120/5139 (95%)]\tLoss: 0.040945\nTrain epoch: 227 [0/5139 (0%)]\tLoss: 0.037507\nTrain epoch: 227 [2560/5139 (48%)]\tLoss: 0.061354\nTrain epoch: 227 [5120/5139 (95%)]\tLoss: 0.115309\nTrain epoch: 228 [0/5139 (0%)]\tLoss: 0.110081\nTrain epoch: 228 [2560/5139 (48%)]\tLoss: 0.051990\nTrain epoch: 228 [5120/5139 (95%)]\tLoss: 0.052727\nTrain epoch: 229 [0/5139 (0%)]\tLoss: 0.057542\nTrain epoch: 229 [2560/5139 (48%)]\tLoss: 0.045660\nTrain epoch: 229 [5120/5139 (95%)]\tLoss: 0.010592\nTrain epoch: 230 [0/5139 (0%)]\tLoss: 0.033167\nTrain epoch: 230 [2560/5139 (48%)]\tLoss: 0.068024\nTrain epoch: 230 [5120/5139 (95%)]\tLoss: 0.063360\nTrain epoch: 231 [0/5139 (0%)]\tLoss: 0.048083\nTrain epoch: 231 [2560/5139 (48%)]\tLoss: 0.154675\nTrain epoch: 231 [5120/5139 (95%)]\tLoss: 0.065178\nTrain epoch: 232 [0/5139 (0%)]\tLoss: 0.080205\nTrain epoch: 232 [2560/5139 (48%)]\tLoss: 0.059753\nTrain epoch: 232 [5120/5139 (95%)]\tLoss: 0.000705\nTrain epoch: 233 [0/5139 (0%)]\tLoss: 0.052224\nTrain epoch: 233 [2560/5139 (48%)]\tLoss: 0.030133\nTrain epoch: 233 [5120/5139 (95%)]\tLoss: 0.038955\nTrain epoch: 234 [0/5139 (0%)]\tLoss: 0.036513\nTrain epoch: 234 [2560/5139 (48%)]\tLoss: 0.094538\nTrain epoch: 234 [5120/5139 (95%)]\tLoss: 0.037071\nTrain epoch: 235 [0/5139 (0%)]\tLoss: 0.058643\nTrain epoch: 235 [2560/5139 (48%)]\tLoss: 0.032826\nTrain epoch: 235 [5120/5139 (95%)]\tLoss: 0.058163\nTrain epoch: 236 [0/5139 (0%)]\tLoss: 0.027331\nTrain epoch: 236 [2560/5139 (48%)]\tLoss: 0.030327\nTrain epoch: 236 [5120/5139 (95%)]\tLoss: 0.008751\nTrain epoch: 237 [0/5139 (0%)]\tLoss: 0.032924\nTrain epoch: 237 [2560/5139 (48%)]\tLoss: 0.035598\nTrain epoch: 237 [5120/5139 (95%)]\tLoss: 0.007237\nTrain epoch: 238 [0/5139 (0%)]\tLoss: 0.043365\nTrain epoch: 238 [2560/5139 (48%)]\tLoss: 0.069761\nTrain epoch: 238 [5120/5139 (95%)]\tLoss: 0.048189\nTrain epoch: 239 [0/5139 (0%)]\tLoss: 0.042558\nTrain epoch: 239 [2560/5139 (48%)]\tLoss: 0.015284\nTrain epoch: 239 [5120/5139 (95%)]\tLoss: 0.072118\nTrain epoch: 240 [0/5139 (0%)]\tLoss: 0.088530\nTrain epoch: 240 [2560/5139 (48%)]\tLoss: 0.117625\nTrain epoch: 240 [5120/5139 (95%)]\tLoss: 0.019841\nTrain epoch: 241 [0/5139 (0%)]\tLoss: 0.043396\nTrain epoch: 241 [2560/5139 (48%)]\tLoss: 0.024531\nTrain epoch: 241 [5120/5139 (95%)]\tLoss: 0.005267\nTrain epoch: 242 [0/5139 (0%)]\tLoss: 0.060120\nTrain epoch: 242 [2560/5139 (48%)]\tLoss: 0.058747\nTrain epoch: 242 [5120/5139 (95%)]\tLoss: 0.009152\nTrain epoch: 243 [0/5139 (0%)]\tLoss: 0.135054\nTrain epoch: 243 [2560/5139 (48%)]\tLoss: 0.027043\nTrain epoch: 243 [5120/5139 (95%)]\tLoss: 0.013348\nTrain epoch: 244 [0/5139 (0%)]\tLoss: 0.253490\nTrain epoch: 244 [2560/5139 (48%)]\tLoss: 0.171635\nTrain epoch: 244 [5120/5139 (95%)]\tLoss: 0.283547\nTrain epoch: 245 [0/5139 (0%)]\tLoss: 0.084219\nTrain epoch: 245 [2560/5139 (48%)]\tLoss: 0.085200\nTrain epoch: 245 [5120/5139 (95%)]\tLoss: 0.029054\nTrain epoch: 246 [0/5139 (0%)]\tLoss: 0.049178\nTrain epoch: 246 [2560/5139 (48%)]\tLoss: 0.066089\nTrain epoch: 246 [5120/5139 (95%)]\tLoss: 0.033890\nTrain epoch: 247 [0/5139 (0%)]\tLoss: 0.024118\nTrain epoch: 247 [2560/5139 (48%)]\tLoss: 0.023690\nTrain epoch: 247 [5120/5139 (95%)]\tLoss: 0.005836\nTrain epoch: 248 [0/5139 (0%)]\tLoss: 0.021428\nTrain epoch: 248 [2560/5139 (48%)]\tLoss: 0.036465\nTrain epoch: 248 [5120/5139 (95%)]\tLoss: 0.194155\nTrain epoch: 249 [0/5139 (0%)]\tLoss: 0.108240\nTrain epoch: 249 [2560/5139 (48%)]\tLoss: 0.107272\nTrain epoch: 249 [5120/5139 (95%)]\tLoss: 0.045078\nTrain epoch: 250 [0/5139 (0%)]\tLoss: 0.104793\nTrain epoch: 250 [2560/5139 (48%)]\tLoss: 0.049173\nTrain epoch: 250 [5120/5139 (95%)]\tLoss: 0.055075\nTrain epoch: 251 [0/5139 (0%)]\tLoss: 0.049719\nTrain epoch: 251 [2560/5139 (48%)]\tLoss: 0.045245\nTrain epoch: 251 [5120/5139 (95%)]\tLoss: 0.013373\nTrain epoch: 252 [0/5139 (0%)]\tLoss: 0.089924\nTrain epoch: 252 [2560/5139 (48%)]\tLoss: 0.089569\nTrain epoch: 252 [5120/5139 (95%)]\tLoss: 0.031212\nTrain epoch: 253 [0/5139 (0%)]\tLoss: 0.022097\nTrain epoch: 253 [2560/5139 (48%)]\tLoss: 0.038001\nTrain epoch: 253 [5120/5139 (95%)]\tLoss: 0.059567\nTrain epoch: 254 [0/5139 (0%)]\tLoss: 0.081220\nTrain epoch: 254 [2560/5139 (48%)]\tLoss: 0.032797\nTrain epoch: 254 [5120/5139 (95%)]\tLoss: 0.023411\nTrain epoch: 255 [0/5139 (0%)]\tLoss: 0.025786\nTrain epoch: 255 [2560/5139 (48%)]\tLoss: 0.037689\nTrain epoch: 255 [5120/5139 (95%)]\tLoss: 0.132323\nTrain epoch: 256 [0/5139 (0%)]\tLoss: 0.036279\nTrain epoch: 256 [2560/5139 (48%)]\tLoss: 0.192945\nTrain epoch: 256 [5120/5139 (95%)]\tLoss: 0.165558\nTrain epoch: 257 [0/5139 (0%)]\tLoss: 0.102239\nTrain epoch: 257 [2560/5139 (48%)]\tLoss: 0.045657\nTrain epoch: 257 [5120/5139 (95%)]\tLoss: 0.047130\nTrain epoch: 258 [0/5139 (0%)]\tLoss: 0.061941\nTrain epoch: 258 [2560/5139 (48%)]\tLoss: 0.019896\nTrain epoch: 258 [5120/5139 (95%)]\tLoss: 0.082364\nTrain epoch: 259 [0/5139 (0%)]\tLoss: 0.060610\nTrain epoch: 259 [2560/5139 (48%)]\tLoss: 0.086010\nTrain epoch: 259 [5120/5139 (95%)]\tLoss: 0.004421\nTrain epoch: 260 [0/5139 (0%)]\tLoss: 0.035021\nTrain epoch: 260 [2560/5139 (48%)]\tLoss: 0.032228\nTrain epoch: 260 [5120/5139 (95%)]\tLoss: 0.003873\nTrain epoch: 261 [0/5139 (0%)]\tLoss: 0.041200\nTrain epoch: 261 [2560/5139 (48%)]\tLoss: 0.030328\nTrain epoch: 261 [5120/5139 (95%)]\tLoss: 0.070178\nTrain epoch: 262 [0/5139 (0%)]\tLoss: 0.087905\nTrain epoch: 262 [2560/5139 (48%)]\tLoss: 0.127623\nTrain epoch: 262 [5120/5139 (95%)]\tLoss: 0.013697\nTrain epoch: 263 [0/5139 (0%)]\tLoss: 0.123383\nTrain epoch: 263 [2560/5139 (48%)]\tLoss: 0.073855\nTrain epoch: 263 [5120/5139 (95%)]\tLoss: 0.056290\nTrain epoch: 264 [0/5139 (0%)]\tLoss: 0.037912\nTrain epoch: 264 [2560/5139 (48%)]\tLoss: 0.059460\nTrain epoch: 264 [5120/5139 (95%)]\tLoss: 0.058145\nTrain epoch: 265 [0/5139 (0%)]\tLoss: 0.023228\nTrain epoch: 265 [2560/5139 (48%)]\tLoss: 0.086574\nTrain epoch: 265 [5120/5139 (95%)]\tLoss: 0.005547\nTrain epoch: 266 [0/5139 (0%)]\tLoss: 0.061526\nTrain epoch: 266 [2560/5139 (48%)]\tLoss: 0.029316\nTrain epoch: 266 [5120/5139 (95%)]\tLoss: 0.015310\nTrain epoch: 267 [0/5139 (0%)]\tLoss: 0.021350\nTrain epoch: 267 [2560/5139 (48%)]\tLoss: 0.023772\nTrain epoch: 267 [5120/5139 (95%)]\tLoss: 0.113447\nTrain epoch: 268 [0/5139 (0%)]\tLoss: 0.093818\nTrain epoch: 268 [2560/5139 (48%)]\tLoss: 0.111339\nTrain epoch: 268 [5120/5139 (95%)]\tLoss: 0.055779\nTrain epoch: 269 [0/5139 (0%)]\tLoss: 0.024966\nTrain epoch: 269 [2560/5139 (48%)]\tLoss: 0.048595\nTrain epoch: 269 [5120/5139 (95%)]\tLoss: 0.065820\nTrain epoch: 270 [0/5139 (0%)]\tLoss: 0.030734\nTrain epoch: 270 [2560/5139 (48%)]\tLoss: 0.026514\nTrain epoch: 270 [5120/5139 (95%)]\tLoss: 0.026180\nTrain epoch: 271 [0/5139 (0%)]\tLoss: 0.101912\nTrain epoch: 271 [2560/5139 (48%)]\tLoss: 0.090614\nTrain epoch: 271 [5120/5139 (95%)]\tLoss: 0.007745\nTrain epoch: 272 [0/5139 (0%)]\tLoss: 0.112032\nTrain epoch: 272 [2560/5139 (48%)]\tLoss: 0.030389\nTrain epoch: 272 [5120/5139 (95%)]\tLoss: 0.009049\nTrain epoch: 273 [0/5139 (0%)]\tLoss: 0.132479\nTrain epoch: 273 [2560/5139 (48%)]\tLoss: 0.076619\nTrain epoch: 273 [5120/5139 (95%)]\tLoss: 0.014574\nTrain epoch: 274 [0/5139 (0%)]\tLoss: 0.061975\nTrain epoch: 274 [2560/5139 (48%)]\tLoss: 0.051092\nTrain epoch: 274 [5120/5139 (95%)]\tLoss: 0.024295\nTrain epoch: 275 [0/5139 (0%)]\tLoss: 0.042886\nTrain epoch: 275 [2560/5139 (48%)]\tLoss: 0.019386\nTrain epoch: 275 [5120/5139 (95%)]\tLoss: 0.021561\nTrain epoch: 276 [0/5139 (0%)]\tLoss: 0.033902\nTrain epoch: 276 [2560/5139 (48%)]\tLoss: 0.034433\nTrain epoch: 276 [5120/5139 (95%)]\tLoss: 0.027641\nTrain epoch: 277 [0/5139 (0%)]\tLoss: 0.188819\nTrain epoch: 277 [2560/5139 (48%)]\tLoss: 0.028445\nTrain epoch: 277 [5120/5139 (95%)]\tLoss: 0.029132\nTrain epoch: 278 [0/5139 (0%)]\tLoss: 0.034820\nTrain epoch: 278 [2560/5139 (48%)]\tLoss: 0.022464\nTrain epoch: 278 [5120/5139 (95%)]\tLoss: 0.006830\nTrain epoch: 279 [0/5139 (0%)]\tLoss: 0.007480\nTrain epoch: 279 [2560/5139 (48%)]\tLoss: 0.027455\nTrain epoch: 279 [5120/5139 (95%)]\tLoss: 0.029281\nTrain epoch: 280 [0/5139 (0%)]\tLoss: 0.035966\nTrain epoch: 280 [2560/5139 (48%)]\tLoss: 0.025828\nTrain epoch: 280 [5120/5139 (95%)]\tLoss: 0.003860\nTrain epoch: 281 [0/5139 (0%)]\tLoss: 0.008604\nTrain epoch: 281 [2560/5139 (48%)]\tLoss: 0.075089\nTrain epoch: 281 [5120/5139 (95%)]\tLoss: 0.003583\nTrain epoch: 282 [0/5139 (0%)]\tLoss: 0.012620\nTrain epoch: 282 [2560/5139 (48%)]\tLoss: 0.075019\nTrain epoch: 282 [5120/5139 (95%)]\tLoss: 0.007837\nTrain epoch: 283 [0/5139 (0%)]\tLoss: 0.051308\nTrain epoch: 283 [2560/5139 (48%)]\tLoss: 0.035965\nTrain epoch: 283 [5120/5139 (95%)]\tLoss: 0.001324\nTrain epoch: 284 [0/5139 (0%)]\tLoss: 0.026321\nTrain epoch: 284 [2560/5139 (48%)]\tLoss: 0.076080\nTrain epoch: 284 [5120/5139 (95%)]\tLoss: 0.188025\nTrain epoch: 285 [0/5139 (0%)]\tLoss: 0.012050\nTrain epoch: 285 [2560/5139 (48%)]\tLoss: 0.069189\nTrain epoch: 285 [5120/5139 (95%)]\tLoss: 0.206469\nTrain epoch: 286 [0/5139 (0%)]\tLoss: 0.048267\nTrain epoch: 286 [2560/5139 (48%)]\tLoss: 0.046665\nTrain epoch: 286 [5120/5139 (95%)]\tLoss: 0.002916\nTrain epoch: 287 [0/5139 (0%)]\tLoss: 0.038982\nTrain epoch: 287 [2560/5139 (48%)]\tLoss: 0.019300\nTrain epoch: 287 [5120/5139 (95%)]\tLoss: 0.005528\nTrain epoch: 288 [0/5139 (0%)]\tLoss: 0.025897\nTrain epoch: 288 [2560/5139 (48%)]\tLoss: 0.021142\nTrain epoch: 288 [5120/5139 (95%)]\tLoss: 0.001483\nTrain epoch: 289 [0/5139 (0%)]\tLoss: 0.092176\nTrain epoch: 289 [2560/5139 (48%)]\tLoss: 0.011180\nTrain epoch: 289 [5120/5139 (95%)]\tLoss: 0.002245\nTrain epoch: 290 [0/5139 (0%)]\tLoss: 0.030130\nTrain epoch: 290 [2560/5139 (48%)]\tLoss: 0.023997\nTrain epoch: 290 [5120/5139 (95%)]\tLoss: 0.002697\nTrain epoch: 291 [0/5139 (0%)]\tLoss: 0.037852\nTrain epoch: 291 [2560/5139 (48%)]\tLoss: 0.007198\nTrain epoch: 291 [5120/5139 (95%)]\tLoss: 0.006179\nTrain epoch: 292 [0/5139 (0%)]\tLoss: 0.009393\nTrain epoch: 292 [2560/5139 (48%)]\tLoss: 0.015352\nTrain epoch: 292 [5120/5139 (95%)]\tLoss: 0.001562\nTrain epoch: 293 [0/5139 (0%)]\tLoss: 0.008935\nTrain epoch: 293 [2560/5139 (48%)]\tLoss: 0.024268\nTrain epoch: 293 [5120/5139 (95%)]\tLoss: 0.001251\nTrain epoch: 294 [0/5139 (0%)]\tLoss: 0.018914\nTrain epoch: 294 [2560/5139 (48%)]\tLoss: 0.015619\nTrain epoch: 294 [5120/5139 (95%)]\tLoss: 0.002645\nTrain epoch: 295 [0/5139 (0%)]\tLoss: 0.009299\nTrain epoch: 295 [2560/5139 (48%)]\tLoss: 0.009210\nTrain epoch: 295 [5120/5139 (95%)]\tLoss: 0.000870\nTrain epoch: 296 [0/5139 (0%)]\tLoss: 0.014521\nTrain epoch: 296 [2560/5139 (48%)]\tLoss: 0.014380\nTrain epoch: 296 [5120/5139 (95%)]\tLoss: 0.001426\nTrain epoch: 297 [0/5139 (0%)]\tLoss: 0.017088\nTrain epoch: 297 [2560/5139 (48%)]\tLoss: 0.009299\nTrain epoch: 297 [5120/5139 (95%)]\tLoss: 0.025031\nTrain epoch: 298 [0/5139 (0%)]\tLoss: 0.105454\nTrain epoch: 298 [2560/5139 (48%)]\tLoss: 0.077450\nTrain epoch: 298 [5120/5139 (95%)]\tLoss: 0.007796\nTrain epoch: 299 [0/5139 (0%)]\tLoss: 0.015548\nTrain epoch: 299 [2560/5139 (48%)]\tLoss: 0.026937\nTrain epoch: 299 [5120/5139 (95%)]\tLoss: 0.002084\nTrain epoch: 300 [0/5139 (0%)]\tLoss: 0.016220\nTrain epoch: 300 [2560/5139 (48%)]\tLoss: 0.025684\nTrain epoch: 300 [5120/5139 (95%)]\tLoss: 0.129315\nTrain epoch: 301 [0/5139 (0%)]\tLoss: 0.008722\nTrain epoch: 301 [2560/5139 (48%)]\tLoss: 0.012921\nTrain epoch: 301 [5120/5139 (95%)]\tLoss: 0.014556\nTrain epoch: 302 [0/5139 (0%)]\tLoss: 0.008166\nTrain epoch: 302 [2560/5139 (48%)]\tLoss: 0.032157\nTrain epoch: 302 [5120/5139 (95%)]\tLoss: 0.036650\nTrain epoch: 303 [0/5139 (0%)]\tLoss: 0.023247\nTrain epoch: 303 [2560/5139 (48%)]\tLoss: 0.013429\nTrain epoch: 303 [5120/5139 (95%)]\tLoss: 0.001947\nTrain epoch: 304 [0/5139 (0%)]\tLoss: 0.005587\nTrain epoch: 304 [2560/5139 (48%)]\tLoss: 0.050558\nTrain epoch: 304 [5120/5139 (95%)]\tLoss: 0.545162\nTrain epoch: 305 [0/5139 (0%)]\tLoss: 0.031065\nTrain epoch: 305 [2560/5139 (48%)]\tLoss: 0.138269\nTrain epoch: 305 [5120/5139 (95%)]\tLoss: 0.021770\nTrain epoch: 306 [0/5139 (0%)]\tLoss: 0.050868\nTrain epoch: 306 [2560/5139 (48%)]\tLoss: 0.019801\nTrain epoch: 306 [5120/5139 (95%)]\tLoss: 0.010105\nTrain epoch: 307 [0/5139 (0%)]\tLoss: 0.031504\nTrain epoch: 307 [2560/5139 (48%)]\tLoss: 0.013275\nTrain epoch: 307 [5120/5139 (95%)]\tLoss: 0.000187\nTrain epoch: 308 [0/5139 (0%)]\tLoss: 0.007563\nTrain epoch: 308 [2560/5139 (48%)]\tLoss: 0.047177\nTrain epoch: 308 [5120/5139 (95%)]\tLoss: 0.003279\nTrain epoch: 309 [0/5139 (0%)]\tLoss: 0.156233\nTrain epoch: 309 [2560/5139 (48%)]\tLoss: 0.060551\nTrain epoch: 309 [5120/5139 (95%)]\tLoss: 0.019063\nTrain epoch: 310 [0/5139 (0%)]\tLoss: 0.049593\nTrain epoch: 310 [2560/5139 (48%)]\tLoss: 0.013208\nTrain epoch: 310 [5120/5139 (95%)]\tLoss: 0.001926\nTrain epoch: 311 [0/5139 (0%)]\tLoss: 0.023241\nTrain epoch: 311 [2560/5139 (48%)]\tLoss: 0.017192\nTrain epoch: 311 [5120/5139 (95%)]\tLoss: 0.008276\nTrain epoch: 312 [0/5139 (0%)]\tLoss: 0.122808\nTrain epoch: 312 [2560/5139 (48%)]\tLoss: 0.022033\nTrain epoch: 312 [5120/5139 (95%)]\tLoss: 0.064341\nTrain epoch: 313 [0/5139 (0%)]\tLoss: 0.030925\nTrain epoch: 313 [2560/5139 (48%)]\tLoss: 0.022482\nTrain epoch: 313 [5120/5139 (95%)]\tLoss: 0.022723\nTrain epoch: 314 [0/5139 (0%)]\tLoss: 0.014151\nTrain epoch: 314 [2560/5139 (48%)]\tLoss: 0.015172\nTrain epoch: 314 [5120/5139 (95%)]\tLoss: 0.049181\nTrain epoch: 315 [0/5139 (0%)]\tLoss: 0.008955\nTrain epoch: 315 [2560/5139 (48%)]\tLoss: 0.018370\nTrain epoch: 315 [5120/5139 (95%)]\tLoss: 0.014801\nTrain epoch: 316 [0/5139 (0%)]\tLoss: 0.163654\nTrain epoch: 316 [2560/5139 (48%)]\tLoss: 0.044293\nTrain epoch: 316 [5120/5139 (95%)]\tLoss: 0.004395\nTrain epoch: 317 [0/5139 (0%)]\tLoss: 0.036929\nTrain epoch: 317 [2560/5139 (48%)]\tLoss: 0.011463\nTrain epoch: 317 [5120/5139 (95%)]\tLoss: 0.001726\nTrain epoch: 318 [0/5139 (0%)]\tLoss: 0.026383\nTrain epoch: 318 [2560/5139 (48%)]\tLoss: 0.026353\nTrain epoch: 318 [5120/5139 (95%)]\tLoss: 0.017301\nTrain epoch: 319 [0/5139 (0%)]\tLoss: 0.018113\nTrain epoch: 319 [2560/5139 (48%)]\tLoss: 0.014943\nTrain epoch: 319 [5120/5139 (95%)]\tLoss: 0.002527\nTrain epoch: 320 [0/5139 (0%)]\tLoss: 0.005196\nTrain epoch: 320 [2560/5139 (48%)]\tLoss: 0.049385\nTrain epoch: 320 [5120/5139 (95%)]\tLoss: 0.000911\nTrain epoch: 321 [0/5139 (0%)]\tLoss: 0.031965\nTrain epoch: 321 [2560/5139 (48%)]\tLoss: 0.014191\nTrain epoch: 321 [5120/5139 (95%)]\tLoss: 0.000734\nTrain epoch: 322 [0/5139 (0%)]\tLoss: 0.009810\nTrain epoch: 322 [2560/5139 (48%)]\tLoss: 0.002518\nTrain epoch: 322 [5120/5139 (95%)]\tLoss: 0.005621\nTrain epoch: 323 [0/5139 (0%)]\tLoss: 0.010634\nTrain epoch: 323 [2560/5139 (48%)]\tLoss: 0.053721\nTrain epoch: 323 [5120/5139 (95%)]\tLoss: 0.016838\nTrain epoch: 324 [0/5139 (0%)]\tLoss: 0.013887\nTrain epoch: 324 [2560/5139 (48%)]\tLoss: 0.008403\nTrain epoch: 324 [5120/5139 (95%)]\tLoss: 0.000475\nTrain epoch: 325 [0/5139 (0%)]\tLoss: 0.017933\nTrain epoch: 325 [2560/5139 (48%)]\tLoss: 0.019471\nTrain epoch: 325 [5120/5139 (95%)]\tLoss: 0.000216\nTrain epoch: 326 [0/5139 (0%)]\tLoss: 0.011991\nTrain epoch: 326 [2560/5139 (48%)]\tLoss: 0.017783\nTrain epoch: 326 [5120/5139 (95%)]\tLoss: 0.012386\nTrain epoch: 327 [0/5139 (0%)]\tLoss: 0.074493\nTrain epoch: 327 [2560/5139 (48%)]\tLoss: 0.026148\nTrain epoch: 327 [5120/5139 (95%)]\tLoss: 0.000850\nTrain epoch: 328 [0/5139 (0%)]\tLoss: 0.022620\nTrain epoch: 328 [2560/5139 (48%)]\tLoss: 0.019921\nTrain epoch: 328 [5120/5139 (95%)]\tLoss: 0.076328\nTrain epoch: 329 [0/5139 (0%)]\tLoss: 0.006042\nTrain epoch: 329 [2560/5139 (48%)]\tLoss: 0.121465\nTrain epoch: 329 [5120/5139 (95%)]\tLoss: 0.004142\nTrain epoch: 330 [0/5139 (0%)]\tLoss: 0.016197\nTrain epoch: 330 [2560/5139 (48%)]\tLoss: 0.030799\nTrain epoch: 330 [5120/5139 (95%)]\tLoss: 0.000399\nTrain epoch: 331 [0/5139 (0%)]\tLoss: 0.015631\nTrain epoch: 331 [2560/5139 (48%)]\tLoss: 0.003789\nTrain epoch: 331 [5120/5139 (95%)]\tLoss: 0.009007\nTrain epoch: 332 [0/5139 (0%)]\tLoss: 0.008228\nTrain epoch: 332 [2560/5139 (48%)]\tLoss: 0.031908\nTrain epoch: 332 [5120/5139 (95%)]\tLoss: 0.109850\nTrain epoch: 333 [0/5139 (0%)]\tLoss: 0.018479\nTrain epoch: 333 [2560/5139 (48%)]\tLoss: 0.248701\nTrain epoch: 333 [5120/5139 (95%)]\tLoss: 0.066224\nTrain epoch: 334 [0/5139 (0%)]\tLoss: 0.063493\nTrain epoch: 334 [2560/5139 (48%)]\tLoss: 0.030260\nTrain epoch: 334 [5120/5139 (95%)]\tLoss: 0.014635\nTrain epoch: 335 [0/5139 (0%)]\tLoss: 0.027794\nTrain epoch: 335 [2560/5139 (48%)]\tLoss: 0.028698\nTrain epoch: 335 [5120/5139 (95%)]\tLoss: 0.006894\nTrain epoch: 336 [0/5139 (0%)]\tLoss: 0.013884\nTrain epoch: 336 [2560/5139 (48%)]\tLoss: 0.014741\nTrain epoch: 336 [5120/5139 (95%)]\tLoss: 0.001663\nTrain epoch: 337 [0/5139 (0%)]\tLoss: 0.033045\nTrain epoch: 337 [2560/5139 (48%)]\tLoss: 0.010096\nTrain epoch: 337 [5120/5139 (95%)]\tLoss: 0.001107\nTrain epoch: 338 [0/5139 (0%)]\tLoss: 0.004556\nTrain epoch: 338 [2560/5139 (48%)]\tLoss: 0.009476\nTrain epoch: 338 [5120/5139 (95%)]\tLoss: 0.084879\nTrain epoch: 339 [0/5139 (0%)]\tLoss: 0.124564\nTrain epoch: 339 [2560/5139 (48%)]\tLoss: 0.082203\nTrain epoch: 339 [5120/5139 (95%)]\tLoss: 0.052672\nTrain epoch: 340 [0/5139 (0%)]\tLoss: 0.085136\nTrain epoch: 340 [2560/5139 (48%)]\tLoss: 0.113741\nTrain epoch: 340 [5120/5139 (95%)]\tLoss: 0.016658\nTrain epoch: 341 [0/5139 (0%)]\tLoss: 0.045030\nTrain epoch: 341 [2560/5139 (48%)]\tLoss: 0.051171\nTrain epoch: 341 [5120/5139 (95%)]\tLoss: 0.057305\nTrain epoch: 342 [0/5139 (0%)]\tLoss: 0.056745\nTrain epoch: 342 [2560/5139 (48%)]\tLoss: 0.012428\nTrain epoch: 342 [5120/5139 (95%)]\tLoss: 0.000111\nTrain epoch: 343 [0/5139 (0%)]\tLoss: 0.096726\nTrain epoch: 343 [2560/5139 (48%)]\tLoss: 0.024765\nTrain epoch: 343 [5120/5139 (95%)]\tLoss: 0.156729\nTrain epoch: 344 [0/5139 (0%)]\tLoss: 0.092710\nTrain epoch: 344 [2560/5139 (48%)]\tLoss: 0.050574\nTrain epoch: 344 [5120/5139 (95%)]\tLoss: 0.096764\nTrain epoch: 345 [0/5139 (0%)]\tLoss: 0.018477\nTrain epoch: 345 [2560/5139 (48%)]\tLoss: 0.078877\nTrain epoch: 345 [5120/5139 (95%)]\tLoss: 0.000594\nTrain epoch: 346 [0/5139 (0%)]\tLoss: 0.107869\nTrain epoch: 346 [2560/5139 (48%)]\tLoss: 0.032468\nTrain epoch: 346 [5120/5139 (95%)]\tLoss: 0.000221\nTrain epoch: 347 [0/5139 (0%)]\tLoss: 0.067025\nTrain epoch: 347 [2560/5139 (48%)]\tLoss: 0.038808\nTrain epoch: 347 [5120/5139 (95%)]\tLoss: 0.047996\nTrain epoch: 348 [0/5139 (0%)]\tLoss: 0.096217\nTrain epoch: 348 [2560/5139 (48%)]\tLoss: 0.011116\nTrain epoch: 348 [5120/5139 (95%)]\tLoss: 0.004082\nTrain epoch: 349 [0/5139 (0%)]\tLoss: 0.015406\nTrain epoch: 349 [2560/5139 (48%)]\tLoss: 0.020286\nTrain epoch: 349 [5120/5139 (95%)]\tLoss: 0.002626\nTrain epoch: 350 [0/5139 (0%)]\tLoss: 0.016489\nTrain epoch: 350 [2560/5139 (48%)]\tLoss: 0.013805\nTrain epoch: 350 [5120/5139 (95%)]\tLoss: 0.145357\nTrain epoch: 351 [0/5139 (0%)]\tLoss: 0.090454\nTrain epoch: 351 [2560/5139 (48%)]\tLoss: 0.345895\nTrain epoch: 351 [5120/5139 (95%)]\tLoss: 0.094378\nTrain epoch: 352 [0/5139 (0%)]\tLoss: 0.174751\nTrain epoch: 352 [2560/5139 (48%)]\tLoss: 0.080095\nTrain epoch: 352 [5120/5139 (95%)]\tLoss: 0.004025\nTrain epoch: 353 [0/5139 (0%)]\tLoss: 0.069097\nTrain epoch: 353 [2560/5139 (48%)]\tLoss: 0.023263\nTrain epoch: 353 [5120/5139 (95%)]\tLoss: 0.018635\nTrain epoch: 354 [0/5139 (0%)]\tLoss: 0.042820\nTrain epoch: 354 [2560/5139 (48%)]\tLoss: 0.017173\nTrain epoch: 354 [5120/5139 (95%)]\tLoss: 0.023190\nTrain epoch: 355 [0/5139 (0%)]\tLoss: 0.013540\nTrain epoch: 355 [2560/5139 (48%)]\tLoss: 0.025841\nTrain epoch: 355 [5120/5139 (95%)]\tLoss: 0.015753\nTrain epoch: 356 [0/5139 (0%)]\tLoss: 0.018563\nTrain epoch: 356 [2560/5139 (48%)]\tLoss: 0.008164\nTrain epoch: 356 [5120/5139 (95%)]\tLoss: 0.000688\nTrain epoch: 357 [0/5139 (0%)]\tLoss: 0.012138\nTrain epoch: 357 [2560/5139 (48%)]\tLoss: 0.014085\nTrain epoch: 357 [5120/5139 (95%)]\tLoss: 0.001137\nTrain epoch: 358 [0/5139 (0%)]\tLoss: 0.002377\nTrain epoch: 358 [2560/5139 (48%)]\tLoss: 0.009568\nTrain epoch: 358 [5120/5139 (95%)]\tLoss: 0.176224\nTrain epoch: 359 [0/5139 (0%)]\tLoss: 0.006111\nTrain epoch: 359 [2560/5139 (48%)]\tLoss: 0.043749\nTrain epoch: 359 [5120/5139 (95%)]\tLoss: 0.003262\nTrain epoch: 360 [0/5139 (0%)]\tLoss: 0.013015\nTrain epoch: 360 [2560/5139 (48%)]\tLoss: 0.020483\nTrain epoch: 360 [5120/5139 (95%)]\tLoss: 0.067173\nTrain epoch: 361 [0/5139 (0%)]\tLoss: 0.033251\nTrain epoch: 361 [2560/5139 (48%)]\tLoss: 0.046787\nTrain epoch: 361 [5120/5139 (95%)]\tLoss: 0.001040\nTrain epoch: 362 [0/5139 (0%)]\tLoss: 0.038964\nTrain epoch: 362 [2560/5139 (48%)]\tLoss: 0.010126\nTrain epoch: 362 [5120/5139 (95%)]\tLoss: 0.002652\nTrain epoch: 363 [0/5139 (0%)]\tLoss: 0.013931\nTrain epoch: 363 [2560/5139 (48%)]\tLoss: 0.003808\nTrain epoch: 363 [5120/5139 (95%)]\tLoss: 0.046536\nTrain epoch: 364 [0/5139 (0%)]\tLoss: 0.004717\nTrain epoch: 364 [2560/5139 (48%)]\tLoss: 0.012228\nTrain epoch: 364 [5120/5139 (95%)]\tLoss: 0.015118\nTrain epoch: 365 [0/5139 (0%)]\tLoss: 0.028454\nTrain epoch: 365 [2560/5139 (48%)]\tLoss: 0.022125\nTrain epoch: 365 [5120/5139 (95%)]\tLoss: 0.000175\nTrain epoch: 366 [0/5139 (0%)]\tLoss: 0.016682\nTrain epoch: 366 [2560/5139 (48%)]\tLoss: 0.002329\nTrain epoch: 366 [5120/5139 (95%)]\tLoss: 0.013597\nTrain epoch: 367 [0/5139 (0%)]\tLoss: 0.010511\nTrain epoch: 367 [2560/5139 (48%)]\tLoss: 0.014271\nTrain epoch: 367 [5120/5139 (95%)]\tLoss: 0.003212\nTrain epoch: 368 [0/5139 (0%)]\tLoss: 0.005759\nTrain epoch: 368 [2560/5139 (48%)]\tLoss: 0.005136\nTrain epoch: 368 [5120/5139 (95%)]\tLoss: 0.022656\nTrain epoch: 369 [0/5139 (0%)]\tLoss: 0.016468\nTrain epoch: 369 [2560/5139 (48%)]\tLoss: 0.024560\nTrain epoch: 369 [5120/5139 (95%)]\tLoss: 0.000582\nTrain epoch: 370 [0/5139 (0%)]\tLoss: 0.046637\nTrain epoch: 370 [2560/5139 (48%)]\tLoss: 0.010018\nTrain epoch: 370 [5120/5139 (95%)]\tLoss: 0.000967\nTrain epoch: 371 [0/5139 (0%)]\tLoss: 0.002980\nTrain epoch: 371 [2560/5139 (48%)]\tLoss: 0.005143\nTrain epoch: 371 [5120/5139 (95%)]\tLoss: 0.000044\nTrain epoch: 372 [0/5139 (0%)]\tLoss: 0.007652\nTrain epoch: 372 [2560/5139 (48%)]\tLoss: 0.001561\nTrain epoch: 372 [5120/5139 (95%)]\tLoss: 0.002492\nTrain epoch: 373 [0/5139 (0%)]\tLoss: 0.012315\nTrain epoch: 373 [2560/5139 (48%)]\tLoss: 0.005582\nTrain epoch: 373 [5120/5139 (95%)]\tLoss: 0.001227\nTrain epoch: 374 [0/5139 (0%)]\tLoss: 0.006299\nTrain epoch: 374 [2560/5139 (48%)]\tLoss: 0.003515\nTrain epoch: 374 [5120/5139 (95%)]\tLoss: 0.000146\nTrain epoch: 375 [0/5139 (0%)]\tLoss: 0.024473\nTrain epoch: 375 [2560/5139 (48%)]\tLoss: 0.025866\nTrain epoch: 375 [5120/5139 (95%)]\tLoss: 0.000095\nTrain epoch: 376 [0/5139 (0%)]\tLoss: 0.117807\nTrain epoch: 376 [2560/5139 (48%)]\tLoss: 0.144795\nTrain epoch: 376 [5120/5139 (95%)]\tLoss: 0.001598\nTrain epoch: 377 [0/5139 (0%)]\tLoss: 0.009641\nTrain epoch: 377 [2560/5139 (48%)]\tLoss: 0.022828\nTrain epoch: 377 [5120/5139 (95%)]\tLoss: 0.000679\nTrain epoch: 378 [0/5139 (0%)]\tLoss: 0.005202\nTrain epoch: 378 [2560/5139 (48%)]\tLoss: 0.018138\nTrain epoch: 378 [5120/5139 (95%)]\tLoss: 0.002814\nTrain epoch: 379 [0/5139 (0%)]\tLoss: 0.004264\nTrain epoch: 379 [2560/5139 (48%)]\tLoss: 0.014292\nTrain epoch: 379 [5120/5139 (95%)]\tLoss: 0.000987\nTrain epoch: 380 [0/5139 (0%)]\tLoss: 0.004391\nTrain epoch: 380 [2560/5139 (48%)]\tLoss: 0.002053\nTrain epoch: 380 [5120/5139 (95%)]\tLoss: 0.000311\nTrain epoch: 381 [0/5139 (0%)]\tLoss: 0.008984\nTrain epoch: 381 [2560/5139 (48%)]\tLoss: 0.019849\nTrain epoch: 381 [5120/5139 (95%)]\tLoss: 0.003785\nTrain epoch: 382 [0/5139 (0%)]\tLoss: 0.018963\nTrain epoch: 382 [2560/5139 (48%)]\tLoss: 0.013145\nTrain epoch: 382 [5120/5139 (95%)]\tLoss: 0.000145\nTrain epoch: 383 [0/5139 (0%)]\tLoss: 0.008635\nTrain epoch: 383 [2560/5139 (48%)]\tLoss: 0.021656\nTrain epoch: 383 [5120/5139 (95%)]\tLoss: 0.002866\nTrain epoch: 384 [0/5139 (0%)]\tLoss: 0.003420\nTrain epoch: 384 [2560/5139 (48%)]\tLoss: 0.015044\nTrain epoch: 384 [5120/5139 (95%)]\tLoss: 0.000092\nTrain epoch: 385 [0/5139 (0%)]\tLoss: 0.002046\nTrain epoch: 385 [2560/5139 (48%)]\tLoss: 0.005299\nTrain epoch: 385 [5120/5139 (95%)]\tLoss: 0.000553\nTrain epoch: 386 [0/5139 (0%)]\tLoss: 0.001583\nTrain epoch: 386 [2560/5139 (48%)]\tLoss: 0.014785\nTrain epoch: 386 [5120/5139 (95%)]\tLoss: 0.000011\nTrain epoch: 387 [0/5139 (0%)]\tLoss: 0.011870\nTrain epoch: 387 [2560/5139 (48%)]\tLoss: 0.013660\nTrain epoch: 387 [5120/5139 (95%)]\tLoss: 0.292643\nTrain epoch: 388 [0/5139 (0%)]\tLoss: 0.071149\nTrain epoch: 388 [2560/5139 (48%)]\tLoss: 0.122424\nTrain epoch: 388 [5120/5139 (95%)]\tLoss: 0.032020\nTrain epoch: 389 [0/5139 (0%)]\tLoss: 0.046540\nTrain epoch: 389 [2560/5139 (48%)]\tLoss: 0.063965\nTrain epoch: 389 [5120/5139 (95%)]\tLoss: 0.003904\nTrain epoch: 390 [0/5139 (0%)]\tLoss: 0.034580\nTrain epoch: 390 [2560/5139 (48%)]\tLoss: 0.019316\nTrain epoch: 390 [5120/5139 (95%)]\tLoss: 0.016412\nTrain epoch: 391 [0/5139 (0%)]\tLoss: 0.022866\nTrain epoch: 391 [2560/5139 (48%)]\tLoss: 0.004500\nTrain epoch: 391 [5120/5139 (95%)]\tLoss: 0.014609\nTrain epoch: 392 [0/5139 (0%)]\tLoss: 0.019958\nTrain epoch: 392 [2560/5139 (48%)]\tLoss: 0.008532\nTrain epoch: 392 [5120/5139 (95%)]\tLoss: 0.000548\nTrain epoch: 393 [0/5139 (0%)]\tLoss: 0.030067\nTrain epoch: 393 [2560/5139 (48%)]\tLoss: 0.070157\nTrain epoch: 393 [5120/5139 (95%)]\tLoss: 0.002173\nTrain epoch: 394 [0/5139 (0%)]\tLoss: 0.058181\nTrain epoch: 394 [2560/5139 (48%)]\tLoss: 0.051448\nTrain epoch: 394 [5120/5139 (95%)]\tLoss: 0.007615\nTrain epoch: 395 [0/5139 (0%)]\tLoss: 0.044745\nTrain epoch: 395 [2560/5139 (48%)]\tLoss: 0.008990\nTrain epoch: 395 [5120/5139 (95%)]\tLoss: 0.001647\nTrain epoch: 396 [0/5139 (0%)]\tLoss: 0.019414\nTrain epoch: 396 [2560/5139 (48%)]\tLoss: 0.006057\nTrain epoch: 396 [5120/5139 (95%)]\tLoss: 0.012271\nTrain epoch: 397 [0/5139 (0%)]\tLoss: 0.053415\nTrain epoch: 397 [2560/5139 (48%)]\tLoss: 0.014532\nTrain epoch: 397 [5120/5139 (95%)]\tLoss: 0.011881\nTrain epoch: 398 [0/5139 (0%)]\tLoss: 0.019466\nTrain epoch: 398 [2560/5139 (48%)]\tLoss: 0.029484\nTrain epoch: 398 [5120/5139 (95%)]\tLoss: 0.000320\nTrain epoch: 399 [0/5139 (0%)]\tLoss: 0.016996\nTrain epoch: 399 [2560/5139 (48%)]\tLoss: 0.015358\nTrain epoch: 399 [5120/5139 (95%)]\tLoss: 0.111564\nTrain epoch: 400 [0/5139 (0%)]\tLoss: 0.183122\nTrain epoch: 400 [2560/5139 (48%)]\tLoss: 0.025936\nTrain epoch: 400 [5120/5139 (95%)]\tLoss: 0.000838\nTrain epoch: 401 [0/5139 (0%)]\tLoss: 0.075522\nTrain epoch: 401 [2560/5139 (48%)]\tLoss: 0.029401\nTrain epoch: 401 [5120/5139 (95%)]\tLoss: 0.001233\nTrain epoch: 402 [0/5139 (0%)]\tLoss: 0.013701\nTrain epoch: 402 [2560/5139 (48%)]\tLoss: 0.012105\nTrain epoch: 402 [5120/5139 (95%)]\tLoss: 0.008101\nTrain epoch: 403 [0/5139 (0%)]\tLoss: 0.006641\nTrain epoch: 403 [2560/5139 (48%)]\tLoss: 0.025392\nTrain epoch: 403 [5120/5139 (95%)]\tLoss: 0.225230\nTrain epoch: 404 [0/5139 (0%)]\tLoss: 0.359916\nTrain epoch: 404 [2560/5139 (48%)]\tLoss: 0.336680\nTrain epoch: 404 [5120/5139 (95%)]\tLoss: 0.094931\nTrain epoch: 405 [0/5139 (0%)]\tLoss: 0.231100\nTrain epoch: 405 [2560/5139 (48%)]\tLoss: 0.109054\nTrain epoch: 405 [5120/5139 (95%)]\tLoss: 0.025014\nTrain epoch: 406 [0/5139 (0%)]\tLoss: 0.092248\nTrain epoch: 406 [2560/5139 (48%)]\tLoss: 0.073750\nTrain epoch: 406 [5120/5139 (95%)]\tLoss: 0.037384\nTrain epoch: 407 [0/5139 (0%)]\tLoss: 0.020329\nTrain epoch: 407 [2560/5139 (48%)]\tLoss: 0.012006\nTrain epoch: 407 [5120/5139 (95%)]\tLoss: 0.009441\nTrain epoch: 408 [0/5139 (0%)]\tLoss: 0.056728\nTrain epoch: 408 [2560/5139 (48%)]\tLoss: 0.033078\nTrain epoch: 408 [5120/5139 (95%)]\tLoss: 0.002972\nTrain epoch: 409 [0/5139 (0%)]\tLoss: 0.029986\nTrain epoch: 409 [2560/5139 (48%)]\tLoss: 0.018093\nTrain epoch: 409 [5120/5139 (95%)]\tLoss: 0.044932\nTrain epoch: 410 [0/5139 (0%)]\tLoss: 0.028748\nTrain epoch: 410 [2560/5139 (48%)]\tLoss: 0.066770\nTrain epoch: 410 [5120/5139 (95%)]\tLoss: 0.009775\nTrain epoch: 411 [0/5139 (0%)]\tLoss: 0.071883\nTrain epoch: 411 [2560/5139 (48%)]\tLoss: 0.013027\nTrain epoch: 411 [5120/5139 (95%)]\tLoss: 0.010962\nTrain epoch: 412 [0/5139 (0%)]\tLoss: 0.008920\nTrain epoch: 412 [2560/5139 (48%)]\tLoss: 0.020553\nTrain epoch: 412 [5120/5139 (95%)]\tLoss: 0.000756\nTrain epoch: 413 [0/5139 (0%)]\tLoss: 0.015405\nTrain epoch: 413 [2560/5139 (48%)]\tLoss: 0.013984\nTrain epoch: 413 [5120/5139 (95%)]\tLoss: 0.002571\nTrain epoch: 414 [0/5139 (0%)]\tLoss: 0.055887\nTrain epoch: 414 [2560/5139 (48%)]\tLoss: 0.014505\nTrain epoch: 414 [5120/5139 (95%)]\tLoss: 0.000966\nTrain epoch: 415 [0/5139 (0%)]\tLoss: 0.017772\nTrain epoch: 415 [2560/5139 (48%)]\tLoss: 0.004937\nTrain epoch: 415 [5120/5139 (95%)]\tLoss: 0.037138\nTrain epoch: 416 [0/5139 (0%)]\tLoss: 0.007624\nTrain epoch: 416 [2560/5139 (48%)]\tLoss: 0.044724\nTrain epoch: 416 [5120/5139 (95%)]\tLoss: 0.000080\nTrain epoch: 417 [0/5139 (0%)]\tLoss: 0.095952\nTrain epoch: 417 [2560/5139 (48%)]\tLoss: 0.003350\nTrain epoch: 417 [5120/5139 (95%)]\tLoss: 0.001954\nTrain epoch: 418 [0/5139 (0%)]\tLoss: 0.027398\nTrain epoch: 418 [2560/5139 (48%)]\tLoss: 0.064287\nTrain epoch: 418 [5120/5139 (95%)]\tLoss: 0.014431\nTrain epoch: 419 [0/5139 (0%)]\tLoss: 0.010303\nTrain epoch: 419 [2560/5139 (48%)]\tLoss: 0.005661\nTrain epoch: 419 [5120/5139 (95%)]\tLoss: 0.028767\nTrain epoch: 420 [0/5139 (0%)]\tLoss: 0.004830\nTrain epoch: 420 [2560/5139 (48%)]\tLoss: 0.007762\nTrain epoch: 420 [5120/5139 (95%)]\tLoss: 0.000205\nTrain epoch: 421 [0/5139 (0%)]\tLoss: 0.013795\nTrain epoch: 421 [2560/5139 (48%)]\tLoss: 0.022543\nTrain epoch: 421 [5120/5139 (95%)]\tLoss: 0.000547\nTrain epoch: 422 [0/5139 (0%)]\tLoss: 0.036667\nTrain epoch: 422 [2560/5139 (48%)]\tLoss: 0.010221\nTrain epoch: 422 [5120/5139 (95%)]\tLoss: 0.000848\nTrain epoch: 423 [0/5139 (0%)]\tLoss: 0.009286\nTrain epoch: 423 [2560/5139 (48%)]\tLoss: 0.013612\nTrain epoch: 423 [5120/5139 (95%)]\tLoss: 0.022419\nTrain epoch: 424 [0/5139 (0%)]\tLoss: 0.006491\nTrain epoch: 424 [2560/5139 (48%)]\tLoss: 0.025739\nTrain epoch: 424 [5120/5139 (95%)]\tLoss: 0.000194\nTrain epoch: 425 [0/5139 (0%)]\tLoss: 0.013531\nTrain epoch: 425 [2560/5139 (48%)]\tLoss: 0.029284\nTrain epoch: 425 [5120/5139 (95%)]\tLoss: 0.067458\nTrain epoch: 426 [0/5139 (0%)]\tLoss: 0.015529\nTrain epoch: 426 [2560/5139 (48%)]\tLoss: 0.038477\nTrain epoch: 426 [5120/5139 (95%)]\tLoss: 0.010900\nTrain epoch: 427 [0/5139 (0%)]\tLoss: 0.010988\nTrain epoch: 427 [2560/5139 (48%)]\tLoss: 0.003964\nTrain epoch: 427 [5120/5139 (95%)]\tLoss: 0.011464\nTrain epoch: 428 [0/5139 (0%)]\tLoss: 0.022247\nTrain epoch: 428 [2560/5139 (48%)]\tLoss: 0.028702\nTrain epoch: 428 [5120/5139 (95%)]\tLoss: 0.011937\nTrain epoch: 429 [0/5139 (0%)]\tLoss: 0.033442\nTrain epoch: 429 [2560/5139 (48%)]\tLoss: 0.015477\nTrain epoch: 429 [5120/5139 (95%)]\tLoss: 0.001448\nTrain epoch: 430 [0/5139 (0%)]\tLoss: 0.026745\nTrain epoch: 430 [2560/5139 (48%)]\tLoss: 0.031845\nTrain epoch: 430 [5120/5139 (95%)]\tLoss: 0.004927\nTrain epoch: 431 [0/5139 (0%)]\tLoss: 0.009926\nTrain epoch: 431 [2560/5139 (48%)]\tLoss: 0.032106\nTrain epoch: 431 [5120/5139 (95%)]\tLoss: 0.144464\nTrain epoch: 432 [0/5139 (0%)]\tLoss: 0.019243\nTrain epoch: 432 [2560/5139 (48%)]\tLoss: 0.093008\nTrain epoch: 432 [5120/5139 (95%)]\tLoss: 0.003035\nTrain epoch: 433 [0/5139 (0%)]\tLoss: 0.023468\nTrain epoch: 433 [2560/5139 (48%)]\tLoss: 0.013723\nTrain epoch: 433 [5120/5139 (95%)]\tLoss: 0.011074\nTrain epoch: 434 [0/5139 (0%)]\tLoss: 0.033558\nTrain epoch: 434 [2560/5139 (48%)]\tLoss: 0.001954\nTrain epoch: 434 [5120/5139 (95%)]\tLoss: 0.000208\nTrain epoch: 435 [0/5139 (0%)]\tLoss: 0.014507\nTrain epoch: 435 [2560/5139 (48%)]\tLoss: 0.006504\nTrain epoch: 435 [5120/5139 (95%)]\tLoss: 0.021712\nTrain epoch: 436 [0/5139 (0%)]\tLoss: 0.022025\nTrain epoch: 436 [2560/5139 (48%)]\tLoss: 0.003199\nTrain epoch: 436 [5120/5139 (95%)]\tLoss: 0.000663\nTrain epoch: 437 [0/5139 (0%)]\tLoss: 0.002518\nTrain epoch: 437 [2560/5139 (48%)]\tLoss: 0.005639\nTrain epoch: 437 [5120/5139 (95%)]\tLoss: 0.014521\nTrain epoch: 438 [0/5139 (0%)]\tLoss: 0.007497\nTrain epoch: 438 [2560/5139 (48%)]\tLoss: 0.002310\nTrain epoch: 438 [5120/5139 (95%)]\tLoss: 0.000020\nTrain epoch: 439 [0/5139 (0%)]\tLoss: 0.044190\nTrain epoch: 439 [2560/5139 (48%)]\tLoss: 0.017787\nTrain epoch: 439 [5120/5139 (95%)]\tLoss: 0.000060\nTrain epoch: 440 [0/5139 (0%)]\tLoss: 0.106454\nTrain epoch: 440 [2560/5139 (48%)]\tLoss: 0.002752\nTrain epoch: 440 [5120/5139 (95%)]\tLoss: 0.016525\nTrain epoch: 441 [0/5139 (0%)]\tLoss: 0.011351\nTrain epoch: 441 [2560/5139 (48%)]\tLoss: 0.014435\nTrain epoch: 441 [5120/5139 (95%)]\tLoss: 0.000705\nTrain epoch: 442 [0/5139 (0%)]\tLoss: 0.006093\nTrain epoch: 442 [2560/5139 (48%)]\tLoss: 0.004173\nTrain epoch: 442 [5120/5139 (95%)]\tLoss: 0.007759\nTrain epoch: 443 [0/5139 (0%)]\tLoss: 0.010616\nTrain epoch: 443 [2560/5139 (48%)]\tLoss: 0.007079\nTrain epoch: 443 [5120/5139 (95%)]\tLoss: 0.015847\nTrain epoch: 444 [0/5139 (0%)]\tLoss: 0.004817\nTrain epoch: 444 [2560/5139 (48%)]\tLoss: 0.026809\nTrain epoch: 444 [5120/5139 (95%)]\tLoss: 0.000981\nTrain epoch: 445 [0/5139 (0%)]\tLoss: 0.028273\nTrain epoch: 445 [2560/5139 (48%)]\tLoss: 0.020575\nTrain epoch: 445 [5120/5139 (95%)]\tLoss: 0.001300\nTrain epoch: 446 [0/5139 (0%)]\tLoss: 0.017564\nTrain epoch: 446 [2560/5139 (48%)]\tLoss: 0.008130\nTrain epoch: 446 [5120/5139 (95%)]\tLoss: 0.003302\nTrain epoch: 447 [0/5139 (0%)]\tLoss: 0.003811\nTrain epoch: 447 [2560/5139 (48%)]\tLoss: 0.010275\nTrain epoch: 447 [5120/5139 (95%)]\tLoss: 0.001373\nTrain epoch: 448 [0/5139 (0%)]\tLoss: 0.031873\nTrain epoch: 448 [2560/5139 (48%)]\tLoss: 0.024598\nTrain epoch: 448 [5120/5139 (95%)]\tLoss: 0.000000\nTrain epoch: 449 [0/5139 (0%)]\tLoss: 0.012334\nTrain epoch: 449 [2560/5139 (48%)]\tLoss: 0.010491\nTrain epoch: 449 [5120/5139 (95%)]\tLoss: 0.000430\nTrain epoch: 450 [0/5139 (0%)]\tLoss: 0.002941\nTrain epoch: 450 [2560/5139 (48%)]\tLoss: 0.014264\nTrain epoch: 450 [5120/5139 (95%)]\tLoss: 0.000284\nTrain epoch: 451 [0/5139 (0%)]\tLoss: 0.011460\nTrain epoch: 451 [2560/5139 (48%)]\tLoss: 0.012324\nTrain epoch: 451 [5120/5139 (95%)]\tLoss: 0.000128\nTrain epoch: 452 [0/5139 (0%)]\tLoss: 0.073501\nTrain epoch: 452 [2560/5139 (48%)]\tLoss: 0.008969\nTrain epoch: 452 [5120/5139 (95%)]\tLoss: 0.003434\nTrain epoch: 453 [0/5139 (0%)]\tLoss: 0.048504\nTrain epoch: 453 [2560/5139 (48%)]\tLoss: 0.014290\nTrain epoch: 453 [5120/5139 (95%)]\tLoss: 0.000011\nTrain epoch: 454 [0/5139 (0%)]\tLoss: 0.012295\nTrain epoch: 454 [2560/5139 (48%)]\tLoss: 0.003663\nTrain epoch: 454 [5120/5139 (95%)]\tLoss: 0.000969\nTrain epoch: 455 [0/5139 (0%)]\tLoss: 0.018393\nTrain epoch: 455 [2560/5139 (48%)]\tLoss: 0.013577\nTrain epoch: 455 [5120/5139 (95%)]\tLoss: 0.001156\nTrain epoch: 456 [0/5139 (0%)]\tLoss: 0.029958\nTrain epoch: 456 [2560/5139 (48%)]\tLoss: 0.005245\nTrain epoch: 456 [5120/5139 (95%)]\tLoss: 0.003960\nTrain epoch: 457 [0/5139 (0%)]\tLoss: 0.014375\nTrain epoch: 457 [2560/5139 (48%)]\tLoss: 0.003293\nTrain epoch: 457 [5120/5139 (95%)]\tLoss: 0.000104\nTrain epoch: 458 [0/5139 (0%)]\tLoss: 0.026651\nTrain epoch: 458 [2560/5139 (48%)]\tLoss: 0.001849\nTrain epoch: 458 [5120/5139 (95%)]\tLoss: 0.004153\nTrain epoch: 459 [0/5139 (0%)]\tLoss: 0.017383\nTrain epoch: 459 [2560/5139 (48%)]\tLoss: 0.085021\nTrain epoch: 459 [5120/5139 (95%)]\tLoss: 0.231925\nTrain epoch: 460 [0/5139 (0%)]\tLoss: 0.159270\nTrain epoch: 460 [2560/5139 (48%)]\tLoss: 0.081046\nTrain epoch: 460 [5120/5139 (95%)]\tLoss: 0.078657\nTrain epoch: 461 [0/5139 (0%)]\tLoss: 0.071593\nTrain epoch: 461 [2560/5139 (48%)]\tLoss: 0.039355\nTrain epoch: 461 [5120/5139 (95%)]\tLoss: 0.073962\nTrain epoch: 462 [0/5139 (0%)]\tLoss: 0.032686\nTrain epoch: 462 [2560/5139 (48%)]\tLoss: 0.036993\nTrain epoch: 462 [5120/5139 (95%)]\tLoss: 0.024978\nTrain epoch: 463 [0/5139 (0%)]\tLoss: 0.028607\nTrain epoch: 463 [2560/5139 (48%)]\tLoss: 0.029858\nTrain epoch: 463 [5120/5139 (95%)]\tLoss: 0.027211\nTrain epoch: 464 [0/5139 (0%)]\tLoss: 0.022036\nTrain epoch: 464 [2560/5139 (48%)]\tLoss: 0.020465\nTrain epoch: 464 [5120/5139 (95%)]\tLoss: 0.000562\nTrain epoch: 465 [0/5139 (0%)]\tLoss: 0.042280\nTrain epoch: 465 [2560/5139 (48%)]\tLoss: 0.029640\nTrain epoch: 465 [5120/5139 (95%)]\tLoss: 0.000493\nTrain epoch: 466 [0/5139 (0%)]\tLoss: 0.027204\nTrain epoch: 466 [2560/5139 (48%)]\tLoss: 0.006237\nTrain epoch: 466 [5120/5139 (95%)]\tLoss: 0.012926\nTrain epoch: 467 [0/5139 (0%)]\tLoss: 0.006770\nTrain epoch: 467 [2560/5139 (48%)]\tLoss: 0.002265\nTrain epoch: 467 [5120/5139 (95%)]\tLoss: 0.000186\nTrain epoch: 468 [0/5139 (0%)]\tLoss: 0.014319\nTrain epoch: 468 [2560/5139 (48%)]\tLoss: 0.016351\nTrain epoch: 468 [5120/5139 (95%)]\tLoss: 0.000313\nTrain epoch: 469 [0/5139 (0%)]\tLoss: 0.018027\nTrain epoch: 469 [2560/5139 (48%)]\tLoss: 0.010216\nTrain epoch: 469 [5120/5139 (95%)]\tLoss: 0.000427\nTrain epoch: 470 [0/5139 (0%)]\tLoss: 0.039026\nTrain epoch: 470 [2560/5139 (48%)]\tLoss: 0.003078\nTrain epoch: 470 [5120/5139 (95%)]\tLoss: 0.000133\nTrain epoch: 471 [0/5139 (0%)]\tLoss: 0.008881\nTrain epoch: 471 [2560/5139 (48%)]\tLoss: 0.001870\nTrain epoch: 471 [5120/5139 (95%)]\tLoss: 0.000014\nTrain epoch: 472 [0/5139 (0%)]\tLoss: 0.018903\nTrain epoch: 472 [2560/5139 (48%)]\tLoss: 0.014643\nTrain epoch: 472 [5120/5139 (95%)]\tLoss: 0.000307\nTrain epoch: 473 [0/5139 (0%)]\tLoss: 0.048947\nTrain epoch: 473 [2560/5139 (48%)]\tLoss: 0.011615\nTrain epoch: 473 [5120/5139 (95%)]\tLoss: 0.000341\nTrain epoch: 474 [0/5139 (0%)]\tLoss: 0.005811\nTrain epoch: 474 [2560/5139 (48%)]\tLoss: 0.012092\nTrain epoch: 474 [5120/5139 (95%)]\tLoss: 0.000359\nTrain epoch: 475 [0/5139 (0%)]\tLoss: 0.013476\nTrain epoch: 475 [2560/5139 (48%)]\tLoss: 0.021835\nTrain epoch: 475 [5120/5139 (95%)]\tLoss: 0.000428\nTrain epoch: 476 [0/5139 (0%)]\tLoss: 0.007660\nTrain epoch: 476 [2560/5139 (48%)]\tLoss: 0.010979\nTrain epoch: 476 [5120/5139 (95%)]\tLoss: 0.003691\nTrain epoch: 477 [0/5139 (0%)]\tLoss: 0.002448\nTrain epoch: 477 [2560/5139 (48%)]\tLoss: 0.007182\nTrain epoch: 477 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 478 [0/5139 (0%)]\tLoss: 0.016878\nTrain epoch: 478 [2560/5139 (48%)]\tLoss: 0.001521\nTrain epoch: 478 [5120/5139 (95%)]\tLoss: 0.000058\nTrain epoch: 479 [0/5139 (0%)]\tLoss: 0.003154\nTrain epoch: 479 [2560/5139 (48%)]\tLoss: 0.033651\nTrain epoch: 479 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 480 [0/5139 (0%)]\tLoss: 0.007877\nTrain epoch: 480 [2560/5139 (48%)]\tLoss: 0.003731\nTrain epoch: 480 [5120/5139 (95%)]\tLoss: 0.001013\nTrain epoch: 481 [0/5139 (0%)]\tLoss: 0.016465\nTrain epoch: 481 [2560/5139 (48%)]\tLoss: 0.002711\nTrain epoch: 481 [5120/5139 (95%)]\tLoss: 0.000003\nTrain epoch: 482 [0/5139 (0%)]\tLoss: 0.001218\nTrain epoch: 482 [2560/5139 (48%)]\tLoss: 0.001431\nTrain epoch: 482 [5120/5139 (95%)]\tLoss: 0.003178\nTrain epoch: 483 [0/5139 (0%)]\tLoss: 0.000490\nTrain epoch: 483 [2560/5139 (48%)]\tLoss: 0.011506\nTrain epoch: 483 [5120/5139 (95%)]\tLoss: 0.000726\nTrain epoch: 484 [0/5139 (0%)]\tLoss: 0.006177\nTrain epoch: 484 [2560/5139 (48%)]\tLoss: 0.005642\nTrain epoch: 484 [5120/5139 (95%)]\tLoss: 0.000385\nTrain epoch: 485 [0/5139 (0%)]\tLoss: 0.001614\nTrain epoch: 485 [2560/5139 (48%)]\tLoss: 0.002124\nTrain epoch: 485 [5120/5139 (95%)]\tLoss: 0.000005\nTrain epoch: 486 [0/5139 (0%)]\tLoss: 0.001162\nTrain epoch: 486 [2560/5139 (48%)]\tLoss: 0.070165\nTrain epoch: 486 [5120/5139 (95%)]\tLoss: 0.000745\nTrain epoch: 487 [0/5139 (0%)]\tLoss: 0.131717\nTrain epoch: 487 [2560/5139 (48%)]\tLoss: 0.003776\nTrain epoch: 487 [5120/5139 (95%)]\tLoss: 0.036989\nTrain epoch: 488 [0/5139 (0%)]\tLoss: 0.019062\nTrain epoch: 488 [2560/5139 (48%)]\tLoss: 0.007228\nTrain epoch: 488 [5120/5139 (95%)]\tLoss: 0.032318\nTrain epoch: 489 [0/5139 (0%)]\tLoss: 0.006355\nTrain epoch: 489 [2560/5139 (48%)]\tLoss: 0.069192\nTrain epoch: 489 [5120/5139 (95%)]\tLoss: 0.001483\nTrain epoch: 490 [0/5139 (0%)]\tLoss: 0.107614\nTrain epoch: 490 [2560/5139 (48%)]\tLoss: 0.014202\nTrain epoch: 490 [5120/5139 (95%)]\tLoss: 0.000047\nTrain epoch: 491 [0/5139 (0%)]\tLoss: 0.019209\nTrain epoch: 491 [2560/5139 (48%)]\tLoss: 0.005879\nTrain epoch: 491 [2560/5139 (48%)]\tLoss: 0.005879\nTrain epoch: 491 [5120/5139 (95%)]\tLoss: 0.001459\nTrain epoch: 492 [0/5139 (0%)]\tLoss: 0.006895\nTrain epoch: 492 [2560/5139 (48%)]\tLoss: 0.003217\nTrain epoch: 492 [5120/5139 (95%)]\tLoss: 0.000122\nTrain epoch: 493 [0/5139 (0%)]\tLoss: 0.004486\nTrain epoch: 493 [2560/5139 (48%)]\tLoss: 0.007846\nTrain epoch: 493 [5120/5139 (95%)]\tLoss: 0.000136\nTrain epoch: 494 [0/5139 (0%)]\tLoss: 0.004951\nTrain epoch: 494 [2560/5139 (48%)]\tLoss: 0.004921\nTrain epoch: 494 [5120/5139 (95%)]\tLoss: 0.000226\nTrain epoch: 495 [0/5139 (0%)]\tLoss: 0.001262\nTrain epoch: 495 [2560/5139 (48%)]\tLoss: 0.028665\nTrain epoch: 495 [5120/5139 (95%)]\tLoss: 0.002165\nTrain epoch: 496 [0/5139 (0%)]\tLoss: 0.012350\nTrain epoch: 496 [2560/5139 (48%)]\tLoss: 0.001972\nTrain epoch: 496 [5120/5139 (95%)]\tLoss: 0.000131\nTrain epoch: 497 [0/5139 (0%)]\tLoss: 0.024023\nTrain epoch: 497 [2560/5139 (48%)]\tLoss: 0.001554\nTrain epoch: 497 [5120/5139 (95%)]\tLoss: 0.000415\nTrain epoch: 498 [0/5139 (0%)]\tLoss: 0.010432\nTrain epoch: 498 [2560/5139 (48%)]\tLoss: 0.002163\nTrain epoch: 498 [5120/5139 (95%)]\tLoss: 0.000004\nTrain epoch: 499 [0/5139 (0%)]\tLoss: 0.003022\nTrain epoch: 499 [2560/5139 (48%)]\tLoss: 0.023012\nTrain epoch: 499 [5120/5139 (95%)]\tLoss: 0.000156\nTrain epoch: 500 [0/5139 (0%)]\tLoss: 0.000842\nTrain epoch: 500 [2560/5139 (48%)]\tLoss: 0.036053\nTrain epoch: 500 [5120/5139 (95%)]\tLoss: 0.121909\nTrain epoch: 501 [0/5139 (0%)]\tLoss: 0.146459\nTrain epoch: 501 [2560/5139 (48%)]\tLoss: 0.078404\nTrain epoch: 501 [5120/5139 (95%)]\tLoss: 0.231591\nTrain epoch: 502 [0/5139 (0%)]\tLoss: 0.102519\nTrain epoch: 502 [2560/5139 (48%)]\tLoss: 0.111860\nTrain epoch: 502 [5120/5139 (95%)]\tLoss: 0.044832\nTrain epoch: 503 [0/5139 (0%)]\tLoss: 0.062279\nTrain epoch: 503 [2560/5139 (48%)]\tLoss: 0.032499\nTrain epoch: 503 [5120/5139 (95%)]\tLoss: 0.135387\nTrain epoch: 504 [0/5139 (0%)]\tLoss: 0.020511\nTrain epoch: 504 [2560/5139 (48%)]\tLoss: 0.042058\nTrain epoch: 504 [5120/5139 (95%)]\tLoss: 0.046828\nTrain epoch: 505 [0/5139 (0%)]\tLoss: 0.035633\nTrain epoch: 505 [2560/5139 (48%)]\tLoss: 0.034134\nTrain epoch: 505 [5120/5139 (95%)]\tLoss: 0.049198\nTrain epoch: 506 [0/5139 (0%)]\tLoss: 0.020963\nTrain epoch: 506 [2560/5139 (48%)]\tLoss: 0.015834\nTrain epoch: 506 [5120/5139 (95%)]\tLoss: 0.040219\nTrain epoch: 507 [0/5139 (0%)]\tLoss: 0.017950\nTrain epoch: 507 [2560/5139 (48%)]\tLoss: 0.021482\nTrain epoch: 507 [5120/5139 (95%)]\tLoss: 0.000710\nTrain epoch: 508 [0/5139 (0%)]\tLoss: 0.013520\nTrain epoch: 508 [2560/5139 (48%)]\tLoss: 0.007439\nTrain epoch: 508 [5120/5139 (95%)]\tLoss: 0.000772\nTrain epoch: 509 [0/5139 (0%)]\tLoss: 0.049070\nTrain epoch: 509 [2560/5139 (48%)]\tLoss: 0.046974\nTrain epoch: 509 [5120/5139 (95%)]\tLoss: 0.000201\nTrain epoch: 510 [0/5139 (0%)]\tLoss: 0.007913\nTrain epoch: 510 [2560/5139 (48%)]\tLoss: 0.001326\nTrain epoch: 510 [5120/5139 (95%)]\tLoss: 0.000030\nTrain epoch: 511 [0/5139 (0%)]\tLoss: 0.022453\nTrain epoch: 511 [2560/5139 (48%)]\tLoss: 0.016231\nTrain epoch: 511 [5120/5139 (95%)]\tLoss: 0.001645\nTrain epoch: 512 [0/5139 (0%)]\tLoss: 0.019364\nTrain epoch: 512 [2560/5139 (48%)]\tLoss: 0.005866\nTrain epoch: 512 [5120/5139 (95%)]\tLoss: 0.000016\nTrain epoch: 513 [0/5139 (0%)]\tLoss: 0.027507\nTrain epoch: 513 [2560/5139 (48%)]\tLoss: 0.008178\nTrain epoch: 513 [5120/5139 (95%)]\tLoss: 0.085300\nTrain epoch: 514 [0/5139 (0%)]\tLoss: 0.092960\nTrain epoch: 514 [2560/5139 (48%)]\tLoss: 0.288782\nTrain epoch: 514 [5120/5139 (95%)]\tLoss: 0.189701\nTrain epoch: 515 [0/5139 (0%)]\tLoss: 0.169054\nTrain epoch: 515 [2560/5139 (48%)]\tLoss: 0.137804\nTrain epoch: 515 [5120/5139 (95%)]\tLoss: 0.014428\nTrain epoch: 516 [0/5139 (0%)]\tLoss: 0.096817\nTrain epoch: 516 [2560/5139 (48%)]\tLoss: 0.064056\nTrain epoch: 516 [5120/5139 (95%)]\tLoss: 0.207005\nTrain epoch: 517 [0/5139 (0%)]\tLoss: 0.122930\nTrain epoch: 517 [2560/5139 (48%)]\tLoss: 0.040657\nTrain epoch: 517 [5120/5139 (95%)]\tLoss: 0.048013\nTrain epoch: 518 [0/5139 (0%)]\tLoss: 0.013250\nTrain epoch: 518 [2560/5139 (48%)]\tLoss: 0.125374\nTrain epoch: 518 [5120/5139 (95%)]\tLoss: 0.045125\nTrain epoch: 519 [0/5139 (0%)]\tLoss: 0.070261\nTrain epoch: 519 [2560/5139 (48%)]\tLoss: 0.043179\nTrain epoch: 519 [5120/5139 (95%)]\tLoss: 0.199643\nTrain epoch: 520 [0/5139 (0%)]\tLoss: 0.034220\nTrain epoch: 520 [2560/5139 (48%)]\tLoss: 0.041623\nTrain epoch: 520 [5120/5139 (95%)]\tLoss: 0.005481\nTrain epoch: 521 [0/5139 (0%)]\tLoss: 0.035899\nTrain epoch: 521 [2560/5139 (48%)]\tLoss: 0.028240\nTrain epoch: 521 [5120/5139 (95%)]\tLoss: 0.001127\nTrain epoch: 522 [0/5139 (0%)]\tLoss: 0.031105\nTrain epoch: 522 [2560/5139 (48%)]\tLoss: 0.013025\nTrain epoch: 522 [5120/5139 (95%)]\tLoss: 0.001996\nTrain epoch: 523 [0/5139 (0%)]\tLoss: 0.024518\nTrain epoch: 523 [2560/5139 (48%)]\tLoss: 0.005949\nTrain epoch: 523 [5120/5139 (95%)]\tLoss: 0.095931\nTrain epoch: 524 [0/5139 (0%)]\tLoss: 0.013677\nTrain epoch: 524 [2560/5139 (48%)]\tLoss: 0.008054\nTrain epoch: 524 [5120/5139 (95%)]\tLoss: 0.040782\nTrain epoch: 525 [0/5139 (0%)]\tLoss: 0.006832\nTrain epoch: 525 [2560/5139 (48%)]\tLoss: 0.014876\nTrain epoch: 525 [5120/5139 (95%)]\tLoss: 0.001493\nTrain epoch: 526 [0/5139 (0%)]\tLoss: 0.014513\nTrain epoch: 526 [2560/5139 (48%)]\tLoss: 0.025004\nTrain epoch: 526 [5120/5139 (95%)]\tLoss: 0.000061\nTrain epoch: 527 [0/5139 (0%)]\tLoss: 0.014385\nTrain epoch: 527 [2560/5139 (48%)]\tLoss: 0.008211\nTrain epoch: 527 [5120/5139 (95%)]\tLoss: 0.002407\nTrain epoch: 528 [0/5139 (0%)]\tLoss: 0.014627\nTrain epoch: 528 [2560/5139 (48%)]\tLoss: 0.004918\nTrain epoch: 528 [5120/5139 (95%)]\tLoss: 0.000050\nTrain epoch: 529 [0/5139 (0%)]\tLoss: 0.021843\nTrain epoch: 529 [2560/5139 (48%)]\tLoss: 0.013159\nTrain epoch: 529 [5120/5139 (95%)]\tLoss: 0.007994\nTrain epoch: 530 [0/5139 (0%)]\tLoss: 0.002043\nTrain epoch: 530 [2560/5139 (48%)]\tLoss: 0.023089\nTrain epoch: 530 [5120/5139 (95%)]\tLoss: 0.003352\nTrain epoch: 531 [0/5139 (0%)]\tLoss: 0.004652\nTrain epoch: 531 [2560/5139 (48%)]\tLoss: 0.010494\nTrain epoch: 531 [5120/5139 (95%)]\tLoss: 0.005849\nTrain epoch: 532 [0/5139 (0%)]\tLoss: 0.012497\nTrain epoch: 532 [2560/5139 (48%)]\tLoss: 0.022296\nTrain epoch: 532 [5120/5139 (95%)]\tLoss: 0.029244\nTrain epoch: 533 [0/5139 (0%)]\tLoss: 0.030596\nTrain epoch: 533 [2560/5139 (48%)]\tLoss: 0.000626\nTrain epoch: 533 [5120/5139 (95%)]\tLoss: 0.000195\nTrain epoch: 534 [0/5139 (0%)]\tLoss: 0.080692\nTrain epoch: 534 [2560/5139 (48%)]\tLoss: 0.031130\nTrain epoch: 534 [5120/5139 (95%)]\tLoss: 0.001467\nTrain epoch: 535 [0/5139 (0%)]\tLoss: 0.007555\nTrain epoch: 535 [2560/5139 (48%)]\tLoss: 0.027302\nTrain epoch: 535 [5120/5139 (95%)]\tLoss: 0.000403\nTrain epoch: 536 [0/5139 (0%)]\tLoss: 0.008466\nTrain epoch: 536 [2560/5139 (48%)]\tLoss: 0.003867\nTrain epoch: 536 [5120/5139 (95%)]\tLoss: 0.003223\nTrain epoch: 537 [0/5139 (0%)]\tLoss: 0.003118\nTrain epoch: 537 [2560/5139 (48%)]\tLoss: 0.005956\nTrain epoch: 537 [5120/5139 (95%)]\tLoss: 0.001881\nTrain epoch: 538 [0/5139 (0%)]\tLoss: 0.001137\nTrain epoch: 538 [2560/5139 (48%)]\tLoss: 0.012720\nTrain epoch: 538 [5120/5139 (95%)]\tLoss: 0.000221\nTrain epoch: 539 [0/5139 (0%)]\tLoss: 0.011077\nTrain epoch: 539 [2560/5139 (48%)]\tLoss: 0.002704\nTrain epoch: 539 [5120/5139 (95%)]\tLoss: 0.001057\nTrain epoch: 540 [0/5139 (0%)]\tLoss: 0.001985\nTrain epoch: 540 [2560/5139 (48%)]\tLoss: 0.000700\nTrain epoch: 540 [5120/5139 (95%)]\tLoss: 0.000210\nTrain epoch: 541 [0/5139 (0%)]\tLoss: 0.016618\nTrain epoch: 541 [2560/5139 (48%)]\tLoss: 0.022431\nTrain epoch: 541 [5120/5139 (95%)]\tLoss: 0.000219\nTrain epoch: 542 [0/5139 (0%)]\tLoss: 0.018426\nTrain epoch: 542 [2560/5139 (48%)]\tLoss: 0.002682\nTrain epoch: 542 [5120/5139 (95%)]\tLoss: 0.004775\nTrain epoch: 543 [0/5139 (0%)]\tLoss: 0.012201\nTrain epoch: 543 [2560/5139 (48%)]\tLoss: 0.004188\nTrain epoch: 543 [5120/5139 (95%)]\tLoss: 0.003430\nTrain epoch: 544 [0/5139 (0%)]\tLoss: 0.016902\nTrain epoch: 544 [2560/5139 (48%)]\tLoss: 0.007674\nTrain epoch: 544 [5120/5139 (95%)]\tLoss: 0.000044\nTrain epoch: 545 [0/5139 (0%)]\tLoss: 0.001843\nTrain epoch: 545 [2560/5139 (48%)]\tLoss: 0.006926\nTrain epoch: 545 [5120/5139 (95%)]\tLoss: 0.000178\nTrain epoch: 546 [0/5139 (0%)]\tLoss: 0.018435\nTrain epoch: 546 [2560/5139 (48%)]\tLoss: 0.008656\nTrain epoch: 546 [5120/5139 (95%)]\tLoss: 0.001819\nTrain epoch: 547 [0/5139 (0%)]\tLoss: 0.025906\nTrain epoch: 547 [2560/5139 (48%)]\tLoss: 0.013044\nTrain epoch: 547 [5120/5139 (95%)]\tLoss: 0.003748\nTrain epoch: 548 [0/5139 (0%)]\tLoss: 0.071519\nTrain epoch: 548 [2560/5139 (48%)]\tLoss: 0.046366\nTrain epoch: 548 [5120/5139 (95%)]\tLoss: 0.006705\nTrain epoch: 549 [0/5139 (0%)]\tLoss: 0.001942\nTrain epoch: 549 [2560/5139 (48%)]\tLoss: 0.008421\nTrain epoch: 549 [5120/5139 (95%)]\tLoss: 0.001167\nTrain epoch: 550 [0/5139 (0%)]\tLoss: 0.002522\nTrain epoch: 550 [2560/5139 (48%)]\tLoss: 0.001229\nTrain epoch: 550 [5120/5139 (95%)]\tLoss: 0.001662\nTrain epoch: 551 [0/5139 (0%)]\tLoss: 0.002820\nTrain epoch: 551 [2560/5139 (48%)]\tLoss: 0.002001\nTrain epoch: 551 [5120/5139 (95%)]\tLoss: 0.000185\nTrain epoch: 552 [0/5139 (0%)]\tLoss: 0.003086\nTrain epoch: 552 [2560/5139 (48%)]\tLoss: 0.002697\nTrain epoch: 552 [5120/5139 (95%)]\tLoss: 0.001467\nTrain epoch: 553 [0/5139 (0%)]\tLoss: 0.013364\nTrain epoch: 553 [2560/5139 (48%)]\tLoss: 0.003983\nTrain epoch: 553 [5120/5139 (95%)]\tLoss: 0.001222\nTrain epoch: 554 [0/5139 (0%)]\tLoss: 0.001208\nTrain epoch: 554 [2560/5139 (48%)]\tLoss: 0.002913\nTrain epoch: 554 [5120/5139 (95%)]\tLoss: 0.000031\nTrain epoch: 555 [0/5139 (0%)]\tLoss: 0.001261\nTrain epoch: 555 [2560/5139 (48%)]\tLoss: 0.033950\nTrain epoch: 555 [5120/5139 (95%)]\tLoss: 0.009424\nTrain epoch: 556 [0/5139 (0%)]\tLoss: 0.041975\nTrain epoch: 556 [2560/5139 (48%)]\tLoss: 0.033602\nTrain epoch: 556 [5120/5139 (95%)]\tLoss: 0.002594\nTrain epoch: 557 [0/5139 (0%)]\tLoss: 0.009167\nTrain epoch: 557 [2560/5139 (48%)]\tLoss: 0.004194\nTrain epoch: 557 [5120/5139 (95%)]\tLoss: 0.001541\nTrain epoch: 558 [0/5139 (0%)]\tLoss: 0.010086\nTrain epoch: 558 [2560/5139 (48%)]\tLoss: 0.004605\nTrain epoch: 558 [5120/5139 (95%)]\tLoss: 0.001206\nTrain epoch: 559 [0/5139 (0%)]\tLoss: 0.010738\nTrain epoch: 559 [2560/5139 (48%)]\tLoss: 0.013493\nTrain epoch: 559 [5120/5139 (95%)]\tLoss: 0.000492\nTrain epoch: 560 [0/5139 (0%)]\tLoss: 0.005161\nTrain epoch: 560 [2560/5139 (48%)]\tLoss: 0.002829\nTrain epoch: 560 [5120/5139 (95%)]\tLoss: 0.000037\nTrain epoch: 561 [0/5139 (0%)]\tLoss: 0.004342\nTrain epoch: 561 [2560/5139 (48%)]\tLoss: 0.007838\nTrain epoch: 561 [5120/5139 (95%)]\tLoss: 0.001550\nTrain epoch: 562 [0/5139 (0%)]\tLoss: 0.000622\nTrain epoch: 562 [2560/5139 (48%)]\tLoss: 0.001887\nTrain epoch: 562 [5120/5139 (95%)]\tLoss: 0.001558\nTrain epoch: 563 [0/5139 (0%)]\tLoss: 0.009928\nTrain epoch: 563 [2560/5139 (48%)]\tLoss: 0.001879\nTrain epoch: 563 [5120/5139 (95%)]\tLoss: 0.000104\nTrain epoch: 564 [0/5139 (0%)]\tLoss: 0.004260\nTrain epoch: 564 [2560/5139 (48%)]\tLoss: 0.001222\nTrain epoch: 564 [5120/5139 (95%)]\tLoss: 0.000431\nTrain epoch: 565 [0/5139 (0%)]\tLoss: 0.001945\nTrain epoch: 565 [2560/5139 (48%)]\tLoss: 0.003671\nTrain epoch: 565 [5120/5139 (95%)]\tLoss: 0.000655\nTrain epoch: 566 [0/5139 (0%)]\tLoss: 0.000503\nTrain epoch: 566 [2560/5139 (48%)]\tLoss: 0.003230\nTrain epoch: 566 [5120/5139 (95%)]\tLoss: 0.002566\nTrain epoch: 567 [0/5139 (0%)]\tLoss: 0.003378\nTrain epoch: 567 [2560/5139 (48%)]\tLoss: 0.013593\nTrain epoch: 567 [5120/5139 (95%)]\tLoss: 0.000018\nTrain epoch: 568 [0/5139 (0%)]\tLoss: 0.075923\nTrain epoch: 568 [2560/5139 (48%)]\tLoss: 0.007152\nTrain epoch: 568 [5120/5139 (95%)]\tLoss: 0.005107\nTrain epoch: 569 [0/5139 (0%)]\tLoss: 0.009292\nTrain epoch: 569 [2560/5139 (48%)]\tLoss: 0.017909\nTrain epoch: 569 [5120/5139 (95%)]\tLoss: 0.001010\nTrain epoch: 570 [0/5139 (0%)]\tLoss: 0.011530\nTrain epoch: 570 [2560/5139 (48%)]\tLoss: 0.010048\nTrain epoch: 570 [5120/5139 (95%)]\tLoss: 0.000094\nTrain epoch: 571 [0/5139 (0%)]\tLoss: 0.002730\nTrain epoch: 571 [2560/5139 (48%)]\tLoss: 0.002856\nTrain epoch: 571 [5120/5139 (95%)]\tLoss: 0.005270\nTrain epoch: 572 [0/5139 (0%)]\tLoss: 0.004362\nTrain epoch: 572 [2560/5139 (48%)]\tLoss: 0.002701\nTrain epoch: 572 [5120/5139 (95%)]\tLoss: 0.000354\nTrain epoch: 573 [0/5139 (0%)]\tLoss: 0.001459\nTrain epoch: 573 [2560/5139 (48%)]\tLoss: 0.002193\nTrain epoch: 573 [5120/5139 (95%)]\tLoss: 0.001557\nTrain epoch: 574 [0/5139 (0%)]\tLoss: 0.054432\nTrain epoch: 574 [2560/5139 (48%)]\tLoss: 0.025899\nTrain epoch: 574 [5120/5139 (95%)]\tLoss: 0.153272\nTrain epoch: 575 [0/5139 (0%)]\tLoss: 0.262811\nTrain epoch: 575 [2560/5139 (48%)]\tLoss: 0.026492\nTrain epoch: 575 [5120/5139 (95%)]\tLoss: 0.018067\nTrain epoch: 576 [0/5139 (0%)]\tLoss: 0.023309\nTrain epoch: 576 [2560/5139 (48%)]\tLoss: 0.012831\nTrain epoch: 576 [5120/5139 (95%)]\tLoss: 0.004547\nTrain epoch: 577 [0/5139 (0%)]\tLoss: 0.009885\nTrain epoch: 577 [2560/5139 (48%)]\tLoss: 0.007595\nTrain epoch: 577 [5120/5139 (95%)]\tLoss: 0.000059\nTrain epoch: 578 [0/5139 (0%)]\tLoss: 0.002236\nTrain epoch: 578 [2560/5139 (48%)]\tLoss: 0.018883\nTrain epoch: 578 [5120/5139 (95%)]\tLoss: 0.000033\nTrain epoch: 579 [0/5139 (0%)]\tLoss: 0.003168\nTrain epoch: 579 [2560/5139 (48%)]\tLoss: 0.006963\nTrain epoch: 579 [5120/5139 (95%)]\tLoss: 0.000807\nTrain epoch: 580 [0/5139 (0%)]\tLoss: 0.005410\nTrain epoch: 580 [2560/5139 (48%)]\tLoss: 0.002419\nTrain epoch: 580 [5120/5139 (95%)]\tLoss: 0.000628\nTrain epoch: 581 [0/5139 (0%)]\tLoss: 0.002867\nTrain epoch: 581 [2560/5139 (48%)]\tLoss: 0.004006\nTrain epoch: 581 [5120/5139 (95%)]\tLoss: 0.000436\nTrain epoch: 582 [0/5139 (0%)]\tLoss: 0.001873\nTrain epoch: 582 [2560/5139 (48%)]\tLoss: 0.000199\nTrain epoch: 582 [5120/5139 (95%)]\tLoss: 0.000162\nTrain epoch: 583 [0/5139 (0%)]\tLoss: 0.006862\nTrain epoch: 583 [2560/5139 (48%)]\tLoss: 0.000567\nTrain epoch: 583 [5120/5139 (95%)]\tLoss: 0.010180\nTrain epoch: 584 [0/5139 (0%)]\tLoss: 0.003402\nTrain epoch: 584 [2560/5139 (48%)]\tLoss: 0.005289\nTrain epoch: 584 [5120/5139 (95%)]\tLoss: 0.140708\nTrain epoch: 585 [0/5139 (0%)]\tLoss: 0.633756\nTrain epoch: 585 [2560/5139 (48%)]\tLoss: 0.143210\nTrain epoch: 585 [5120/5139 (95%)]\tLoss: 0.141359\nTrain epoch: 586 [0/5139 (0%)]\tLoss: 0.195857\nTrain epoch: 586 [2560/5139 (48%)]\tLoss: 0.099224\nTrain epoch: 586 [5120/5139 (95%)]\tLoss: 0.235974\nTrain epoch: 587 [0/5139 (0%)]\tLoss: 0.066323\nTrain epoch: 587 [2560/5139 (48%)]\tLoss: 0.079291\nTrain epoch: 587 [5120/5139 (95%)]\tLoss: 0.079925\nTrain epoch: 588 [0/5139 (0%)]\tLoss: 0.007483\nTrain epoch: 588 [2560/5139 (48%)]\tLoss: 0.045955\nTrain epoch: 588 [5120/5139 (95%)]\tLoss: 0.015600\nTrain epoch: 589 [0/5139 (0%)]\tLoss: 0.023901\nTrain epoch: 589 [2560/5139 (48%)]\tLoss: 0.024174\nTrain epoch: 589 [5120/5139 (95%)]\tLoss: 0.002289\nTrain epoch: 590 [0/5139 (0%)]\tLoss: 0.043263\nTrain epoch: 590 [2560/5139 (48%)]\tLoss: 0.010611\nTrain epoch: 590 [5120/5139 (95%)]\tLoss: 0.004384\nTrain epoch: 591 [0/5139 (0%)]\tLoss: 0.014864\nTrain epoch: 591 [2560/5139 (48%)]\tLoss: 0.009079\nTrain epoch: 591 [5120/5139 (95%)]\tLoss: 0.001308\nTrain epoch: 592 [0/5139 (0%)]\tLoss: 0.004378\nTrain epoch: 592 [2560/5139 (48%)]\tLoss: 0.005023\nTrain epoch: 592 [5120/5139 (95%)]\tLoss: 0.003447\nTrain epoch: 593 [0/5139 (0%)]\tLoss: 0.003817\nTrain epoch: 593 [2560/5139 (48%)]\tLoss: 0.037326\nTrain epoch: 593 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 594 [0/5139 (0%)]\tLoss: 0.004293\nTrain epoch: 594 [2560/5139 (48%)]\tLoss: 0.012483\nTrain epoch: 594 [5120/5139 (95%)]\tLoss: 0.000344\nTrain epoch: 595 [0/5139 (0%)]\tLoss: 0.022229\nTrain epoch: 595 [2560/5139 (48%)]\tLoss: 0.003557\nTrain epoch: 595 [5120/5139 (95%)]\tLoss: 0.003475\nTrain epoch: 596 [0/5139 (0%)]\tLoss: 0.005932\nTrain epoch: 596 [2560/5139 (48%)]\tLoss: 0.001271\nTrain epoch: 596 [5120/5139 (95%)]\tLoss: 0.001553\nTrain epoch: 597 [0/5139 (0%)]\tLoss: 0.007290\nTrain epoch: 597 [2560/5139 (48%)]\tLoss: 0.003399\nTrain epoch: 597 [5120/5139 (95%)]\tLoss: 0.004521\nTrain epoch: 598 [0/5139 (0%)]\tLoss: 0.004264\nTrain epoch: 598 [2560/5139 (48%)]\tLoss: 0.004404\nTrain epoch: 598 [5120/5139 (95%)]\tLoss: 0.000257\nTrain epoch: 599 [0/5139 (0%)]\tLoss: 0.127971\nTrain epoch: 599 [2560/5139 (48%)]\tLoss: 0.009360\nTrain epoch: 599 [5120/5139 (95%)]\tLoss: 0.017698\nTrain epoch: 600 [0/5139 (0%)]\tLoss: 0.009754\nTrain epoch: 600 [2560/5139 (48%)]\tLoss: 0.016144\nTrain epoch: 600 [5120/5139 (95%)]\tLoss: 0.006146\nTrain epoch: 601 [0/5139 (0%)]\tLoss: 0.013852\nTrain epoch: 601 [2560/5139 (48%)]\tLoss: 0.007009\nTrain epoch: 601 [5120/5139 (95%)]\tLoss: 0.000143\nTrain epoch: 602 [0/5139 (0%)]\tLoss: 0.009583\nTrain epoch: 602 [2560/5139 (48%)]\tLoss: 0.011052\nTrain epoch: 602 [5120/5139 (95%)]\tLoss: 0.022269\nTrain epoch: 603 [0/5139 (0%)]\tLoss: 0.003260\nTrain epoch: 603 [2560/5139 (48%)]\tLoss: 0.001242\nTrain epoch: 603 [5120/5139 (95%)]\tLoss: 0.001999\nTrain epoch: 604 [0/5139 (0%)]\tLoss: 0.000964\nTrain epoch: 604 [2560/5139 (48%)]\tLoss: 0.006401\nTrain epoch: 604 [5120/5139 (95%)]\tLoss: 0.004694\nTrain epoch: 605 [0/5139 (0%)]\tLoss: 0.002642\nTrain epoch: 605 [2560/5139 (48%)]\tLoss: 0.051617\nTrain epoch: 605 [5120/5139 (95%)]\tLoss: 0.004409\nTrain epoch: 606 [0/5139 (0%)]\tLoss: 0.004036\nTrain epoch: 606 [2560/5139 (48%)]\tLoss: 0.009029\nTrain epoch: 606 [5120/5139 (95%)]\tLoss: 0.016131\nTrain epoch: 607 [0/5139 (0%)]\tLoss: 0.012948\nTrain epoch: 607 [2560/5139 (48%)]\tLoss: 0.011104\nTrain epoch: 607 [5120/5139 (95%)]\tLoss: 0.000074\nTrain epoch: 608 [0/5139 (0%)]\tLoss: 0.012561\nTrain epoch: 608 [2560/5139 (48%)]\tLoss: 0.053041\nTrain epoch: 608 [5120/5139 (95%)]\tLoss: 0.187454\nTrain epoch: 609 [0/5139 (0%)]\tLoss: 0.014174\nTrain epoch: 609 [2560/5139 (48%)]\tLoss: 0.007472\nTrain epoch: 609 [5120/5139 (95%)]\tLoss: 0.108417\nTrain epoch: 610 [0/5139 (0%)]\tLoss: 0.029912\nTrain epoch: 610 [2560/5139 (48%)]\tLoss: 0.033218\nTrain epoch: 610 [5120/5139 (95%)]\tLoss: 0.002881\nTrain epoch: 611 [0/5139 (0%)]\tLoss: 0.018229\nTrain epoch: 611 [2560/5139 (48%)]\tLoss: 0.011021\nTrain epoch: 611 [5120/5139 (95%)]\tLoss: 0.052842\nTrain epoch: 612 [0/5139 (0%)]\tLoss: 0.013355\nTrain epoch: 612 [2560/5139 (48%)]\tLoss: 0.008098\nTrain epoch: 612 [5120/5139 (95%)]\tLoss: 0.000267\nTrain epoch: 613 [0/5139 (0%)]\tLoss: 0.005082\nTrain epoch: 613 [2560/5139 (48%)]\tLoss: 0.005767\nTrain epoch: 613 [5120/5139 (95%)]\tLoss: 0.153294\nTrain epoch: 614 [0/5139 (0%)]\tLoss: 0.003492\nTrain epoch: 614 [2560/5139 (48%)]\tLoss: 0.014797\nTrain epoch: 614 [5120/5139 (95%)]\tLoss: 0.001150\nTrain epoch: 615 [0/5139 (0%)]\tLoss: 0.022432\nTrain epoch: 615 [2560/5139 (48%)]\tLoss: 0.014314\nTrain epoch: 615 [5120/5139 (95%)]\tLoss: 0.003425\nTrain epoch: 616 [0/5139 (0%)]\tLoss: 0.011557\nTrain epoch: 616 [2560/5139 (48%)]\tLoss: 0.009008\nTrain epoch: 616 [5120/5139 (95%)]\tLoss: 0.007129\nTrain epoch: 617 [0/5139 (0%)]\tLoss: 0.002792\nTrain epoch: 617 [2560/5139 (48%)]\tLoss: 0.005068\nTrain epoch: 617 [5120/5139 (95%)]\tLoss: 0.006357\nTrain epoch: 618 [0/5139 (0%)]\tLoss: 0.003538\nTrain epoch: 618 [2560/5139 (48%)]\tLoss: 0.001768\nTrain epoch: 618 [5120/5139 (95%)]\tLoss: 0.102829\nTrain epoch: 619 [0/5139 (0%)]\tLoss: 0.016824\nTrain epoch: 619 [2560/5139 (48%)]\tLoss: 0.183348\nTrain epoch: 619 [5120/5139 (95%)]\tLoss: 0.075407\nTrain epoch: 620 [0/5139 (0%)]\tLoss: 0.161690\nTrain epoch: 620 [2560/5139 (48%)]\tLoss: 0.056392\nTrain epoch: 620 [5120/5139 (95%)]\tLoss: 0.007100\nTrain epoch: 621 [0/5139 (0%)]\tLoss: 0.061464\nTrain epoch: 621 [2560/5139 (48%)]\tLoss: 0.015413\nTrain epoch: 621 [5120/5139 (95%)]\tLoss: 0.002993\nTrain epoch: 622 [0/5139 (0%)]\tLoss: 0.023682\nTrain epoch: 622 [2560/5139 (48%)]\tLoss: 0.024717\nTrain epoch: 622 [5120/5139 (95%)]\tLoss: 0.001473\nTrain epoch: 623 [0/5139 (0%)]\tLoss: 0.007627\nTrain epoch: 623 [2560/5139 (48%)]\tLoss: 0.007904\nTrain epoch: 623 [5120/5139 (95%)]\tLoss: 0.000091\nTrain epoch: 624 [0/5139 (0%)]\tLoss: 0.006784\nTrain epoch: 624 [2560/5139 (48%)]\tLoss: 0.006729\nTrain epoch: 624 [5120/5139 (95%)]\tLoss: 0.000046\nTrain epoch: 625 [0/5139 (0%)]\tLoss: 0.001652\nTrain epoch: 625 [2560/5139 (48%)]\tLoss: 0.005258\nTrain epoch: 625 [5120/5139 (95%)]\tLoss: 0.000552\nTrain epoch: 626 [0/5139 (0%)]\tLoss: 0.005400\nTrain epoch: 626 [2560/5139 (48%)]\tLoss: 0.007148\nTrain epoch: 626 [5120/5139 (95%)]\tLoss: 0.001028\nTrain epoch: 627 [0/5139 (0%)]\tLoss: 0.004523\nTrain epoch: 627 [2560/5139 (48%)]\tLoss: 0.003018\nTrain epoch: 627 [5120/5139 (95%)]\tLoss: 0.000142\nTrain epoch: 628 [0/5139 (0%)]\tLoss: 0.008413\nTrain epoch: 628 [2560/5139 (48%)]\tLoss: 0.015407\nTrain epoch: 628 [5120/5139 (95%)]\tLoss: 0.040527\nTrain epoch: 629 [0/5139 (0%)]\tLoss: 0.026140\nTrain epoch: 629 [2560/5139 (48%)]\tLoss: 0.164920\nTrain epoch: 629 [5120/5139 (95%)]\tLoss: 0.003629\nTrain epoch: 630 [0/5139 (0%)]\tLoss: 0.056782\nTrain epoch: 630 [2560/5139 (48%)]\tLoss: 0.023883\nTrain epoch: 630 [5120/5139 (95%)]\tLoss: 0.001846\nTrain epoch: 631 [0/5139 (0%)]\tLoss: 0.016898\nTrain epoch: 631 [2560/5139 (48%)]\tLoss: 0.017430\nTrain epoch: 631 [5120/5139 (95%)]\tLoss: 0.002402\nTrain epoch: 632 [0/5139 (0%)]\tLoss: 0.009435\nTrain epoch: 632 [2560/5139 (48%)]\tLoss: 0.001660\nTrain epoch: 632 [5120/5139 (95%)]\tLoss: 0.001742\nTrain epoch: 633 [0/5139 (0%)]\tLoss: 0.005392\nTrain epoch: 633 [2560/5139 (48%)]\tLoss: 0.001362\nTrain epoch: 633 [5120/5139 (95%)]\tLoss: 0.007097\nTrain epoch: 634 [0/5139 (0%)]\tLoss: 0.000889\nTrain epoch: 634 [2560/5139 (48%)]\tLoss: 0.002290\nTrain epoch: 634 [5120/5139 (95%)]\tLoss: 0.000091\nTrain epoch: 635 [0/5139 (0%)]\tLoss: 0.008467\nTrain epoch: 635 [2560/5139 (48%)]\tLoss: 0.002258\nTrain epoch: 635 [5120/5139 (95%)]\tLoss: 0.000035\nTrain epoch: 636 [0/5139 (0%)]\tLoss: 0.001140\nTrain epoch: 636 [2560/5139 (48%)]\tLoss: 0.000957\nTrain epoch: 636 [5120/5139 (95%)]\tLoss: 0.001337\nTrain epoch: 637 [0/5139 (0%)]\tLoss: 0.049013\nTrain epoch: 637 [2560/5139 (48%)]\tLoss: 0.006796\nTrain epoch: 637 [5120/5139 (95%)]\tLoss: 0.001983\nTrain epoch: 638 [0/5139 (0%)]\tLoss: 0.008532\nTrain epoch: 638 [2560/5139 (48%)]\tLoss: 0.060382\nTrain epoch: 638 [5120/5139 (95%)]\tLoss: 0.020107\nTrain epoch: 639 [0/5139 (0%)]\tLoss: 0.002534\nTrain epoch: 639 [2560/5139 (48%)]\tLoss: 0.006518\nTrain epoch: 639 [5120/5139 (95%)]\tLoss: 0.004400\nTrain epoch: 640 [0/5139 (0%)]\tLoss: 0.005851\nTrain epoch: 640 [2560/5139 (48%)]\tLoss: 0.008351\nTrain epoch: 640 [5120/5139 (95%)]\tLoss: 0.000012\nTrain epoch: 641 [0/5139 (0%)]\tLoss: 0.001020\nTrain epoch: 641 [2560/5139 (48%)]\tLoss: 0.008003\nTrain epoch: 641 [5120/5139 (95%)]\tLoss: 0.000910\nTrain epoch: 642 [0/5139 (0%)]\tLoss: 0.001654\nTrain epoch: 642 [2560/5139 (48%)]\tLoss: 0.001253\nTrain epoch: 642 [5120/5139 (95%)]\tLoss: 0.003820\nTrain epoch: 643 [0/5139 (0%)]\tLoss: 0.002478\nTrain epoch: 643 [2560/5139 (48%)]\tLoss: 0.004930\nTrain epoch: 643 [5120/5139 (95%)]\tLoss: 0.000031\nTrain epoch: 644 [0/5139 (0%)]\tLoss: 0.002825\nTrain epoch: 644 [2560/5139 (48%)]\tLoss: 0.001420\nTrain epoch: 644 [5120/5139 (95%)]\tLoss: 0.000052\nTrain epoch: 645 [0/5139 (0%)]\tLoss: 0.000590\nTrain epoch: 645 [2560/5139 (48%)]\tLoss: 0.000416\nTrain epoch: 645 [5120/5139 (95%)]\tLoss: 0.003469\nTrain epoch: 646 [0/5139 (0%)]\tLoss: 0.002192\nTrain epoch: 646 [2560/5139 (48%)]\tLoss: 0.000755\nTrain epoch: 646 [5120/5139 (95%)]\tLoss: 0.001896\nTrain epoch: 647 [0/5139 (0%)]\tLoss: 0.001384\nTrain epoch: 647 [2560/5139 (48%)]\tLoss: 0.000589\nTrain epoch: 647 [5120/5139 (95%)]\tLoss: 0.000325\nTrain epoch: 648 [0/5139 (0%)]\tLoss: 0.006246\nTrain epoch: 648 [2560/5139 (48%)]\tLoss: 0.010626\nTrain epoch: 648 [5120/5139 (95%)]\tLoss: 0.000052\nTrain epoch: 649 [0/5139 (0%)]\tLoss: 0.038270\nTrain epoch: 649 [2560/5139 (48%)]\tLoss: 0.000804\nTrain epoch: 649 [5120/5139 (95%)]\tLoss: 0.000024\nTrain epoch: 650 [0/5139 (0%)]\tLoss: 0.016241\nTrain epoch: 650 [2560/5139 (48%)]\tLoss: 0.003944\nTrain epoch: 650 [5120/5139 (95%)]\tLoss: 0.000166\nTrain epoch: 651 [0/5139 (0%)]\tLoss: 0.003269\nTrain epoch: 651 [2560/5139 (48%)]\tLoss: 0.014810\nTrain epoch: 651 [5120/5139 (95%)]\tLoss: 0.013722\nTrain epoch: 652 [0/5139 (0%)]\tLoss: 0.000251\nTrain epoch: 652 [2560/5139 (48%)]\tLoss: 0.000372\nTrain epoch: 652 [5120/5139 (95%)]\tLoss: 0.000218\nTrain epoch: 653 [0/5139 (0%)]\tLoss: 0.005912\nTrain epoch: 653 [2560/5139 (48%)]\tLoss: 0.000225\nTrain epoch: 653 [5120/5139 (95%)]\tLoss: 0.000175\nTrain epoch: 654 [0/5139 (0%)]\tLoss: 0.000650\nTrain epoch: 654 [2560/5139 (48%)]\tLoss: 0.010810\nTrain epoch: 654 [5120/5139 (95%)]\tLoss: 0.000040\nTrain epoch: 655 [0/5139 (0%)]\tLoss: 0.000651\nTrain epoch: 655 [2560/5139 (48%)]\tLoss: 0.002377\nTrain epoch: 655 [5120/5139 (95%)]\tLoss: 0.000006\nTrain epoch: 656 [0/5139 (0%)]\tLoss: 0.003924\nTrain epoch: 656 [2560/5139 (48%)]\tLoss: 0.001611\nTrain epoch: 656 [5120/5139 (95%)]\tLoss: 0.000008\nTrain epoch: 657 [0/5139 (0%)]\tLoss: 0.001417\nTrain epoch: 657 [2560/5139 (48%)]\tLoss: 0.005467\nTrain epoch: 657 [5120/5139 (95%)]\tLoss: 0.257231\nTrain epoch: 658 [0/5139 (0%)]\tLoss: 0.017298\nTrain epoch: 658 [2560/5139 (48%)]\tLoss: 0.204498\nTrain epoch: 658 [5120/5139 (95%)]\tLoss: 0.027785\nTrain epoch: 659 [0/5139 (0%)]\tLoss: 0.042012\nTrain epoch: 659 [2560/5139 (48%)]\tLoss: 0.014424\nTrain epoch: 659 [5120/5139 (95%)]\tLoss: 0.007881\nTrain epoch: 660 [0/5139 (0%)]\tLoss: 0.040396\nTrain epoch: 660 [2560/5139 (48%)]\tLoss: 0.007206\nTrain epoch: 660 [5120/5139 (95%)]\tLoss: 0.009130\nTrain epoch: 661 [0/5139 (0%)]\tLoss: 0.003788\nTrain epoch: 661 [2560/5139 (48%)]\tLoss: 0.007836\nTrain epoch: 661 [5120/5139 (95%)]\tLoss: 0.003667\nTrain epoch: 662 [0/5139 (0%)]\tLoss: 0.005575\nTrain epoch: 662 [2560/5139 (48%)]\tLoss: 0.003172\nTrain epoch: 662 [5120/5139 (95%)]\tLoss: 0.000261\nTrain epoch: 663 [0/5139 (0%)]\tLoss: 0.006835\nTrain epoch: 663 [2560/5139 (48%)]\tLoss: 0.004887\nTrain epoch: 663 [5120/5139 (95%)]\tLoss: 0.000408\nTrain epoch: 664 [0/5139 (0%)]\tLoss: 0.001219\nTrain epoch: 664 [2560/5139 (48%)]\tLoss: 0.005738\nTrain epoch: 664 [5120/5139 (95%)]\tLoss: 0.000999\nTrain epoch: 665 [0/5139 (0%)]\tLoss: 0.003068\nTrain epoch: 665 [2560/5139 (48%)]\tLoss: 0.003905\nTrain epoch: 665 [5120/5139 (95%)]\tLoss: 0.000006\nTrain epoch: 666 [0/5139 (0%)]\tLoss: 0.003884\nTrain epoch: 666 [2560/5139 (48%)]\tLoss: 0.004696\nTrain epoch: 666 [5120/5139 (95%)]\tLoss: 0.078347\nTrain epoch: 667 [0/5139 (0%)]\tLoss: 0.005583\nTrain epoch: 667 [2560/5139 (48%)]\tLoss: 0.003955\nTrain epoch: 667 [5120/5139 (95%)]\tLoss: 0.000137\nTrain epoch: 668 [0/5139 (0%)]\tLoss: 0.039717\nTrain epoch: 668 [2560/5139 (48%)]\tLoss: 0.004770\nTrain epoch: 668 [5120/5139 (95%)]\tLoss: 0.001877\nTrain epoch: 669 [0/5139 (0%)]\tLoss: 0.003404\nTrain epoch: 669 [2560/5139 (48%)]\tLoss: 0.002483\nTrain epoch: 669 [5120/5139 (95%)]\tLoss: 0.004655\nTrain epoch: 670 [0/5139 (0%)]\tLoss: 0.001084\nTrain epoch: 670 [2560/5139 (48%)]\tLoss: 0.003819\nTrain epoch: 670 [5120/5139 (95%)]\tLoss: 0.000421\nTrain epoch: 671 [0/5139 (0%)]\tLoss: 0.000938\nTrain epoch: 671 [2560/5139 (48%)]\tLoss: 0.000358\nTrain epoch: 671 [5120/5139 (95%)]\tLoss: 0.000344\nTrain epoch: 672 [0/5139 (0%)]\tLoss: 0.007925\nTrain epoch: 672 [2560/5139 (48%)]\tLoss: 0.004005\nTrain epoch: 672 [5120/5139 (95%)]\tLoss: 0.001085\nTrain epoch: 673 [0/5139 (0%)]\tLoss: 0.004105\nTrain epoch: 673 [2560/5139 (48%)]\tLoss: 0.003324\nTrain epoch: 673 [5120/5139 (95%)]\tLoss: 0.001082\nTrain epoch: 674 [0/5139 (0%)]\tLoss: 0.005015\nTrain epoch: 674 [2560/5139 (48%)]\tLoss: 0.003612\nTrain epoch: 674 [5120/5139 (95%)]\tLoss: 0.000769\nTrain epoch: 675 [0/5139 (0%)]\tLoss: 0.000579\nTrain epoch: 675 [2560/5139 (48%)]\tLoss: 0.000426\nTrain epoch: 675 [5120/5139 (95%)]\tLoss: 0.000035\nTrain epoch: 676 [0/5139 (0%)]\tLoss: 0.001143\nTrain epoch: 676 [2560/5139 (48%)]\tLoss: 0.001854\nTrain epoch: 676 [5120/5139 (95%)]\tLoss: 0.000149\nTrain epoch: 677 [0/5139 (0%)]\tLoss: 0.005865\nTrain epoch: 677 [2560/5139 (48%)]\tLoss: 0.004937\nTrain epoch: 677 [5120/5139 (95%)]\tLoss: 0.000046\nTrain epoch: 678 [0/5139 (0%)]\tLoss: 0.026083\nTrain epoch: 678 [2560/5139 (48%)]\tLoss: 0.029577\nTrain epoch: 678 [5120/5139 (95%)]\tLoss: 0.042669\nTrain epoch: 679 [0/5139 (0%)]\tLoss: 0.024851\nTrain epoch: 679 [2560/5139 (48%)]\tLoss: 0.000722\nTrain epoch: 679 [5120/5139 (95%)]\tLoss: 0.003734\nTrain epoch: 680 [0/5139 (0%)]\tLoss: 0.047565\nTrain epoch: 680 [2560/5139 (48%)]\tLoss: 0.024639\nTrain epoch: 680 [5120/5139 (95%)]\tLoss: 0.029588\nTrain epoch: 681 [0/5139 (0%)]\tLoss: 0.002380\nTrain epoch: 681 [2560/5139 (48%)]\tLoss: 0.009312\nTrain epoch: 681 [5120/5139 (95%)]\tLoss: 0.000619\nTrain epoch: 682 [0/5139 (0%)]\tLoss: 0.003077\nTrain epoch: 682 [2560/5139 (48%)]\tLoss: 0.000644\nTrain epoch: 682 [5120/5139 (95%)]\tLoss: 0.001247\nTrain epoch: 683 [0/5139 (0%)]\tLoss: 0.023363\nTrain epoch: 683 [2560/5139 (48%)]\tLoss: 0.006093\nTrain epoch: 683 [5120/5139 (95%)]\tLoss: 0.023576\nTrain epoch: 684 [0/5139 (0%)]\tLoss: 0.000247\nTrain epoch: 684 [2560/5139 (48%)]\tLoss: 0.003264\nTrain epoch: 684 [5120/5139 (95%)]\tLoss: 0.001958\nTrain epoch: 685 [0/5139 (0%)]\tLoss: 0.019505\nTrain epoch: 685 [2560/5139 (48%)]\tLoss: 0.000695\nTrain epoch: 685 [5120/5139 (95%)]\tLoss: 0.000059\nTrain epoch: 686 [0/5139 (0%)]\tLoss: 0.052218\nTrain epoch: 686 [2560/5139 (48%)]\tLoss: 0.010962\nTrain epoch: 686 [5120/5139 (95%)]\tLoss: 0.005230\nTrain epoch: 687 [0/5139 (0%)]\tLoss: 0.005032\nTrain epoch: 687 [2560/5139 (48%)]\tLoss: 0.008336\nTrain epoch: 687 [5120/5139 (95%)]\tLoss: 0.039130\nTrain epoch: 688 [0/5139 (0%)]\tLoss: 0.016206\nTrain epoch: 688 [2560/5139 (48%)]\tLoss: 0.056216\nTrain epoch: 688 [5120/5139 (95%)]\tLoss: 0.000785\nTrain epoch: 689 [0/5139 (0%)]\tLoss: 0.029034\nTrain epoch: 689 [2560/5139 (48%)]\tLoss: 0.003859\nTrain epoch: 689 [5120/5139 (95%)]\tLoss: 0.002075\nTrain epoch: 690 [0/5139 (0%)]\tLoss: 0.007067\nTrain epoch: 690 [2560/5139 (48%)]\tLoss: 0.005296\nTrain epoch: 690 [5120/5139 (95%)]\tLoss: 0.001508\nTrain epoch: 691 [0/5139 (0%)]\tLoss: 0.040639\nTrain epoch: 691 [2560/5139 (48%)]\tLoss: 0.001129\nTrain epoch: 691 [5120/5139 (95%)]\tLoss: 0.002190\nTrain epoch: 692 [0/5139 (0%)]\tLoss: 0.001722\nTrain epoch: 692 [2560/5139 (48%)]\tLoss: 0.009898\nTrain epoch: 692 [5120/5139 (95%)]\tLoss: 0.001285\nTrain epoch: 693 [0/5139 (0%)]\tLoss: 0.000950\nTrain epoch: 693 [2560/5139 (48%)]\tLoss: 0.000784\nTrain epoch: 693 [5120/5139 (95%)]\tLoss: 0.000056\nTrain epoch: 694 [0/5139 (0%)]\tLoss: 0.021293\nTrain epoch: 694 [2560/5139 (48%)]\tLoss: 0.001566\nTrain epoch: 694 [5120/5139 (95%)]\tLoss: 0.000080\nTrain epoch: 695 [0/5139 (0%)]\tLoss: 0.001038\nTrain epoch: 695 [2560/5139 (48%)]\tLoss: 0.001618\nTrain epoch: 695 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 696 [0/5139 (0%)]\tLoss: 0.000592\nTrain epoch: 696 [2560/5139 (48%)]\tLoss: 0.002235\nTrain epoch: 696 [5120/5139 (95%)]\tLoss: 0.000227\nTrain epoch: 697 [0/5139 (0%)]\tLoss: 0.000195\nTrain epoch: 697 [2560/5139 (48%)]\tLoss: 0.007383\nTrain epoch: 697 [5120/5139 (95%)]\tLoss: 0.000076\nTrain epoch: 698 [0/5139 (0%)]\tLoss: 0.008402\nTrain epoch: 698 [2560/5139 (48%)]\tLoss: 0.005825\nTrain epoch: 698 [5120/5139 (95%)]\tLoss: 0.109616\nTrain epoch: 699 [0/5139 (0%)]\tLoss: 0.044717\nTrain epoch: 699 [2560/5139 (48%)]\tLoss: 0.106799\nTrain epoch: 699 [5120/5139 (95%)]\tLoss: 0.062344\nTrain epoch: 700 [0/5139 (0%)]\tLoss: 0.030166\nTrain epoch: 700 [2560/5139 (48%)]\tLoss: 0.012634\nTrain epoch: 700 [5120/5139 (95%)]\tLoss: 0.001658\nTrain epoch: 701 [0/5139 (0%)]\tLoss: 0.032997\nTrain epoch: 701 [2560/5139 (48%)]\tLoss: 0.020722\nTrain epoch: 701 [5120/5139 (95%)]\tLoss: 0.002129\nTrain epoch: 702 [0/5139 (0%)]\tLoss: 0.013565\nTrain epoch: 702 [2560/5139 (48%)]\tLoss: 0.002802\nTrain epoch: 702 [5120/5139 (95%)]\tLoss: 0.011743\nTrain epoch: 703 [0/5139 (0%)]\tLoss: 0.005188\nTrain epoch: 703 [2560/5139 (48%)]\tLoss: 0.008506\nTrain epoch: 703 [5120/5139 (95%)]\tLoss: 0.004376\nTrain epoch: 704 [0/5139 (0%)]\tLoss: 0.026431\nTrain epoch: 704 [2560/5139 (48%)]\tLoss: 0.003762\nTrain epoch: 704 [5120/5139 (95%)]\tLoss: 0.001624\nTrain epoch: 705 [0/5139 (0%)]\tLoss: 0.008006\nTrain epoch: 705 [2560/5139 (48%)]\tLoss: 0.009280\nTrain epoch: 705 [5120/5139 (95%)]\tLoss: 0.001092\nTrain epoch: 706 [0/5139 (0%)]\tLoss: 0.003453\nTrain epoch: 706 [2560/5139 (48%)]\tLoss: 0.000984\nTrain epoch: 706 [5120/5139 (95%)]\tLoss: 0.000045\nTrain epoch: 707 [0/5139 (0%)]\tLoss: 0.001286\nTrain epoch: 707 [2560/5139 (48%)]\tLoss: 0.001066\nTrain epoch: 707 [5120/5139 (95%)]\tLoss: 0.000033\nTrain epoch: 708 [0/5139 (0%)]\tLoss: 0.001361\nTrain epoch: 708 [2560/5139 (48%)]\tLoss: 0.031979\nTrain epoch: 708 [5120/5139 (95%)]\tLoss: 0.001028\nTrain epoch: 709 [0/5139 (0%)]\tLoss: 0.004722\nTrain epoch: 709 [2560/5139 (48%)]\tLoss: 0.019034\nTrain epoch: 709 [5120/5139 (95%)]\tLoss: 0.000019\nTrain epoch: 710 [0/5139 (0%)]\tLoss: 0.006611\nTrain epoch: 710 [2560/5139 (48%)]\tLoss: 0.007221\nTrain epoch: 710 [5120/5139 (95%)]\tLoss: 0.003373\nTrain epoch: 711 [0/5139 (0%)]\tLoss: 0.010919\nTrain epoch: 711 [2560/5139 (48%)]\tLoss: 0.004707\nTrain epoch: 711 [5120/5139 (95%)]\tLoss: 0.003508\nTrain epoch: 712 [0/5139 (0%)]\tLoss: 0.023304\nTrain epoch: 712 [2560/5139 (48%)]\tLoss: 0.040002\nTrain epoch: 712 [5120/5139 (95%)]\tLoss: 0.000738\nTrain epoch: 713 [0/5139 (0%)]\tLoss: 0.062388\nTrain epoch: 713 [2560/5139 (48%)]\tLoss: 0.006123\nTrain epoch: 713 [5120/5139 (95%)]\tLoss: 0.000257\nTrain epoch: 714 [0/5139 (0%)]\tLoss: 0.014115\nTrain epoch: 714 [2560/5139 (48%)]\tLoss: 0.004527\nTrain epoch: 714 [5120/5139 (95%)]\tLoss: 0.000003\nTrain epoch: 715 [0/5139 (0%)]\tLoss: 0.007057\nTrain epoch: 715 [2560/5139 (48%)]\tLoss: 0.006747\nTrain epoch: 715 [5120/5139 (95%)]\tLoss: 0.012843\nTrain epoch: 716 [0/5139 (0%)]\tLoss: 0.001570\nTrain epoch: 716 [2560/5139 (48%)]\tLoss: 0.005166\nTrain epoch: 716 [5120/5139 (95%)]\tLoss: 0.000834\nTrain epoch: 717 [0/5139 (0%)]\tLoss: 0.001297\nTrain epoch: 717 [2560/5139 (48%)]\tLoss: 0.008603\nTrain epoch: 717 [5120/5139 (95%)]\tLoss: 0.000627\nTrain epoch: 718 [0/5139 (0%)]\tLoss: 0.122390\nTrain epoch: 718 [2560/5139 (48%)]\tLoss: 0.020931\nTrain epoch: 718 [5120/5139 (95%)]\tLoss: 0.050738\nTrain epoch: 719 [0/5139 (0%)]\tLoss: 0.046384\nTrain epoch: 719 [2560/5139 (48%)]\tLoss: 0.012842\nTrain epoch: 719 [5120/5139 (95%)]\tLoss: 0.003913\nTrain epoch: 720 [0/5139 (0%)]\tLoss: 0.002288\nTrain epoch: 720 [2560/5139 (48%)]\tLoss: 0.001313\nTrain epoch: 720 [5120/5139 (95%)]\tLoss: 0.009105\nTrain epoch: 721 [0/5139 (0%)]\tLoss: 0.006949\nTrain epoch: 721 [2560/5139 (48%)]\tLoss: 0.013070\nTrain epoch: 721 [5120/5139 (95%)]\tLoss: 0.000284\nTrain epoch: 722 [0/5139 (0%)]\tLoss: 0.000894\nTrain epoch: 722 [2560/5139 (48%)]\tLoss: 0.000781\nTrain epoch: 722 [5120/5139 (95%)]\tLoss: 0.001701\nTrain epoch: 723 [0/5139 (0%)]\tLoss: 0.000738\nTrain epoch: 723 [2560/5139 (48%)]\tLoss: 0.000572\nTrain epoch: 723 [5120/5139 (95%)]\tLoss: 0.000597\nTrain epoch: 724 [0/5139 (0%)]\tLoss: 0.001430\nTrain epoch: 724 [2560/5139 (48%)]\tLoss: 0.000703\nTrain epoch: 724 [5120/5139 (95%)]\tLoss: 0.000138\nTrain epoch: 725 [0/5139 (0%)]\tLoss: 0.007604\nTrain epoch: 725 [2560/5139 (48%)]\tLoss: 0.010571\nTrain epoch: 725 [5120/5139 (95%)]\tLoss: 0.038459\nTrain epoch: 726 [0/5139 (0%)]\tLoss: 0.038894\nTrain epoch: 726 [2560/5139 (48%)]\tLoss: 0.003886\nTrain epoch: 726 [5120/5139 (95%)]\tLoss: 0.002839\nTrain epoch: 727 [0/5139 (0%)]\tLoss: 0.002394\nTrain epoch: 727 [2560/5139 (48%)]\tLoss: 0.003649\nTrain epoch: 727 [5120/5139 (95%)]\tLoss: 0.001386\nTrain epoch: 728 [0/5139 (0%)]\tLoss: 0.001104\nTrain epoch: 728 [2560/5139 (48%)]\tLoss: 0.001270\nTrain epoch: 728 [5120/5139 (95%)]\tLoss: 0.000711\nTrain epoch: 729 [0/5139 (0%)]\tLoss: 0.002339\nTrain epoch: 729 [2560/5139 (48%)]\tLoss: 0.001215\nTrain epoch: 729 [5120/5139 (95%)]\tLoss: 0.001076\nTrain epoch: 730 [0/5139 (0%)]\tLoss: 0.000451\nTrain epoch: 730 [2560/5139 (48%)]\tLoss: 0.000209\nTrain epoch: 730 [5120/5139 (95%)]\tLoss: 0.000384\nTrain epoch: 731 [0/5139 (0%)]\tLoss: 0.000183\nTrain epoch: 731 [2560/5139 (48%)]\tLoss: 0.000602\nTrain epoch: 731 [5120/5139 (95%)]\tLoss: 0.005496\nTrain epoch: 732 [0/5139 (0%)]\tLoss: 0.000069\nTrain epoch: 732 [2560/5139 (48%)]\tLoss: 0.000534\nTrain epoch: 732 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 733 [0/5139 (0%)]\tLoss: 0.042380\nTrain epoch: 733 [2560/5139 (48%)]\tLoss: 0.004752\nTrain epoch: 733 [5120/5139 (95%)]\tLoss: 0.001806\nTrain epoch: 734 [0/5139 (0%)]\tLoss: 0.000343\nTrain epoch: 734 [2560/5139 (48%)]\tLoss: 0.002819\nTrain epoch: 734 [5120/5139 (95%)]\tLoss: 0.001307\nTrain epoch: 735 [0/5139 (0%)]\tLoss: 0.004069\nTrain epoch: 735 [2560/5139 (48%)]\tLoss: 0.021771\nTrain epoch: 735 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 736 [0/5139 (0%)]\tLoss: 0.003299\nTrain epoch: 736 [2560/5139 (48%)]\tLoss: 0.014367\nTrain epoch: 736 [5120/5139 (95%)]\tLoss: 0.007193\nTrain epoch: 737 [0/5139 (0%)]\tLoss: 0.022433\nTrain epoch: 737 [2560/5139 (48%)]\tLoss: 0.001483\nTrain epoch: 737 [5120/5139 (95%)]\tLoss: 0.172769\nTrain epoch: 738 [0/5139 (0%)]\tLoss: 0.018367\nTrain epoch: 738 [2560/5139 (48%)]\tLoss: 0.082539\nTrain epoch: 738 [5120/5139 (95%)]\tLoss: 0.027677\nTrain epoch: 739 [0/5139 (0%)]\tLoss: 0.024708\nTrain epoch: 739 [2560/5139 (48%)]\tLoss: 0.033799\nTrain epoch: 739 [5120/5139 (95%)]\tLoss: 0.025812\nTrain epoch: 740 [0/5139 (0%)]\tLoss: 0.014091\nTrain epoch: 740 [2560/5139 (48%)]\tLoss: 0.013752\nTrain epoch: 740 [5120/5139 (95%)]\tLoss: 0.000312\nTrain epoch: 741 [0/5139 (0%)]\tLoss: 0.029710\nTrain epoch: 741 [2560/5139 (48%)]\tLoss: 0.013349\nTrain epoch: 741 [5120/5139 (95%)]\tLoss: 0.198299\nTrain epoch: 742 [0/5139 (0%)]\tLoss: 0.229554\nTrain epoch: 742 [2560/5139 (48%)]\tLoss: 0.130083\nTrain epoch: 742 [5120/5139 (95%)]\tLoss: 0.205086\nTrain epoch: 743 [0/5139 (0%)]\tLoss: 0.168922\nTrain epoch: 743 [2560/5139 (48%)]\tLoss: 0.097179\nTrain epoch: 743 [5120/5139 (95%)]\tLoss: 0.018588\nTrain epoch: 744 [0/5139 (0%)]\tLoss: 0.099804\nTrain epoch: 744 [2560/5139 (48%)]\tLoss: 0.063934\nTrain epoch: 744 [5120/5139 (95%)]\tLoss: 0.014927\nTrain epoch: 745 [0/5139 (0%)]\tLoss: 0.045974\nTrain epoch: 745 [2560/5139 (48%)]\tLoss: 0.023120\nTrain epoch: 745 [5120/5139 (95%)]\tLoss: 0.000626\nTrain epoch: 746 [0/5139 (0%)]\tLoss: 0.020122\nTrain epoch: 746 [2560/5139 (48%)]\tLoss: 0.035943\nTrain epoch: 746 [5120/5139 (95%)]\tLoss: 0.012453\nTrain epoch: 747 [0/5139 (0%)]\tLoss: 0.004690\nTrain epoch: 747 [2560/5139 (48%)]\tLoss: 0.020240\nTrain epoch: 747 [5120/5139 (95%)]\tLoss: 0.020936\nTrain epoch: 748 [0/5139 (0%)]\tLoss: 0.021686\nTrain epoch: 748 [2560/5139 (48%)]\tLoss: 0.018536\nTrain epoch: 748 [5120/5139 (95%)]\tLoss: 0.000535\nTrain epoch: 749 [0/5139 (0%)]\tLoss: 0.072177\nTrain epoch: 749 [2560/5139 (48%)]\tLoss: 0.007690\nTrain epoch: 749 [5120/5139 (95%)]\tLoss: 0.000688\nTrain epoch: 750 [0/5139 (0%)]\tLoss: 0.010957\nTrain epoch: 750 [2560/5139 (48%)]\tLoss: 0.021320\nTrain epoch: 750 [5120/5139 (95%)]\tLoss: 0.003888\nTrain epoch: 751 [0/5139 (0%)]\tLoss: 0.007189\nTrain epoch: 751 [2560/5139 (48%)]\tLoss: 0.004428\nTrain epoch: 751 [5120/5139 (95%)]\tLoss: 0.001516\nTrain epoch: 752 [0/5139 (0%)]\tLoss: 0.003302\nTrain epoch: 752 [2560/5139 (48%)]\tLoss: 0.002660\nTrain epoch: 752 [5120/5139 (95%)]\tLoss: 0.001090\nTrain epoch: 753 [0/5139 (0%)]\tLoss: 0.003881\nTrain epoch: 753 [2560/5139 (48%)]\tLoss: 0.001026\nTrain epoch: 753 [5120/5139 (95%)]\tLoss: 0.001059\nTrain epoch: 754 [0/5139 (0%)]\tLoss: 0.006541\nTrain epoch: 754 [2560/5139 (48%)]\tLoss: 0.009086\nTrain epoch: 754 [5120/5139 (95%)]\tLoss: 0.000081\nTrain epoch: 755 [0/5139 (0%)]\tLoss: 0.005324\nTrain epoch: 755 [2560/5139 (48%)]\tLoss: 0.002622\nTrain epoch: 755 [5120/5139 (95%)]\tLoss: 0.000038\nTrain epoch: 756 [0/5139 (0%)]\tLoss: 0.003362\nTrain epoch: 756 [2560/5139 (48%)]\tLoss: 0.005572\nTrain epoch: 756 [5120/5139 (95%)]\tLoss: 0.001819\nTrain epoch: 757 [0/5139 (0%)]\tLoss: 0.036226\nTrain epoch: 757 [2560/5139 (48%)]\tLoss: 0.000793\nTrain epoch: 757 [5120/5139 (95%)]\tLoss: 0.012642\nTrain epoch: 758 [0/5139 (0%)]\tLoss: 0.037801\nTrain epoch: 758 [2560/5139 (48%)]\tLoss: 0.102983\nTrain epoch: 758 [5120/5139 (95%)]\tLoss: 0.000824\nTrain epoch: 759 [0/5139 (0%)]\tLoss: 0.017231\nTrain epoch: 759 [2560/5139 (48%)]\tLoss: 0.006098\nTrain epoch: 759 [5120/5139 (95%)]\tLoss: 0.001399\nTrain epoch: 760 [0/5139 (0%)]\tLoss: 0.013801\nTrain epoch: 760 [2560/5139 (48%)]\tLoss: 0.010202\nTrain epoch: 760 [5120/5139 (95%)]\tLoss: 0.001566\nTrain epoch: 761 [0/5139 (0%)]\tLoss: 0.008844\nTrain epoch: 761 [2560/5139 (48%)]\tLoss: 0.003398\nTrain epoch: 761 [5120/5139 (95%)]\tLoss: 0.011029\nTrain epoch: 762 [0/5139 (0%)]\tLoss: 0.003330\nTrain epoch: 762 [2560/5139 (48%)]\tLoss: 0.003480\nTrain epoch: 762 [5120/5139 (95%)]\tLoss: 0.008920\nTrain epoch: 763 [0/5139 (0%)]\tLoss: 0.007715\nTrain epoch: 763 [2560/5139 (48%)]\tLoss: 0.007443\nTrain epoch: 763 [5120/5139 (95%)]\tLoss: 0.010305\nTrain epoch: 764 [0/5139 (0%)]\tLoss: 0.008524\nTrain epoch: 764 [2560/5139 (48%)]\tLoss: 0.002515\nTrain epoch: 764 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 765 [0/5139 (0%)]\tLoss: 0.004567\nTrain epoch: 765 [2560/5139 (48%)]\tLoss: 0.030310\nTrain epoch: 765 [5120/5139 (95%)]\tLoss: 0.000009\nTrain epoch: 766 [0/5139 (0%)]\tLoss: 0.020903\nTrain epoch: 766 [2560/5139 (48%)]\tLoss: 0.009275\nTrain epoch: 766 [5120/5139 (95%)]\tLoss: 0.000719\nTrain epoch: 767 [0/5139 (0%)]\tLoss: 0.000922\nTrain epoch: 767 [2560/5139 (48%)]\tLoss: 0.090855\nTrain epoch: 767 [5120/5139 (95%)]\tLoss: 0.043456\nTrain epoch: 768 [0/5139 (0%)]\tLoss: 0.004724\nTrain epoch: 768 [2560/5139 (48%)]\tLoss: 0.011513\nTrain epoch: 768 [5120/5139 (95%)]\tLoss: 0.073651\nTrain epoch: 769 [0/5139 (0%)]\tLoss: 0.015195\nTrain epoch: 769 [2560/5139 (48%)]\tLoss: 0.028461\nTrain epoch: 769 [5120/5139 (95%)]\tLoss: 0.004870\nTrain epoch: 770 [0/5139 (0%)]\tLoss: 0.029949\nTrain epoch: 770 [2560/5139 (48%)]\tLoss: 0.002609\nTrain epoch: 770 [5120/5139 (95%)]\tLoss: 0.002449\nTrain epoch: 771 [0/5139 (0%)]\tLoss: 0.010539\nTrain epoch: 771 [2560/5139 (48%)]\tLoss: 0.006327\nTrain epoch: 771 [5120/5139 (95%)]\tLoss: 0.051261\nTrain epoch: 772 [0/5139 (0%)]\tLoss: 0.015735\nTrain epoch: 772 [2560/5139 (48%)]\tLoss: 0.052174\nTrain epoch: 772 [5120/5139 (95%)]\tLoss: 0.006117\nTrain epoch: 773 [0/5139 (0%)]\tLoss: 0.005264\nTrain epoch: 773 [2560/5139 (48%)]\tLoss: 0.006841\nTrain epoch: 773 [5120/5139 (95%)]\tLoss: 0.000069\nTrain epoch: 774 [0/5139 (0%)]\tLoss: 0.005116\nTrain epoch: 774 [2560/5139 (48%)]\tLoss: 0.008003\nTrain epoch: 774 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 775 [0/5139 (0%)]\tLoss: 0.006703\nTrain epoch: 775 [2560/5139 (48%)]\tLoss: 0.019060\nTrain epoch: 775 [5120/5139 (95%)]\tLoss: 0.001085\nTrain epoch: 776 [0/5139 (0%)]\tLoss: 0.007092\nTrain epoch: 776 [2560/5139 (48%)]\tLoss: 0.006511\nTrain epoch: 776 [5120/5139 (95%)]\tLoss: 0.005623\nTrain epoch: 777 [0/5139 (0%)]\tLoss: 0.001224\nTrain epoch: 777 [2560/5139 (48%)]\tLoss: 0.000218\nTrain epoch: 777 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 778 [0/5139 (0%)]\tLoss: 0.015413\nTrain epoch: 778 [2560/5139 (48%)]\tLoss: 0.001202\nTrain epoch: 778 [5120/5139 (95%)]\tLoss: 0.000018\nTrain epoch: 779 [0/5139 (0%)]\tLoss: 0.024391\nTrain epoch: 779 [2560/5139 (48%)]\tLoss: 0.020287\nTrain epoch: 779 [5120/5139 (95%)]\tLoss: 0.002261\nTrain epoch: 780 [0/5139 (0%)]\tLoss: 0.002351\nTrain epoch: 780 [2560/5139 (48%)]\tLoss: 0.002373\nTrain epoch: 780 [5120/5139 (95%)]\tLoss: 0.000088\nTrain epoch: 781 [0/5139 (0%)]\tLoss: 0.002994\nTrain epoch: 781 [2560/5139 (48%)]\tLoss: 0.028740\nTrain epoch: 781 [5120/5139 (95%)]\tLoss: 0.000078\nTrain epoch: 782 [0/5139 (0%)]\tLoss: 0.001252\nTrain epoch: 782 [2560/5139 (48%)]\tLoss: 0.000222\nTrain epoch: 782 [5120/5139 (95%)]\tLoss: 0.000090\nTrain epoch: 783 [0/5139 (0%)]\tLoss: 0.006562\nTrain epoch: 783 [2560/5139 (48%)]\tLoss: 0.011978\nTrain epoch: 783 [5120/5139 (95%)]\tLoss: 0.000308\nTrain epoch: 784 [0/5139 (0%)]\tLoss: 0.077708\nTrain epoch: 784 [2560/5139 (48%)]\tLoss: 0.000585\nTrain epoch: 784 [5120/5139 (95%)]\tLoss: 0.000149\nTrain epoch: 785 [0/5139 (0%)]\tLoss: 0.014598\nTrain epoch: 785 [2560/5139 (48%)]\tLoss: 0.011611\nTrain epoch: 785 [5120/5139 (95%)]\tLoss: 0.000175\nTrain epoch: 786 [0/5139 (0%)]\tLoss: 0.009752\nTrain epoch: 786 [2560/5139 (48%)]\tLoss: 0.003000\nTrain epoch: 786 [5120/5139 (95%)]\tLoss: 0.000701\nTrain epoch: 787 [0/5139 (0%)]\tLoss: 0.001753\nTrain epoch: 787 [2560/5139 (48%)]\tLoss: 0.000463\nTrain epoch: 787 [5120/5139 (95%)]\tLoss: 0.000025\nTrain epoch: 788 [0/5139 (0%)]\tLoss: 0.003022\nTrain epoch: 788 [2560/5139 (48%)]\tLoss: 0.001632\nTrain epoch: 788 [5120/5139 (95%)]\tLoss: 0.000139\nTrain epoch: 789 [0/5139 (0%)]\tLoss: 0.004019\nTrain epoch: 789 [2560/5139 (48%)]\tLoss: 0.000157\nTrain epoch: 789 [5120/5139 (95%)]\tLoss: 0.000126\nTrain epoch: 790 [0/5139 (0%)]\tLoss: 0.001324\nTrain epoch: 790 [2560/5139 (48%)]\tLoss: 0.002207\nTrain epoch: 790 [5120/5139 (95%)]\tLoss: 0.000108\nTrain epoch: 791 [0/5139 (0%)]\tLoss: 0.005542\nTrain epoch: 791 [2560/5139 (48%)]\tLoss: 0.000522\nTrain epoch: 791 [5120/5139 (95%)]\tLoss: 0.001627\nTrain epoch: 792 [0/5139 (0%)]\tLoss: 0.003337\nTrain epoch: 792 [2560/5139 (48%)]\tLoss: 0.000917\nTrain epoch: 792 [5120/5139 (95%)]\tLoss: 0.000404\nTrain epoch: 793 [0/5139 (0%)]\tLoss: 0.001245\nTrain epoch: 793 [2560/5139 (48%)]\tLoss: 0.000020\nTrain epoch: 793 [5120/5139 (95%)]\tLoss: 0.001710\nTrain epoch: 794 [0/5139 (0%)]\tLoss: 0.002365\nTrain epoch: 794 [2560/5139 (48%)]\tLoss: 0.000436\nTrain epoch: 794 [5120/5139 (95%)]\tLoss: 0.000247\nTrain epoch: 795 [0/5139 (0%)]\tLoss: 0.000222\nTrain epoch: 795 [2560/5139 (48%)]\tLoss: 0.000551\nTrain epoch: 795 [5120/5139 (95%)]\tLoss: 0.000066\nTrain epoch: 796 [0/5139 (0%)]\tLoss: 0.001552\nTrain epoch: 796 [2560/5139 (48%)]\tLoss: 0.001550\nTrain epoch: 796 [5120/5139 (95%)]\tLoss: 0.000025\nTrain epoch: 797 [0/5139 (0%)]\tLoss: 0.001224\nTrain epoch: 797 [2560/5139 (48%)]\tLoss: 0.002851\nTrain epoch: 797 [5120/5139 (95%)]\tLoss: 0.007402\nTrain epoch: 798 [0/5139 (0%)]\tLoss: 0.000839\nTrain epoch: 798 [2560/5139 (48%)]\tLoss: 0.020994\nTrain epoch: 798 [5120/5139 (95%)]\tLoss: 0.000017\nTrain epoch: 799 [0/5139 (0%)]\tLoss: 0.011290\nTrain epoch: 799 [2560/5139 (48%)]\tLoss: 0.002490\nTrain epoch: 799 [5120/5139 (95%)]\tLoss: 0.000013\nTrain epoch: 800 [0/5139 (0%)]\tLoss: 0.004678\nTrain epoch: 800 [2560/5139 (48%)]\tLoss: 0.000220\nTrain epoch: 800 [5120/5139 (95%)]\tLoss: 0.000727\nTrain epoch: 801 [0/5139 (0%)]\tLoss: 0.160225\nTrain epoch: 801 [2560/5139 (48%)]\tLoss: 0.904324\nTrain epoch: 801 [5120/5139 (95%)]\tLoss: 0.134241\nTrain epoch: 802 [0/5139 (0%)]\tLoss: 0.242490\nTrain epoch: 802 [2560/5139 (48%)]\tLoss: 0.169313\nTrain epoch: 802 [5120/5139 (95%)]\tLoss: 0.085373\nTrain epoch: 803 [0/5139 (0%)]\tLoss: 0.140513\nTrain epoch: 803 [2560/5139 (48%)]\tLoss: 0.084601\nTrain epoch: 803 [5120/5139 (95%)]\tLoss: 0.015499\nTrain epoch: 804 [0/5139 (0%)]\tLoss: 0.058027\nTrain epoch: 804 [2560/5139 (48%)]\tLoss: 0.028970\nTrain epoch: 804 [5120/5139 (95%)]\tLoss: 0.002959\nTrain epoch: 805 [0/5139 (0%)]\tLoss: 0.036024\nTrain epoch: 805 [2560/5139 (48%)]\tLoss: 0.029450\nTrain epoch: 805 [5120/5139 (95%)]\tLoss: 0.000726\nTrain epoch: 806 [0/5139 (0%)]\tLoss: 0.032197\nTrain epoch: 806 [2560/5139 (48%)]\tLoss: 0.017729\nTrain epoch: 806 [5120/5139 (95%)]\tLoss: 0.026466\nTrain epoch: 807 [0/5139 (0%)]\tLoss: 0.018067\nTrain epoch: 807 [2560/5139 (48%)]\tLoss: 0.026640\nTrain epoch: 807 [5120/5139 (95%)]\tLoss: 0.003789\nTrain epoch: 808 [0/5139 (0%)]\tLoss: 0.024234\nTrain epoch: 808 [2560/5139 (48%)]\tLoss: 0.012867\nTrain epoch: 808 [5120/5139 (95%)]\tLoss: 0.002627\nTrain epoch: 809 [0/5139 (0%)]\tLoss: 0.003467\nTrain epoch: 809 [2560/5139 (48%)]\tLoss: 0.032162\nTrain epoch: 809 [5120/5139 (95%)]\tLoss: 0.009859\nTrain epoch: 810 [0/5139 (0%)]\tLoss: 0.004691\nTrain epoch: 810 [2560/5139 (48%)]\tLoss: 0.025254\nTrain epoch: 810 [5120/5139 (95%)]\tLoss: 0.016780\nTrain epoch: 811 [0/5139 (0%)]\tLoss: 0.004572\nTrain epoch: 811 [2560/5139 (48%)]\tLoss: 0.009190\nTrain epoch: 811 [5120/5139 (95%)]\tLoss: 0.000206\nTrain epoch: 812 [0/5139 (0%)]\tLoss: 0.002004\nTrain epoch: 812 [2560/5139 (48%)]\tLoss: 0.003972\nTrain epoch: 812 [5120/5139 (95%)]\tLoss: 0.011791\nTrain epoch: 813 [0/5139 (0%)]\tLoss: 0.013548\nTrain epoch: 813 [2560/5139 (48%)]\tLoss: 0.002349\nTrain epoch: 813 [5120/5139 (95%)]\tLoss: 0.000781\nTrain epoch: 814 [0/5139 (0%)]\tLoss: 0.007066\nTrain epoch: 814 [2560/5139 (48%)]\tLoss: 0.001308\nTrain epoch: 814 [5120/5139 (95%)]\tLoss: 0.000685\nTrain epoch: 815 [0/5139 (0%)]\tLoss: 0.014020\nTrain epoch: 815 [2560/5139 (48%)]\tLoss: 0.014899\nTrain epoch: 815 [5120/5139 (95%)]\tLoss: 0.001305\nTrain epoch: 816 [0/5139 (0%)]\tLoss: 0.006872\nTrain epoch: 816 [2560/5139 (48%)]\tLoss: 0.013618\nTrain epoch: 816 [5120/5139 (95%)]\tLoss: 0.002878\nTrain epoch: 817 [0/5139 (0%)]\tLoss: 0.002982\nTrain epoch: 817 [2560/5139 (48%)]\tLoss: 0.012277\nTrain epoch: 817 [5120/5139 (95%)]\tLoss: 0.000218\nTrain epoch: 818 [0/5139 (0%)]\tLoss: 0.000676\nTrain epoch: 818 [2560/5139 (48%)]\tLoss: 0.001023\nTrain epoch: 818 [5120/5139 (95%)]\tLoss: 0.006428\nTrain epoch: 819 [0/5139 (0%)]\tLoss: 0.003952\nTrain epoch: 819 [2560/5139 (48%)]\tLoss: 0.001981\nTrain epoch: 819 [5120/5139 (95%)]\tLoss: 0.000678\nTrain epoch: 820 [0/5139 (0%)]\tLoss: 0.006232\nTrain epoch: 820 [2560/5139 (48%)]\tLoss: 0.009236\nTrain epoch: 820 [5120/5139 (95%)]\tLoss: 0.000042\nTrain epoch: 821 [0/5139 (0%)]\tLoss: 0.003253\nTrain epoch: 821 [2560/5139 (48%)]\tLoss: 0.002053\nTrain epoch: 821 [5120/5139 (95%)]\tLoss: 0.000795\nTrain epoch: 822 [0/5139 (0%)]\tLoss: 0.008969\nTrain epoch: 822 [2560/5139 (48%)]\tLoss: 0.012313\nTrain epoch: 822 [5120/5139 (95%)]\tLoss: 0.000108\nTrain epoch: 823 [0/5139 (0%)]\tLoss: 0.023543\nTrain epoch: 823 [2560/5139 (48%)]\tLoss: 0.005225\nTrain epoch: 823 [5120/5139 (95%)]\tLoss: 0.000007\nTrain epoch: 824 [0/5139 (0%)]\tLoss: 0.008860\nTrain epoch: 824 [2560/5139 (48%)]\tLoss: 0.001533\nTrain epoch: 824 [5120/5139 (95%)]\tLoss: 0.000355\nTrain epoch: 825 [0/5139 (0%)]\tLoss: 0.001959\nTrain epoch: 825 [2560/5139 (48%)]\tLoss: 0.014886\nTrain epoch: 825 [5120/5139 (95%)]\tLoss: 0.016242\nTrain epoch: 826 [0/5139 (0%)]\tLoss: 0.005814\nTrain epoch: 826 [2560/5139 (48%)]\tLoss: 0.002958\nTrain epoch: 826 [5120/5139 (95%)]\tLoss: 0.002949\nTrain epoch: 827 [0/5139 (0%)]\tLoss: 0.000906\nTrain epoch: 827 [2560/5139 (48%)]\tLoss: 0.003733\nTrain epoch: 827 [5120/5139 (95%)]\tLoss: 0.316340\nTrain epoch: 828 [0/5139 (0%)]\tLoss: 0.323307\nTrain epoch: 828 [2560/5139 (48%)]\tLoss: 0.108510\nTrain epoch: 828 [5120/5139 (95%)]\tLoss: 0.031026\nTrain epoch: 829 [0/5139 (0%)]\tLoss: 0.062770\nTrain epoch: 829 [2560/5139 (48%)]\tLoss: 0.033114\nTrain epoch: 829 [5120/5139 (95%)]\tLoss: 0.001501\nTrain epoch: 830 [0/5139 (0%)]\tLoss: 0.011792\nTrain epoch: 830 [2560/5139 (48%)]\tLoss: 0.019701\nTrain epoch: 830 [5120/5139 (95%)]\tLoss: 0.000329\nTrain epoch: 831 [0/5139 (0%)]\tLoss: 0.009869\nTrain epoch: 831 [2560/5139 (48%)]\tLoss: 0.010931\nTrain epoch: 831 [5120/5139 (95%)]\tLoss: 0.000011\nTrain epoch: 832 [0/5139 (0%)]\tLoss: 0.029594\nTrain epoch: 832 [2560/5139 (48%)]\tLoss: 0.006134\nTrain epoch: 832 [5120/5139 (95%)]\tLoss: 0.000035\nTrain epoch: 833 [0/5139 (0%)]\tLoss: 0.005683\nTrain epoch: 833 [2560/5139 (48%)]\tLoss: 0.002666\nTrain epoch: 833 [5120/5139 (95%)]\tLoss: 0.000144\nTrain epoch: 834 [0/5139 (0%)]\tLoss: 0.002248\nTrain epoch: 834 [2560/5139 (48%)]\tLoss: 0.006293\nTrain epoch: 834 [5120/5139 (95%)]\tLoss: 0.000027\nTrain epoch: 835 [0/5139 (0%)]\tLoss: 0.012383\nTrain epoch: 835 [2560/5139 (48%)]\tLoss: 0.003385\nTrain epoch: 835 [5120/5139 (95%)]\tLoss: 0.000226\nTrain epoch: 836 [0/5139 (0%)]\tLoss: 0.004503\nTrain epoch: 836 [2560/5139 (48%)]\tLoss: 0.001462\nTrain epoch: 836 [5120/5139 (95%)]\tLoss: 0.000239\nTrain epoch: 837 [0/5139 (0%)]\tLoss: 0.000631\nTrain epoch: 837 [2560/5139 (48%)]\tLoss: 0.003071\nTrain epoch: 837 [5120/5139 (95%)]\tLoss: 0.000282\nTrain epoch: 838 [0/5139 (0%)]\tLoss: 0.026306\nTrain epoch: 838 [2560/5139 (48%)]\tLoss: 0.001315\nTrain epoch: 838 [5120/5139 (95%)]\tLoss: 0.000013\nTrain epoch: 839 [0/5139 (0%)]\tLoss: 0.000818\nTrain epoch: 839 [2560/5139 (48%)]\tLoss: 0.008038\nTrain epoch: 839 [5120/5139 (95%)]\tLoss: 0.000205\nTrain epoch: 840 [0/5139 (0%)]\tLoss: 0.001023\nTrain epoch: 840 [2560/5139 (48%)]\tLoss: 0.002341\nTrain epoch: 840 [5120/5139 (95%)]\tLoss: 0.003184\nTrain epoch: 841 [0/5139 (0%)]\tLoss: 0.041783\nTrain epoch: 841 [2560/5139 (48%)]\tLoss: 0.003660\nTrain epoch: 841 [5120/5139 (95%)]\tLoss: 0.014639\nTrain epoch: 842 [0/5139 (0%)]\tLoss: 0.004205\nTrain epoch: 842 [2560/5139 (48%)]\tLoss: 0.004018\nTrain epoch: 842 [5120/5139 (95%)]\tLoss: 0.000089\nTrain epoch: 843 [0/5139 (0%)]\tLoss: 0.006474\nTrain epoch: 843 [2560/5139 (48%)]\tLoss: 0.001122\nTrain epoch: 843 [5120/5139 (95%)]\tLoss: 0.001644\nTrain epoch: 844 [0/5139 (0%)]\tLoss: 0.001241\nTrain epoch: 844 [2560/5139 (48%)]\tLoss: 0.001814\nTrain epoch: 844 [5120/5139 (95%)]\tLoss: 0.000095\nTrain epoch: 845 [0/5139 (0%)]\tLoss: 0.000992\nTrain epoch: 845 [2560/5139 (48%)]\tLoss: 0.001476\nTrain epoch: 845 [5120/5139 (95%)]\tLoss: 0.000060\nTrain epoch: 846 [0/5139 (0%)]\tLoss: 0.000731\nTrain epoch: 846 [2560/5139 (48%)]\tLoss: 0.000801\nTrain epoch: 846 [5120/5139 (95%)]\tLoss: 0.008990\nTrain epoch: 847 [0/5139 (0%)]\tLoss: 0.003304\nTrain epoch: 847 [2560/5139 (48%)]\tLoss: 0.244225\nTrain epoch: 847 [5120/5139 (95%)]\tLoss: 0.084129\nTrain epoch: 848 [0/5139 (0%)]\tLoss: 0.013695\nTrain epoch: 848 [2560/5139 (48%)]\tLoss: 0.010852\nTrain epoch: 848 [5120/5139 (95%)]\tLoss: 0.002826\nTrain epoch: 849 [0/5139 (0%)]\tLoss: 0.012498\nTrain epoch: 849 [2560/5139 (48%)]\tLoss: 0.006288\nTrain epoch: 849 [5120/5139 (95%)]\tLoss: 0.003852\nTrain epoch: 850 [0/5139 (0%)]\tLoss: 0.013183\nTrain epoch: 850 [2560/5139 (48%)]\tLoss: 0.004625\nTrain epoch: 850 [5120/5139 (95%)]\tLoss: 0.000697\nTrain epoch: 851 [0/5139 (0%)]\tLoss: 0.002642\nTrain epoch: 851 [2560/5139 (48%)]\tLoss: 0.010078\nTrain epoch: 851 [5120/5139 (95%)]\tLoss: 0.000774\nTrain epoch: 852 [0/5139 (0%)]\tLoss: 0.000981\nTrain epoch: 852 [2560/5139 (48%)]\tLoss: 0.005742\nTrain epoch: 852 [5120/5139 (95%)]\tLoss: 0.000903\nTrain epoch: 853 [0/5139 (0%)]\tLoss: 0.002031\nTrain epoch: 853 [2560/5139 (48%)]\tLoss: 0.001783\nTrain epoch: 853 [5120/5139 (95%)]\tLoss: 0.000376\nTrain epoch: 854 [0/5139 (0%)]\tLoss: 0.001317\nTrain epoch: 854 [2560/5139 (48%)]\tLoss: 0.000809\nTrain epoch: 854 [5120/5139 (95%)]\tLoss: 0.001894\nTrain epoch: 855 [0/5139 (0%)]\tLoss: 0.000877\nTrain epoch: 855 [2560/5139 (48%)]\tLoss: 0.000339\nTrain epoch: 855 [5120/5139 (95%)]\tLoss: 0.000009\nTrain epoch: 856 [0/5139 (0%)]\tLoss: 0.001188\nTrain epoch: 856 [2560/5139 (48%)]\tLoss: 0.015689\nTrain epoch: 856 [5120/5139 (95%)]\tLoss: 0.000077\nTrain epoch: 857 [0/5139 (0%)]\tLoss: 0.023736\nTrain epoch: 857 [2560/5139 (48%)]\tLoss: 0.016801\nTrain epoch: 857 [5120/5139 (95%)]\tLoss: 0.030442\nTrain epoch: 858 [0/5139 (0%)]\tLoss: 0.004860\nTrain epoch: 858 [2560/5139 (48%)]\tLoss: 0.253573\nTrain epoch: 858 [5120/5139 (95%)]\tLoss: 0.020095\nTrain epoch: 859 [0/5139 (0%)]\tLoss: 0.161750\nTrain epoch: 859 [2560/5139 (48%)]\tLoss: 0.049790\nTrain epoch: 859 [5120/5139 (95%)]\tLoss: 0.013902\nTrain epoch: 860 [0/5139 (0%)]\tLoss: 0.020639\nTrain epoch: 860 [2560/5139 (48%)]\tLoss: 0.050946\nTrain epoch: 860 [5120/5139 (95%)]\tLoss: 0.000305\nTrain epoch: 861 [0/5139 (0%)]\tLoss: 0.034116\nTrain epoch: 861 [2560/5139 (48%)]\tLoss: 0.018632\nTrain epoch: 861 [5120/5139 (95%)]\tLoss: 0.002768\nTrain epoch: 862 [0/5139 (0%)]\tLoss: 0.014050\nTrain epoch: 862 [2560/5139 (48%)]\tLoss: 0.004296\nTrain epoch: 862 [5120/5139 (95%)]\tLoss: 0.000282\nTrain epoch: 863 [0/5139 (0%)]\tLoss: 0.005262\nTrain epoch: 863 [2560/5139 (48%)]\tLoss: 0.007138\nTrain epoch: 863 [5120/5139 (95%)]\tLoss: 0.002794\nTrain epoch: 864 [0/5139 (0%)]\tLoss: 0.050414\nTrain epoch: 864 [2560/5139 (48%)]\tLoss: 0.006450\nTrain epoch: 864 [5120/5139 (95%)]\tLoss: 0.002090\nTrain epoch: 865 [0/5139 (0%)]\tLoss: 0.005319\nTrain epoch: 865 [2560/5139 (48%)]\tLoss: 0.013398\nTrain epoch: 865 [5120/5139 (95%)]\tLoss: 0.000077\nTrain epoch: 866 [0/5139 (0%)]\tLoss: 0.002472\nTrain epoch: 866 [2560/5139 (48%)]\tLoss: 0.004596\nTrain epoch: 866 [5120/5139 (95%)]\tLoss: 0.000143\nTrain epoch: 867 [0/5139 (0%)]\tLoss: 0.000747\nTrain epoch: 867 [2560/5139 (48%)]\tLoss: 0.000692\nTrain epoch: 867 [5120/5139 (95%)]\tLoss: 0.000213\nTrain epoch: 868 [0/5139 (0%)]\tLoss: 0.004533\nTrain epoch: 868 [2560/5139 (48%)]\tLoss: 0.002080\nTrain epoch: 868 [5120/5139 (95%)]\tLoss: 0.000658\nTrain epoch: 869 [0/5139 (0%)]\tLoss: 0.001123\nTrain epoch: 869 [2560/5139 (48%)]\tLoss: 0.001300\nTrain epoch: 869 [5120/5139 (95%)]\tLoss: 0.000221\nTrain epoch: 870 [0/5139 (0%)]\tLoss: 0.004750\nTrain epoch: 870 [2560/5139 (48%)]\tLoss: 0.001736\nTrain epoch: 870 [5120/5139 (95%)]\tLoss: 0.000508\nTrain epoch: 871 [0/5139 (0%)]\tLoss: 0.003432\nTrain epoch: 871 [2560/5139 (48%)]\tLoss: 0.001192\nTrain epoch: 871 [5120/5139 (95%)]\tLoss: 0.000325\nTrain epoch: 872 [0/5139 (0%)]\tLoss: 0.056496\nTrain epoch: 872 [2560/5139 (48%)]\tLoss: 0.002476\nTrain epoch: 872 [5120/5139 (95%)]\tLoss: 0.000195\nTrain epoch: 873 [0/5139 (0%)]\tLoss: 0.206545\nTrain epoch: 873 [2560/5139 (48%)]\tLoss: 0.027267\nTrain epoch: 873 [5120/5139 (95%)]\tLoss: 0.021427\nTrain epoch: 874 [0/5139 (0%)]\tLoss: 0.052148\nTrain epoch: 874 [2560/5139 (48%)]\tLoss: 0.008436\nTrain epoch: 874 [5120/5139 (95%)]\tLoss: 0.000617\nTrain epoch: 875 [0/5139 (0%)]\tLoss: 0.001677\nTrain epoch: 875 [2560/5139 (48%)]\tLoss: 0.001376\nTrain epoch: 875 [5120/5139 (95%)]\tLoss: 0.000753\nTrain epoch: 876 [0/5139 (0%)]\tLoss: 0.008268\nTrain epoch: 876 [2560/5139 (48%)]\tLoss: 0.001005\nTrain epoch: 876 [5120/5139 (95%)]\tLoss: 0.000074\nTrain epoch: 877 [0/5139 (0%)]\tLoss: 0.000822\nTrain epoch: 877 [2560/5139 (48%)]\tLoss: 0.001629\nTrain epoch: 877 [5120/5139 (95%)]\tLoss: 0.000184\nTrain epoch: 878 [0/5139 (0%)]\tLoss: 0.001602\nTrain epoch: 878 [2560/5139 (48%)]\tLoss: 0.001846\nTrain epoch: 878 [5120/5139 (95%)]\tLoss: 0.000162\nTrain epoch: 879 [0/5139 (0%)]\tLoss: 0.001430\nTrain epoch: 879 [2560/5139 (48%)]\tLoss: 0.002590\nTrain epoch: 879 [5120/5139 (95%)]\tLoss: 0.127675\nTrain epoch: 880 [0/5139 (0%)]\tLoss: 0.021885\nTrain epoch: 880 [2560/5139 (48%)]\tLoss: 0.131160\nTrain epoch: 880 [5120/5139 (95%)]\tLoss: 0.013611\nTrain epoch: 881 [0/5139 (0%)]\tLoss: 0.137701\nTrain epoch: 881 [2560/5139 (48%)]\tLoss: 0.081166\nTrain epoch: 881 [5120/5139 (95%)]\tLoss: 0.005219\nTrain epoch: 882 [0/5139 (0%)]\tLoss: 0.019008\nTrain epoch: 882 [2560/5139 (48%)]\tLoss: 0.014265\nTrain epoch: 882 [5120/5139 (95%)]\tLoss: 0.019832\nTrain epoch: 883 [0/5139 (0%)]\tLoss: 0.020108\nTrain epoch: 883 [2560/5139 (48%)]\tLoss: 0.009191\nTrain epoch: 883 [5120/5139 (95%)]\tLoss: 0.000020\nTrain epoch: 884 [0/5139 (0%)]\tLoss: 0.000926\nTrain epoch: 884 [2560/5139 (48%)]\tLoss: 0.007359\nTrain epoch: 884 [5120/5139 (95%)]\tLoss: 0.000925\nTrain epoch: 885 [0/5139 (0%)]\tLoss: 0.006633\nTrain epoch: 885 [2560/5139 (48%)]\tLoss: 0.002972\nTrain epoch: 885 [5120/5139 (95%)]\tLoss: 0.000288\nTrain epoch: 886 [0/5139 (0%)]\tLoss: 0.004905\nTrain epoch: 886 [2560/5139 (48%)]\tLoss: 0.007017\nTrain epoch: 886 [5120/5139 (95%)]\tLoss: 0.028045\nTrain epoch: 887 [0/5139 (0%)]\tLoss: 0.000563\nTrain epoch: 887 [2560/5139 (48%)]\tLoss: 0.000430\nTrain epoch: 887 [5120/5139 (95%)]\tLoss: 0.007713\nTrain epoch: 888 [0/5139 (0%)]\tLoss: 0.008819\nTrain epoch: 888 [2560/5139 (48%)]\tLoss: 0.015088\nTrain epoch: 888 [5120/5139 (95%)]\tLoss: 0.000207\nTrain epoch: 889 [0/5139 (0%)]\tLoss: 0.002313\nTrain epoch: 889 [2560/5139 (48%)]\tLoss: 0.002569\nTrain epoch: 889 [5120/5139 (95%)]\tLoss: 0.000051\nTrain epoch: 890 [0/5139 (0%)]\tLoss: 0.000362\nTrain epoch: 890 [2560/5139 (48%)]\tLoss: 0.001276\nTrain epoch: 890 [5120/5139 (95%)]\tLoss: 0.051898\nTrain epoch: 891 [0/5139 (0%)]\tLoss: 0.013416\nTrain epoch: 891 [2560/5139 (48%)]\tLoss: 0.001675\nTrain epoch: 891 [5120/5139 (95%)]\tLoss: 0.010242\nTrain epoch: 892 [0/5139 (0%)]\tLoss: 0.000757\nTrain epoch: 892 [2560/5139 (48%)]\tLoss: 0.000107\nTrain epoch: 892 [5120/5139 (95%)]\tLoss: 0.000079\nTrain epoch: 893 [0/5139 (0%)]\tLoss: 0.000946\nTrain epoch: 893 [2560/5139 (48%)]\tLoss: 0.000324\nTrain epoch: 893 [5120/5139 (95%)]\tLoss: 0.000009\nTrain epoch: 894 [0/5139 (0%)]\tLoss: 0.002913\nTrain epoch: 894 [2560/5139 (48%)]\tLoss: 0.025985\nTrain epoch: 894 [5120/5139 (95%)]\tLoss: 0.000436\nTrain epoch: 895 [0/5139 (0%)]\tLoss: 0.004339\nTrain epoch: 895 [2560/5139 (48%)]\tLoss: 0.032256\nTrain epoch: 895 [5120/5139 (95%)]\tLoss: 0.004194\nTrain epoch: 896 [0/5139 (0%)]\tLoss: 0.049057\nTrain epoch: 896 [2560/5139 (48%)]\tLoss: 0.019341\nTrain epoch: 896 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 897 [0/5139 (0%)]\tLoss: 0.001139\nTrain epoch: 897 [2560/5139 (48%)]\tLoss: 0.001164\nTrain epoch: 897 [5120/5139 (95%)]\tLoss: 0.001081\nTrain epoch: 898 [0/5139 (0%)]\tLoss: 0.003492\nTrain epoch: 898 [2560/5139 (48%)]\tLoss: 0.006443\nTrain epoch: 898 [5120/5139 (95%)]\tLoss: 0.001626\nTrain epoch: 899 [0/5139 (0%)]\tLoss: 0.014564\nTrain epoch: 899 [2560/5139 (48%)]\tLoss: 0.008075\nTrain epoch: 899 [5120/5139 (95%)]\tLoss: 0.000412\nTrain epoch: 900 [0/5139 (0%)]\tLoss: 0.002411\nTrain epoch: 900 [2560/5139 (48%)]\tLoss: 0.002044\nTrain epoch: 900 [5120/5139 (95%)]\tLoss: 0.000584\nTrain epoch: 901 [0/5139 (0%)]\tLoss: 0.000926\nTrain epoch: 901 [2560/5139 (48%)]\tLoss: 0.021954\nTrain epoch: 901 [5120/5139 (95%)]\tLoss: 0.008902\nTrain epoch: 902 [0/5139 (0%)]\tLoss: 0.001665\nTrain epoch: 902 [2560/5139 (48%)]\tLoss: 0.003385\nTrain epoch: 902 [5120/5139 (95%)]\tLoss: 0.000198\nTrain epoch: 903 [0/5139 (0%)]\tLoss: 0.000773\nTrain epoch: 903 [2560/5139 (48%)]\tLoss: 0.002089\nTrain epoch: 903 [5120/5139 (95%)]\tLoss: 0.000103\nTrain epoch: 904 [0/5139 (0%)]\tLoss: 0.001856\nTrain epoch: 904 [2560/5139 (48%)]\tLoss: 0.000983\nTrain epoch: 904 [5120/5139 (95%)]\tLoss: 0.000646\nTrain epoch: 905 [0/5139 (0%)]\tLoss: 0.000404\nTrain epoch: 905 [2560/5139 (48%)]\tLoss: 0.000680\nTrain epoch: 905 [5120/5139 (95%)]\tLoss: 0.000026\nTrain epoch: 906 [0/5139 (0%)]\tLoss: 0.005818\nTrain epoch: 906 [2560/5139 (48%)]\tLoss: 0.012817\nTrain epoch: 906 [5120/5139 (95%)]\tLoss: 0.000041\nTrain epoch: 907 [0/5139 (0%)]\tLoss: 0.000954\nTrain epoch: 907 [2560/5139 (48%)]\tLoss: 0.003754\nTrain epoch: 907 [5120/5139 (95%)]\tLoss: 0.008031\nTrain epoch: 908 [0/5139 (0%)]\tLoss: 0.033255\nTrain epoch: 908 [2560/5139 (48%)]\tLoss: 0.003310\nTrain epoch: 908 [5120/5139 (95%)]\tLoss: 0.003446\nTrain epoch: 909 [0/5139 (0%)]\tLoss: 0.001074\nTrain epoch: 909 [2560/5139 (48%)]\tLoss: 0.005180\nTrain epoch: 909 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 910 [0/5139 (0%)]\tLoss: 0.002706\nTrain epoch: 910 [2560/5139 (48%)]\tLoss: 0.000750\nTrain epoch: 910 [5120/5139 (95%)]\tLoss: 0.000448\nTrain epoch: 911 [0/5139 (0%)]\tLoss: 0.039165\nTrain epoch: 911 [2560/5139 (48%)]\tLoss: 0.005427\nTrain epoch: 911 [5120/5139 (95%)]\tLoss: 0.000919\nTrain epoch: 912 [0/5139 (0%)]\tLoss: 0.003430\nTrain epoch: 912 [2560/5139 (48%)]\tLoss: 0.000445\nTrain epoch: 912 [5120/5139 (95%)]\tLoss: 0.000084\nTrain epoch: 913 [0/5139 (0%)]\tLoss: 0.001042\nTrain epoch: 913 [2560/5139 (48%)]\tLoss: 0.000345\nTrain epoch: 913 [5120/5139 (95%)]\tLoss: 0.001207\nTrain epoch: 914 [0/5139 (0%)]\tLoss: 0.000330\nTrain epoch: 914 [2560/5139 (48%)]\tLoss: 0.019371\nTrain epoch: 914 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 915 [0/5139 (0%)]\tLoss: 0.001085\nTrain epoch: 915 [2560/5139 (48%)]\tLoss: 0.014696\nTrain epoch: 915 [5120/5139 (95%)]\tLoss: 0.000057\nTrain epoch: 916 [0/5139 (0%)]\tLoss: 0.001821\nTrain epoch: 916 [2560/5139 (48%)]\tLoss: 0.003538\nTrain epoch: 916 [5120/5139 (95%)]\tLoss: 0.000006\nTrain epoch: 917 [0/5139 (0%)]\tLoss: 0.009546\nTrain epoch: 917 [2560/5139 (48%)]\tLoss: 0.015670\nTrain epoch: 917 [5120/5139 (95%)]\tLoss: 0.001862\nTrain epoch: 918 [0/5139 (0%)]\tLoss: 0.002722\nTrain epoch: 918 [2560/5139 (48%)]\tLoss: 0.000582\nTrain epoch: 918 [5120/5139 (95%)]\tLoss: 0.004307\nTrain epoch: 919 [0/5139 (0%)]\tLoss: 0.000988\nTrain epoch: 919 [2560/5139 (48%)]\tLoss: 0.000316\nTrain epoch: 919 [5120/5139 (95%)]\tLoss: 0.000028\nTrain epoch: 920 [0/5139 (0%)]\tLoss: 0.000399\nTrain epoch: 920 [2560/5139 (48%)]\tLoss: 0.014203\nTrain epoch: 920 [5120/5139 (95%)]\tLoss: 0.000889\nTrain epoch: 921 [0/5139 (0%)]\tLoss: 0.003485\nTrain epoch: 921 [2560/5139 (48%)]\tLoss: 0.006104\nTrain epoch: 921 [5120/5139 (95%)]\tLoss: 0.000187\nTrain epoch: 922 [0/5139 (0%)]\tLoss: 0.001062\nTrain epoch: 922 [2560/5139 (48%)]\tLoss: 0.003478\nTrain epoch: 922 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 923 [0/5139 (0%)]\tLoss: 0.000667\nTrain epoch: 923 [2560/5139 (48%)]\tLoss: 0.004849\nTrain epoch: 923 [5120/5139 (95%)]\tLoss: 0.000150\nTrain epoch: 924 [0/5139 (0%)]\tLoss: 0.001029\nTrain epoch: 924 [2560/5139 (48%)]\tLoss: 0.000747\nTrain epoch: 924 [5120/5139 (95%)]\tLoss: 0.000008\nTrain epoch: 925 [0/5139 (0%)]\tLoss: 0.001973\nTrain epoch: 925 [2560/5139 (48%)]\tLoss: 0.002117\nTrain epoch: 925 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 926 [0/5139 (0%)]\tLoss: 0.014652\nTrain epoch: 926 [2560/5139 (48%)]\tLoss: 0.000662\nTrain epoch: 926 [5120/5139 (95%)]\tLoss: 0.000014\nTrain epoch: 927 [0/5139 (0%)]\tLoss: 0.000861\nTrain epoch: 927 [2560/5139 (48%)]\tLoss: 0.001387\nTrain epoch: 927 [5120/5139 (95%)]\tLoss: 0.035603\nTrain epoch: 928 [0/5139 (0%)]\tLoss: 0.077554\nTrain epoch: 928 [2560/5139 (48%)]\tLoss: 0.008476\nTrain epoch: 928 [5120/5139 (95%)]\tLoss: 0.000696\nTrain epoch: 929 [0/5139 (0%)]\tLoss: 0.006863\nTrain epoch: 929 [2560/5139 (48%)]\tLoss: 0.004437\nTrain epoch: 929 [5120/5139 (95%)]\tLoss: 0.000312\nTrain epoch: 930 [0/5139 (0%)]\tLoss: 0.001802\nTrain epoch: 930 [2560/5139 (48%)]\tLoss: 0.001886\nTrain epoch: 930 [5120/5139 (95%)]\tLoss: 0.000007\nTrain epoch: 931 [0/5139 (0%)]\tLoss: 0.000679\nTrain epoch: 931 [2560/5139 (48%)]\tLoss: 0.001403\nTrain epoch: 931 [5120/5139 (95%)]\tLoss: 0.000063\nTrain epoch: 932 [0/5139 (0%)]\tLoss: 0.013334\nTrain epoch: 932 [2560/5139 (48%)]\tLoss: 0.000471\nTrain epoch: 932 [5120/5139 (95%)]\tLoss: 0.000655\nTrain epoch: 933 [0/5139 (0%)]\tLoss: 0.000906\nTrain epoch: 933 [2560/5139 (48%)]\tLoss: 0.000820\nTrain epoch: 933 [5120/5139 (95%)]\tLoss: 0.000244\nTrain epoch: 934 [0/5139 (0%)]\tLoss: 0.006424\nTrain epoch: 934 [2560/5139 (48%)]\tLoss: 0.017760\nTrain epoch: 934 [5120/5139 (95%)]\tLoss: 0.000715\nTrain epoch: 935 [0/5139 (0%)]\tLoss: 0.018799\nTrain epoch: 935 [2560/5139 (48%)]\tLoss: 0.006758\nTrain epoch: 935 [5120/5139 (95%)]\tLoss: 0.017239\nTrain epoch: 936 [0/5139 (0%)]\tLoss: 0.081761\nTrain epoch: 936 [2560/5139 (48%)]\tLoss: 0.014053\nTrain epoch: 936 [5120/5139 (95%)]\tLoss: 0.000381\nTrain epoch: 937 [0/5139 (0%)]\tLoss: 0.007209\nTrain epoch: 937 [2560/5139 (48%)]\tLoss: 0.018862\nTrain epoch: 937 [5120/5139 (95%)]\tLoss: 0.006176\nTrain epoch: 938 [0/5139 (0%)]\tLoss: 0.003107\nTrain epoch: 938 [2560/5139 (48%)]\tLoss: 0.000365\nTrain epoch: 938 [5120/5139 (95%)]\tLoss: 0.261988\nTrain epoch: 939 [0/5139 (0%)]\tLoss: 0.007181\nTrain epoch: 939 [2560/5139 (48%)]\tLoss: 0.079849\nTrain epoch: 939 [5120/5139 (95%)]\tLoss: 0.205183\nTrain epoch: 940 [0/5139 (0%)]\tLoss: 0.139470\nTrain epoch: 940 [2560/5139 (48%)]\tLoss: 0.088876\nTrain epoch: 940 [5120/5139 (95%)]\tLoss: 0.047223\nTrain epoch: 941 [0/5139 (0%)]\tLoss: 0.061162\nTrain epoch: 941 [2560/5139 (48%)]\tLoss: 0.010427\nTrain epoch: 941 [5120/5139 (95%)]\tLoss: 0.023061\nTrain epoch: 942 [0/5139 (0%)]\tLoss: 0.009865\nTrain epoch: 942 [2560/5139 (48%)]\tLoss: 0.030712\nTrain epoch: 942 [5120/5139 (95%)]\tLoss: 0.005478\nTrain epoch: 943 [0/5139 (0%)]\tLoss: 0.003457\nTrain epoch: 943 [2560/5139 (48%)]\tLoss: 0.006793\nTrain epoch: 943 [5120/5139 (95%)]\tLoss: 0.016670\nTrain epoch: 944 [0/5139 (0%)]\tLoss: 0.006882\nTrain epoch: 944 [2560/5139 (48%)]\tLoss: 0.010698\nTrain epoch: 944 [5120/5139 (95%)]\tLoss: 0.001578\nTrain epoch: 945 [0/5139 (0%)]\tLoss: 0.002539\nTrain epoch: 945 [2560/5139 (48%)]\tLoss: 0.018852\nTrain epoch: 945 [5120/5139 (95%)]\tLoss: 0.000142\nTrain epoch: 946 [0/5139 (0%)]\tLoss: 0.004383\nTrain epoch: 946 [2560/5139 (48%)]\tLoss: 0.011582\nTrain epoch: 946 [5120/5139 (95%)]\tLoss: 0.000863\nTrain epoch: 947 [0/5139 (0%)]\tLoss: 0.005451\nTrain epoch: 947 [2560/5139 (48%)]\tLoss: 0.006053\nTrain epoch: 947 [5120/5139 (95%)]\tLoss: 0.000015\nTrain epoch: 948 [0/5139 (0%)]\tLoss: 0.000759\nTrain epoch: 948 [2560/5139 (48%)]\tLoss: 0.008289\nTrain epoch: 948 [5120/5139 (95%)]\tLoss: 0.000024\nTrain epoch: 949 [0/5139 (0%)]\tLoss: 0.000501\nTrain epoch: 949 [2560/5139 (48%)]\tLoss: 0.001923\nTrain epoch: 949 [5120/5139 (95%)]\tLoss: 0.000008\nTrain epoch: 950 [0/5139 (0%)]\tLoss: 0.006996\nTrain epoch: 950 [2560/5139 (48%)]\tLoss: 0.003935\nTrain epoch: 950 [5120/5139 (95%)]\tLoss: 0.026926\nTrain epoch: 951 [0/5139 (0%)]\tLoss: 0.000320\nTrain epoch: 951 [2560/5139 (48%)]\tLoss: 0.048522\nTrain epoch: 951 [5120/5139 (95%)]\tLoss: 0.024695\nTrain epoch: 952 [0/5139 (0%)]\tLoss: 0.037237\nTrain epoch: 952 [2560/5139 (48%)]\tLoss: 0.011329\nTrain epoch: 952 [5120/5139 (95%)]\tLoss: 0.027564\nTrain epoch: 953 [0/5139 (0%)]\tLoss: 0.005050\nTrain epoch: 953 [2560/5139 (48%)]\tLoss: 0.001987\nTrain epoch: 953 [5120/5139 (95%)]\tLoss: 0.000005\nTrain epoch: 954 [0/5139 (0%)]\tLoss: 0.017646\nTrain epoch: 954 [2560/5139 (48%)]\tLoss: 0.006515\nTrain epoch: 954 [5120/5139 (95%)]\tLoss: 0.006771\nTrain epoch: 955 [0/5139 (0%)]\tLoss: 0.004406\nTrain epoch: 955 [2560/5139 (48%)]\tLoss: 0.010061\nTrain epoch: 955 [5120/5139 (95%)]\tLoss: 0.001027\nTrain epoch: 956 [0/5139 (0%)]\tLoss: 0.002209\nTrain epoch: 956 [2560/5139 (48%)]\tLoss: 0.003536\nTrain epoch: 956 [5120/5139 (95%)]\tLoss: 0.000902\nTrain epoch: 957 [0/5139 (0%)]\tLoss: 0.001558\nTrain epoch: 957 [2560/5139 (48%)]\tLoss: 0.000611\nTrain epoch: 957 [5120/5139 (95%)]\tLoss: 0.002802\nTrain epoch: 958 [0/5139 (0%)]\tLoss: 0.001951\nTrain epoch: 958 [2560/5139 (48%)]\tLoss: 0.000457\nTrain epoch: 958 [5120/5139 (95%)]\tLoss: 0.004627\nTrain epoch: 959 [0/5139 (0%)]\tLoss: 0.000160\nTrain epoch: 959 [2560/5139 (48%)]\tLoss: 0.000243\nTrain epoch: 959 [5120/5139 (95%)]\tLoss: 0.010076\nTrain epoch: 960 [0/5139 (0%)]\tLoss: 0.001227\nTrain epoch: 960 [2560/5139 (48%)]\tLoss: 0.001303\nTrain epoch: 960 [5120/5139 (95%)]\tLoss: 0.002797\nTrain epoch: 961 [0/5139 (0%)]\tLoss: 0.002656\nTrain epoch: 961 [2560/5139 (48%)]\tLoss: 0.002275\nTrain epoch: 961 [5120/5139 (95%)]\tLoss: 0.000024\nTrain epoch: 962 [0/5139 (0%)]\tLoss: 0.004270\nTrain epoch: 962 [2560/5139 (48%)]\tLoss: 0.001470\nTrain epoch: 962 [5120/5139 (95%)]\tLoss: 0.000165\nTrain epoch: 963 [0/5139 (0%)]\tLoss: 0.003366\nTrain epoch: 963 [2560/5139 (48%)]\tLoss: 0.003595\nTrain epoch: 963 [5120/5139 (95%)]\tLoss: 0.064119\nTrain epoch: 964 [0/5139 (0%)]\tLoss: 0.128629\nTrain epoch: 964 [2560/5139 (48%)]\tLoss: 0.200605\nTrain epoch: 964 [5120/5139 (95%)]\tLoss: 0.093925\nTrain epoch: 965 [0/5139 (0%)]\tLoss: 0.019859\nTrain epoch: 965 [2560/5139 (48%)]\tLoss: 0.009415\nTrain epoch: 965 [5120/5139 (95%)]\tLoss: 0.000073\nTrain epoch: 966 [0/5139 (0%)]\tLoss: 0.011932\nTrain epoch: 966 [2560/5139 (48%)]\tLoss: 0.015785\nTrain epoch: 966 [5120/5139 (95%)]\tLoss: 0.000617\nTrain epoch: 967 [0/5139 (0%)]\tLoss: 0.002198\nTrain epoch: 967 [2560/5139 (48%)]\tLoss: 0.003631\nTrain epoch: 967 [5120/5139 (95%)]\tLoss: 0.000564\nTrain epoch: 968 [0/5139 (0%)]\tLoss: 0.003391\nTrain epoch: 968 [2560/5139 (48%)]\tLoss: 0.007063\nTrain epoch: 968 [5120/5139 (95%)]\tLoss: 0.000016\nTrain epoch: 969 [0/5139 (0%)]\tLoss: 0.001345\nTrain epoch: 969 [2560/5139 (48%)]\tLoss: 0.011922\nTrain epoch: 969 [5120/5139 (95%)]\tLoss: 0.007011\nTrain epoch: 970 [0/5139 (0%)]\tLoss: 0.008030\nTrain epoch: 970 [2560/5139 (48%)]\tLoss: 0.035426\nTrain epoch: 970 [5120/5139 (95%)]\tLoss: 0.000213\nTrain epoch: 971 [0/5139 (0%)]\tLoss: 0.002943\nTrain epoch: 971 [2560/5139 (48%)]\tLoss: 0.003279\nTrain epoch: 971 [5120/5139 (95%)]\tLoss: 0.000238\nTrain epoch: 972 [0/5139 (0%)]\tLoss: 0.010167\nTrain epoch: 972 [2560/5139 (48%)]\tLoss: 0.001572\nTrain epoch: 972 [5120/5139 (95%)]\tLoss: 0.000014\nTrain epoch: 973 [0/5139 (0%)]\tLoss: 0.004279\nTrain epoch: 973 [2560/5139 (48%)]\tLoss: 0.000686\nTrain epoch: 973 [5120/5139 (95%)]\tLoss: 0.001624\nTrain epoch: 974 [0/5139 (0%)]\tLoss: 0.003830\nTrain epoch: 974 [2560/5139 (48%)]\tLoss: 0.003047\nTrain epoch: 974 [5120/5139 (95%)]\tLoss: 0.003709\nTrain epoch: 975 [0/5139 (0%)]\tLoss: 0.001269\nTrain epoch: 975 [2560/5139 (48%)]\tLoss: 0.004755\nTrain epoch: 975 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 976 [0/5139 (0%)]\tLoss: 0.000445\nTrain epoch: 976 [2560/5139 (48%)]\tLoss: 0.002119\nTrain epoch: 976 [5120/5139 (95%)]\tLoss: 0.000001\nTrain epoch: 977 [0/5139 (0%)]\tLoss: 0.003488\nTrain epoch: 977 [2560/5139 (48%)]\tLoss: 0.000217\nTrain epoch: 977 [5120/5139 (95%)]\tLoss: 0.000056\nTrain epoch: 978 [0/5139 (0%)]\tLoss: 0.002678\nTrain epoch: 978 [2560/5139 (48%)]\tLoss: 0.012219\nTrain epoch: 978 [5120/5139 (95%)]\tLoss: 0.000143\nTrain epoch: 979 [0/5139 (0%)]\tLoss: 0.001257\nTrain epoch: 979 [2560/5139 (48%)]\tLoss: 0.000252\nTrain epoch: 979 [5120/5139 (95%)]\tLoss: 0.000601\nTrain epoch: 980 [0/5139 (0%)]\tLoss: 0.000752\nTrain epoch: 980 [2560/5139 (48%)]\tLoss: 0.001676\nTrain epoch: 980 [5120/5139 (95%)]\tLoss: 0.000002\nTrain epoch: 981 [0/5139 (0%)]\tLoss: 0.018058\nTrain epoch: 981 [2560/5139 (48%)]\tLoss: 0.000220\nTrain epoch: 981 [5120/5139 (95%)]\tLoss: 0.000331\nTrain epoch: 982 [0/5139 (0%)]\tLoss: 0.016728\nTrain epoch: 982 [2560/5139 (48%)]\tLoss: 0.041205\nTrain epoch: 982 [5120/5139 (95%)]\tLoss: 0.001998\nTrain epoch: 983 [0/5139 (0%)]\tLoss: 0.021815\nTrain epoch: 983 [2560/5139 (48%)]\tLoss: 0.014868\nTrain epoch: 983 [5120/5139 (95%)]\tLoss: 0.000180\nTrain epoch: 984 [0/5139 (0%)]\tLoss: 0.003026\nTrain epoch: 984 [2560/5139 (48%)]\tLoss: 0.009743\nTrain epoch: 984 [5120/5139 (95%)]\tLoss: 0.000056\nTrain epoch: 985 [0/5139 (0%)]\tLoss: 0.002558\nTrain epoch: 985 [2560/5139 (48%)]\tLoss: 0.000344\nTrain epoch: 985 [5120/5139 (95%)]\tLoss: 0.000341\nTrain epoch: 986 [0/5139 (0%)]\tLoss: 0.035956\nTrain epoch: 986 [2560/5139 (48%)]\tLoss: 0.002668\nTrain epoch: 986 [5120/5139 (95%)]\tLoss: 0.000132\nTrain epoch: 987 [0/5139 (0%)]\tLoss: 0.010163\nTrain epoch: 987 [2560/5139 (48%)]\tLoss: 0.016827\nTrain epoch: 987 [5120/5139 (95%)]\tLoss: 0.000003\nTrain epoch: 988 [0/5139 (0%)]\tLoss: 0.006114\nTrain epoch: 988 [2560/5139 (48%)]\tLoss: 0.002351\nTrain epoch: 988 [5120/5139 (95%)]\tLoss: 0.000000\nTrain epoch: 989 [0/5139 (0%)]\tLoss: 0.004736\nTrain epoch: 989 [2560/5139 (48%)]\tLoss: 0.000085\nTrain epoch: 989 [5120/5139 (95%)]\tLoss: 0.000044\nTrain epoch: 990 [0/5139 (0%)]\tLoss: 0.000514\nTrain epoch: 990 [2560/5139 (48%)]\tLoss: 0.000362\nTrain epoch: 990 [5120/5139 (95%)]\tLoss: 0.000802\nTrain epoch: 991 [0/5139 (0%)]\tLoss: 0.007034\nTrain epoch: 991 [2560/5139 (48%)]\tLoss: 0.005511\nTrain epoch: 991 [5120/5139 (95%)]\tLoss: 0.008483\nTrain epoch: 992 [0/5139 (0%)]\tLoss: 0.003320\nTrain epoch: 992 [2560/5139 (48%)]\tLoss: 0.021119\nTrain epoch: 992 [5120/5139 (95%)]\tLoss: 0.000000\nTrain epoch: 993 [0/5139 (0%)]\tLoss: 0.000871\nTrain epoch: 993 [2560/5139 (48%)]\tLoss: 0.004216\nTrain epoch: 993 [5120/5139 (95%)]\tLoss: 0.000326\nTrain epoch: 994 [0/5139 (0%)]\tLoss: 0.001782\nTrain epoch: 994 [2560/5139 (48%)]\tLoss: 0.018487\nTrain epoch: 994 [5120/5139 (95%)]\tLoss: 0.000181\nTrain epoch: 995 [0/5139 (0%)]\tLoss: 0.006417\nTrain epoch: 995 [2560/5139 (48%)]\tLoss: 0.003506\nTrain epoch: 995 [5120/5139 (95%)]\tLoss: 0.000041\nTrain epoch: 996 [0/5139 (0%)]\tLoss: 0.001281\nTrain epoch: 996 [2560/5139 (48%)]\tLoss: 0.001264\nTrain epoch: 996 [5120/5139 (95%)]\tLoss: 0.000363\nTrain epoch: 997 [0/5139 (0%)]\tLoss: 0.001293\nTrain epoch: 997 [2560/5139 (48%)]\tLoss: 0.000973\nTrain epoch: 997 [5120/5139 (95%)]\tLoss: 0.001046\nTrain epoch: 998 [0/5139 (0%)]\tLoss: 0.001982\nTrain epoch: 998 [2560/5139 (48%)]\tLoss: 0.000765\nTrain epoch: 998 [5120/5139 (95%)]\tLoss: 0.000013\nTrain epoch: 999 [0/5139 (0%)]\tLoss: 0.000166\nTrain epoch: 999 [2560/5139 (48%)]\tLoss: 0.000210\nTrain epoch: 999 [5120/5139 (95%)]\tLoss: 0.000005\nTrain epoch: 1000 [0/5139 (0%)]\tLoss: 0.001056\nTrain epoch: 1000 [2560/5139 (48%)]\tLoss: 0.007207\nTrain epoch: 1000 [5120/5139 (95%)]\tLoss: 0.000008\n","output_type":"stream"}]},{"cell_type":"code","source":"# # test\nprint('Test set 1')\nprint('all training done. Testing...')\nmodel_p = Model()\nmodel_p.to(device)\nmodel_p.load_state_dict(torch.load(model_file_name))\ntest_T, test_P = predicting(model_p, device, test_loader)\n\ntest_accuracy = accuracy_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_mcc = matthews_corrcoef(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_auc = roc_auc_score(test_T, test_P)\ntest_recall = recall_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_precision = precision_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_f1_score=f1_score(test_T, np.where(test_P >= 0.5, 1, 0))\nresult_str = 'test result:' + '\\n' +'test_accuracy:' + str(test_accuracy) + '\\n' +'test_auc:' + str(test_auc) + '\\n' + 'test_recall:' + str(\n    test_recall) + '\\n' + 'test_precision:' + str(test_precision) + '\\n'+ 'test_f1_core:' + str(test_f1_score) + '\\n' +'test_mcc:' + str(test_mcc) + '\\n'\n \nprint(result_str)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:16:41.365416Z","iopub.execute_input":"2024-10-15T03:16:41.366323Z","iopub.status.idle":"2024-10-15T03:16:41.548229Z","shell.execute_reply.started":"2024-10-15T03:16:41.366283Z","shell.execute_reply":"2024-10-15T03:16:41.547180Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Test set 1\nall training done. Testing...\ntest result:\ntest_accuracy:0.9753954305799648\ntest_auc:0.9883216873706004\ntest_recall:0.9652173913043478\ntest_precision:0.9940298507462687\ntest_f1_core:0.9794117647058824\ntest_mcc:0.9494941709089748\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# # test\nprint('Test set 2')\nprint('all training done. Testing...')\nmodel_p = Model()\nmodel_p.to(device)\nmodel_p.load_state_dict(torch.load(model_file_name))\ntest_T, test_P = predicting(model_p, device, test_loader2)\n\ntest_accuracy = accuracy_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_mcc = matthews_corrcoef(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_auc = roc_auc_score(test_T, test_P)\ntest_recall = recall_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_precision = precision_score(test_T, np.where(test_P >= 0.5, 1, 0))\ntest_f1_score=f1_score(test_T, np.where(test_P >= 0.5, 1, 0))\nresult_str = 'test result:' + '\\n' +'test_accuracy:' + str(test_accuracy) + '\\n' +'test_auc:' + str(test_auc) + '\\n' + 'test_recall:' + str(\n    test_recall) + '\\n' + 'test_precision:' + str(test_precision) + '\\n'+ 'test_f1_core:' + str(test_f1_score) + '\\n' +'test_mcc:' + str(test_mcc) + '\\n'\n \nprint(result_str)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:16:41.549542Z","iopub.execute_input":"2024-10-15T03:16:41.549848Z","iopub.status.idle":"2024-10-15T03:16:41.692307Z","shell.execute_reply.started":"2024-10-15T03:16:41.549803Z","shell.execute_reply":"2024-10-15T03:16:41.691322Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Test set 2\nall training done. Testing...\ntest result:\ntest_accuracy:0.9974683544303797\ntest_auc:0.9968185550082101\ntest_recall:1.0\ntest_precision:0.9950980392156863\ntest_f1_core:0.9975429975429976\ntest_mcc:0.9949448411485455\n\n","output_type":"stream"}]}]}